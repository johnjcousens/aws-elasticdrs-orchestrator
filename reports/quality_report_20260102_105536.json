{
  "metadata": {
    "generated_at": "2026-01-02T10:55:36.052188",
    "generator": "AWS DRS Orchestration Quality Reporter",
    "version": "1.0.0"
  },
  "summary": {
    "total_files_analyzed": 53,
    "tools_run": 3,
    "tools_passed": 1,
    "tool_compliance_percentage": 33.33,
    "flake8_violations": 372,
    "overall_status": "FAILED",
    "timestamp": "2026-01-02T10:55:36.052188",
    "files_analyzed": [
      "lambda/drs_tag_sync.py",
      "lambda/deploy_lambda.py",
      "lambda/index.py",
      "lambda/execution_registry.py",
      "lambda/build_and_deploy.py",
      "lambda/tag_discovery.py",
      "lambda/rbac_middleware.py",
      "lambda/orchestration_stepfunctions.py",
      "scripts/generate_quality_report.py",
      "scripts/monitor_lambda_drill.py",
      "scripts/manage-user-roles.py",
      "scripts/add-current-account.py",
      "scripts/test_drs_drill_trace.py",
      "tests/python/monitor_execution.py",
      "tests/python/conftest.py",
      "tests/python/create_test_plan.py",
      "tests/python/create_test_ui.py",
      "tests/python/test_drs_validation.py",
      "tests/python/create_real_test_data.py",
      "tests/python/monitor_drs_drill.py",
      "tests/python/validate_setup.py",
      "tests/python/cleanup_all_data.py",
      "tests/python/standalone_drs_drill.py",
      "tests/python/automated_e2e_test.py",
      "tests/python/test_with_lambda_invoke.py",
      "tests/python/check_pg_response.py",
      "tests/security/validate_framework.py",
      "tests/security/rbac_security_tests.py",
      "tests/security/demo_security_test.py",
      "tests/security/run_security_tests.py",
      "tests/security/run_existing_user_tests.py",
      "tests/python/mocks/__init__.py",
      "tests/python/mocks/mock_drs_client.py",
      "tests/python/unit/test_data_generator.py",
      "tests/python/unit/__init__.py",
      "tests/python/unit/test_drs_service_limits.py",
      "tests/python/unit/test_infrastructure_smoke.py",
      "tests/python/unit/test_wave_transformation.py",
      "tests/python/unit/test_fixtures.py",
      "tests/python/unit/test_mock_drs_client.py",
      "tests/python/unit/test_recovery_plan_delete.py",
      "tests/python/integration/__init__.py",
      "tests/python/utils/test_data_generator.py",
      "tests/python/utils/__init__.py",
      "tests/python/fixtures/__init__.py",
      "tests/python/fixtures/recovery_plan_fixtures.py",
      "tests/python/e2e/test_recovery_plan_bugs.py",
      "tests/python/e2e/test_protection_group_crud.py",
      "tests/python/e2e/test_recovery_plan_e2e.py",
      "tests/python/e2e/__init__.py",
      "tests/python/e2e/get_auth_token.py",
      "tests/python/e2e/test_recovery_plan_api_crud.py",
      "tests/python/e2e/test_protection_groups_fix.py"
    ]
  },
  "tool_results": {
    "black": {
      "tool": "black",
      "status": "failed",
      "files_checked": 53,
      "files_needing_format": "unknown",
      "output": "--- lambda/build_and_deploy.py\t2026-01-02 15:52:46.017242+00:00\n+++ lambda/build_and_deploy.py\t2026-01-02 15:55:36.674015+00:00\n@@ -26,20 +26,24 @@\n             f\"Pre-built dist directory not found at {dist_dir}. \"\n             f\"Lambda package must include pre-built frontend/dist/ folder.\"\n         )\n \n     # Count files in dist\n-    file_count = sum(1 for root, dirs, files in os.walk(dist_dir) for f in files)\n+    file_count = sum(\n+        1 for root, dirs, files in os.walk(dist_dir) for f in files\n+    )\n     print(f\"Found pre-built dist directory with {file_count} files\")\n \n     return dist_dir\n \n \n def inject_aws_config_into_dist(dist_dir, properties):\n     \"\"\"Generate aws-config.js, aws-config.json, and inject script tag into index.html\"\"\"\n     print(\"Injecting AWS configuration into pre-built dist...\")\n-    region = properties.get(\"Region\", os.environ.get(\"AWS_REGION\", \"us-west-2\"))\n+    region = properties.get(\n+        \"Region\", os.environ.get(\"AWS_REGION\", \"us-west-2\")\n+    )\n \n     # Create configuration object\n     config_obj = {\n         \"region\": region,\n         \"userPoolId\": properties.get(\"UserPoolId\", \"\"),\n@@ -91,11 +95,13 @@\n \n     config_js_path = os.path.join(assets_dir, \"aws-config.js\")\n     with open(config_js_path, \"w\") as f:\n         f.write(config_js_content)\n \n-    print(\"\u2705 Generated aws-config.js in dist/assets/ (backwards compatibility)\")\n+    print(\n+        \"\u2705 Generated aws-config.js in dist/assets/ (backwards compatibility)\"\n+    )\n \n     # 3. Inject script tag into index.html to load aws-config.js before React app\n     index_html_path = os.path.join(dist_dir, \"index.html\")\n     if os.path.exists(index_html_path):\n         with open(index_html_path, \"r\") as f:\n@@ -110,18 +116,24 @@\n         match = re.search(script_pattern, html_content)\n \n         if match:\n             # Insert config script before first script tag\n             insert_pos = match.start()\n-            config_script = '    <script src=\"/assets/aws-config.js\"></script>\\n    '\n+            config_script = (\n+                '    <script src=\"/assets/aws-config.js\"></script>\\n    '\n+            )\n             html_content = (\n-                html_content[:insert_pos] + config_script + html_content[insert_pos:]\n+                html_content[:insert_pos]\n+                + config_script\n+                + html_content[insert_pos:]\n             )\n             print(\"\u2705 Injected aws-config.js script tag BEFORE React bundle\")\n         else:\n             # Fallback: insert before </head> if no script tags found\n-            script_tag = '    <script src=\"/assets/aws-config.js\"></script>\\n  </head>'\n+            script_tag = (\n+                '    <script src=\"/assets/aws-config.js\"></script>\\n  </head>'\n+            )\n             html_content = html_content.replace(\"</head>\", script_tag)\n             print(\n                 \"\u2705 Injected aws-config.js script tag before </head> (no script tags found)\"\n             )\n \n@@ -137,21 +149,25 @@\n     \"\"\"Deploy pre-built frontend\"\"\"\n     properties = event[\"ResourceProperties\"]\n     bucket_name = properties[\"BucketName\"]\n     distribution_id = properties[\"DistributionId\"]\n \n-    print(f\"Frontend Builder: Deploying pre-built frontend to bucket: {bucket_name}\")\n+    print(\n+        f\"Frontend Builder: Deploying pre-built frontend to bucket: {bucket_name}\"\n+    )\n \n     try:\n         # Get frontend source from Lambda package\n         lambda_frontend_path = \"/var/task/frontend\"\n         if not os.path.exists(lambda_frontend_path):\n             raise Exception(\n                 f\"Frontend source not found in Lambda package at {lambda_frontend_path}\"  # noqa: E713\n             )\n \n-        print(f\"Found frontend source in Lambda package at {lambda_frontend_path}\")\n+        print(\n+            f\"Found frontend source in Lambda package at {lambda_frontend_path}\"\n+        )\n \n         # Use pre-built dist/ folder (no npm build required)\n         dist_dir = use_prebuilt_dist(lambda_frontend_path)\n \n         # Create temporary copy of dist to inject config\n@@ -268,11 +284,13 @@\n def delete(event, context):\n     \"\"\"Empty S3 bucket before CloudFormation deletes it\"\"\"\n     properties = event[\"ResourceProperties\"]\n     bucket_name = properties[\"BucketName\"]\n \n-    print(f\"Frontend Builder: Emptying bucket {bucket_name} before deletion...\")\n+    print(\n+        f\"Frontend Builder: Emptying bucket {bucket_name} before deletion...\"\n+    )\n \n     try:\n         # Delete all objects (including versions if versioning enabled)\n         paginator = s3.get_paginator(\"list_object_versions\")\n \n@@ -307,11 +325,13 @@\n                     Bucket=bucket_name, Delete={\"Objects\": objects_to_delete}\n                 )\n                 delete_count += len(objects_to_delete)\n                 print(f\"  Deleted {len(objects_to_delete)} objects/versions\")\n \n-        print(f\"\u2705 Bucket emptied successfully. Total objects deleted: {delete_count}\")\n+        print(\n+            f\"\u2705 Bucket emptied successfully. Total objects deleted: {delete_count}\"\n+        )\n         return None\n \n     except s3.exceptions.NoSuchBucket:\n         print(f\"Bucket {bucket_name} doesn't exist - skipping cleanup\")\n         return None\n@@ -327,7 +347,9 @@\n         return None\n \n \n def lambda_handler(event, context):\n     \"\"\"Main handler for CloudFormation custom resource\"\"\"\n-    print(f\"Frontend Builder: Received event: {json.dumps(event, default=str)}\")\n+    print(\n+        f\"Frontend Builder: Received event: {json.dumps(event, default=str)}\"\n+    )\n     helper(event, context)\n--- lambda/deploy_lambda.py\t2026-01-02 15:54:52.613706+00:00\n+++ lambda/deploy_lambda.py\t2026-01-02 15:55:36.682467+00:00\n@@ -90,11 +90,14 @@\n     \"\"\"Upload file to S3\"\"\"\n     print(f\"\\n\u2601\ufe0f  Uploading to S3: s3://{bucket}/{s3_key}\")\n \n     try:\n         s3.upload_file(\n-            local_file, bucket, s3_key, ExtraArgs={\"ContentType\": \"application/zip\"}\n+            local_file,\n+            bucket,\n+            s3_key,\n+            ExtraArgs={\"ContentType\": \"application/zip\"},\n         )\n         print(\"  \u2705 Upload successful\")\n         return True\n     except Exception as e:\n         print(f\"  \u274c Upload failed: {e}\")\n@@ -189,11 +192,13 @@\n     # Create package\n     create_lambda_package(str(source_file), str(package_dir), str(output_zip))\n \n     if args.s3_only or args.full:\n         # Upload to S3\n-        if upload_to_s3(str(output_zip), args.bucket, \"lambda/api-handler.zip\"):\n+        if upload_to_s3(\n+            str(output_zip), args.bucket, \"lambda/api-handler.zip\"\n+        ):\n             print(\n                 f\"  \u2705 Lambda package available at s3://{args.bucket}/lambda/api-handler.zip\"\n             )\n \n     if args.direct or args.full:\n--- tests/python/cleanup_all_data.py\t2026-01-02 15:52:46.130872+00:00\n+++ tests/python/cleanup_all_data.py\t2026-01-02 15:55:36.709057+00:00\n@@ -19,36 +19,45 @@\n     ClientId=USER_POOL_CLIENT_ID,\n     AuthFlow=\"USER_PASSWORD_AUTH\",\n     AuthParameters={\"USERNAME\": USERNAME, \"PASSWORD\": PASSWORD},\n )\n token = response[\"AuthenticationResult\"][\"IdToken\"]\n-headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n+headers = {\n+    \"Authorization\": f\"Bearer {token}\",\n+    \"Content-Type\": \"application/json\",\n+}\n \n print(\"\\n\ud83d\uddd1\ufe0f  Deleting all Recovery Plans...\")\n r = requests.get(f\"{API_ENDPOINT}/recovery-plans\", headers=headers)\n plans = r.json().get(\"plans\", [])\n print(f\"Found {len(plans)} Recovery Plans\")\n for plan in plans:\n     plan_id = plan.get(\"recoveryPlanId\") or plan.get(\"PlanId\")\n     plan_name = plan.get(\"recoveryPlanName\") or plan.get(\"PlanName\", \"Unknown\")\n     if plan_id:\n         print(f\"  Deleting: {plan_name} ({plan_id})\")\n-        requests.delete(f\"{API_ENDPOINT}/recovery-plans/{plan_id}\", headers=headers)\n+        requests.delete(\n+            f\"{API_ENDPOINT}/recovery-plans/{plan_id}\", headers=headers\n+        )\n \n print(\"\\n\ud83d\uddd1\ufe0f  Deleting all Protection Groups...\")\n r = requests.get(f\"{API_ENDPOINT}/protection-groups\", headers=headers)\n groups = r.json().get(\"groups\", [])\n print(f\"Found {len(groups)} Protection Groups\")\n for pg in groups:\n     pg_id = pg[\"protectionGroupId\"]\n     pg_name = pg[\"name\"]\n     print(f\"  Deleting: {pg_name} ({pg_id})\")\n-    requests.delete(f\"{API_ENDPOINT}/protection-groups/{pg_id}\", headers=headers)\n+    requests.delete(\n+        f\"{API_ENDPOINT}/protection-groups/{pg_id}\", headers=headers\n+    )\n \n print(\"\\n\u2705 All data cleaned up!\")\n print(\"\\n\ud83d\udccb Ready for manual UI testing:\")\n-print(\"   1. Navigate to: https://d1wfyuosowt0hl.cloudfront.net/recovery-plans\")\n+print(\n+    \"   1. Navigate to: https://d1wfyuosowt0hl.cloudfront.net/recovery-plans\"\n+)\n print(\"   2. First create 3 Protection Groups:\")\n print(\"      - WebServers (2 DRS servers)\")\n print(\"      - AppServers (4 DRS servers)\")\n print(\"      - DatabaseServers (2 DRS servers)\")\n print(\"   3. Then create TEST Recovery Plan with 3 waves as specified\")\n--- tests/python/conftest.py\t2026-01-02 15:55:23.569520+00:00\n+++ tests/python/conftest.py\t2026-01-02 15:55:36.714410+00:00\n@@ -82,11 +82,13 @@\n def protection_groups_table(dynamodb_mock):\n     \"\"\"Create mock Protection Groups DynamoDB table.\"\"\"\n     table = dynamodb_mock.create_table(\n         TableName=\"protection-groups-test\",\n         KeySchema=[{\"AttributeName\": \"GroupId\", \"KeyType\": \"HASH\"}],\n-        AttributeDefinitions=[{\"AttributeName\": \"GroupId\", \"AttributeType\": \"S\"}],\n+        AttributeDefinitions=[\n+            {\"AttributeName\": \"GroupId\", \"AttributeType\": \"S\"}\n+        ],\n         BillingMode=\"PAY_PER_REQUEST\",\n     )\n     return table\n \n \n@@ -94,11 +96,13 @@\n def recovery_plans_table(dynamodb_mock):\n     \"\"\"Create mock Recovery Plans DynamoDB table.\"\"\"\n     table = dynamodb_mock.create_table(\n         TableName=\"recovery-plans-test\",\n         KeySchema=[{\"AttributeName\": \"PlanId\", \"KeyType\": \"HASH\"}],\n-        AttributeDefinitions=[{\"AttributeName\": \"PlanId\", \"AttributeType\": \"S\"}],\n+        AttributeDefinitions=[\n+            {\"AttributeName\": \"PlanId\", \"AttributeType\": \"S\"}\n+        ],\n         BillingMode=\"PAY_PER_REQUEST\",\n     )\n     return table\n \n \n@@ -106,11 +110,13 @@\n def execution_history_table(dynamodb_mock):\n     \"\"\"Create mock Execution History DynamoDB table.\"\"\"\n     table = dynamodb_mock.create_table(\n         TableName=\"execution-history-test\",\n         KeySchema=[{\"AttributeName\": \"ExecutionId\", \"KeyType\": \"HASH\"}],\n-        AttributeDefinitions=[{\"AttributeName\": \"ExecutionId\", \"AttributeType\": \"S\"}],\n+        AttributeDefinitions=[\n+            {\"AttributeName\": \"ExecutionId\", \"AttributeType\": \"S\"}\n+        ],\n         BillingMode=\"PAY_PER_REQUEST\",\n     )\n     return table\n \n \n--- lambda/drs_tag_sync.py\t2026-01-02 15:52:46.025405+00:00\n+++ lambda/drs_tag_sync.py\t2026-01-02 15:55:36.716272+00:00\n@@ -73,11 +73,13 @@\n             if result[\"total\"] > 0:\n                 regions_with_servers.append(region)\n                 total_servers += result[\"total\"]\n                 total_synced += result[\"synced\"]\n                 total_failed += result[\"failed\"]\n-                logger.info(f\"{region}: {result['synced']}/{result['total']} synced\")\n+                logger.info(\n+                    f\"{region}: {result['synced']}/{result['total']} synced\"\n+                )\n         except Exception as e:\n             # Log but continue - don't fail entire sync for one region\n             logger.warning(f\"{region}: skipped - {e}\")\n \n     summary = {\n@@ -227,6 +229,8 @@\n     try:\n         drs_client.update_launch_configuration(\n             sourceServerID=source_server_id, copyTags=True\n         )\n     except Exception as e:\n-        logger.warning(f\"Failed to enable copyTags for {source_server_id}: {e}\")\n+        logger.warning(\n+            f\"Failed to enable copyTags for {source_server_id}: {e}\"\n+        )\n--- lambda/execution_registry.py\t2026-01-02 15:55:05.640339+00:00\n+++ lambda/execution_registry.py\t2026-01-02 15:55:36.750126+00:00\n@@ -16,11 +16,13 @@\n \n # Initialize AWS clients\n dynamodb = boto3.resource(\"dynamodb\")\n \n # Environment variables\n-EXECUTION_HISTORY_TABLE = os.environ.get(\"EXECUTION_HISTORY_TABLE\", \"execution-history\")\n+EXECUTION_HISTORY_TABLE = os.environ.get(\n+    \"EXECUTION_HISTORY_TABLE\", \"execution-history\"\n+)\n \n # Get table reference\n execution_table = dynamodb.Table(EXECUTION_HISTORY_TABLE)\n \n \n@@ -41,11 +43,13 @@\n     - register: Create new execution record\n     - update: Update execution status/waves\n     - complete: Mark execution as complete\n     - get: Get execution by ID\n     \"\"\"\n-    print(f\"Execution Registry received: {json.dumps(event, cls=DecimalEncoder)}\")\n+    print(\n+        f\"Execution Registry received: {json.dumps(event, cls=DecimalEncoder)}\"\n+    )\n \n     action = event.get(\"action\", \"register\")\n \n     try:\n         if action == \"register\":\n@@ -138,11 +142,13 @@\n \n     if source == \"UI\":\n         email = sanitize_input(details.get(\"userEmail\", \"UI User\"))\n         return email\n     elif source == \"CLI\":\n-        user = details.get(\"iamUser\") or details.get(\"correlationId\", \"unknown\")\n+        user = details.get(\"iamUser\") or details.get(\n+            \"correlationId\", \"unknown\"\n+        )\n         user = sanitize_input(user)\n         return f\"cli:{user}\"\n     elif source == \"EVENTBRIDGE\":\n         rule = sanitize_input(details.get(\"scheduleRuleName\", \"unknown\"))\n         return f\"schedule:{rule}\"\n@@ -191,26 +197,34 @@\n         expr_values[\":waves\"] = event[\"waves\"]\n \n     # Total waves\n     if \"totalWaves\" in event:\n         update_parts.append(\"TotalWaves = :totalWaves\")\n-        expr_values[\":totalWaves\"] = int(event[\"totalWaves\"])  # Ensure integer type\n+        expr_values[\":totalWaves\"] = int(\n+            event[\"totalWaves\"]\n+        )  # Ensure integer type\n \n     # Total servers\n     if \"totalServers\" in event:\n         update_parts.append(\"TotalServers = :totalServers\")\n-        expr_values[\":totalServers\"] = int(event[\"totalServers\"])  # Ensure integer type\n+        expr_values[\":totalServers\"] = int(\n+            event[\"totalServers\"]\n+        )  # Ensure integer type\n \n     # Current wave\n     if \"currentWave\" in event:\n         update_parts.append(\"CurrentWave = :currentWave\")\n-        expr_values[\":currentWave\"] = int(event[\"currentWave\"])  # Ensure integer type\n+        expr_values[\":currentWave\"] = int(\n+            event[\"currentWave\"]\n+        )  # Ensure integer type\n \n     # Error message\n     if \"errorMessage\" in event:\n         update_parts.append(\"ErrorMessage = :errorMessage\")\n-        expr_values[\":errorMessage\"] = str(event[\"errorMessage\"])  # Ensure string type\n+        expr_values[\":errorMessage\"] = str(\n+            event[\"errorMessage\"]\n+        )  # Ensure string type\n \n     update_expr = \"SET \" + \", \".join(update_parts)\n \n     update_kwargs = {\n         \"Key\": {\"ExecutionId\": execution_id.strip()},\n@@ -303,11 +317,13 @@\n     import re\n \n     if not re.match(r\"^[a-f0-9-]{36}$\", execution_id.strip()):\n         raise ValueError(f\"Invalid execution ID format: {execution_id}\")\n \n-    result = execution_table.get_item(Key={\"ExecutionId\": execution_id.strip()})\n+    result = execution_table.get_item(\n+        Key={\"ExecutionId\": execution_id.strip()}\n+    )\n     item = result.get(\"Item\")\n \n     if not item:\n         raise ValueError(f\"Execution not found: {execution_id}\")\n \n--- tests/python/create_test_ui.py\t2026-01-02 15:52:46.182717+00:00\n+++ tests/python/create_test_ui.py\t2026-01-02 15:55:36.754232+00:00\n@@ -17,11 +17,14 @@\n     AuthFlow=\"USER_PASSWORD_AUTH\",\n     AuthParameters={\"USERNAME\": USERNAME, \"PASSWORD\": PASSWORD},\n     ClientId=CLIENT_ID,\n )\n token = response[\"AuthenticationResult\"][\"IdToken\"]\n-headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n+headers = {\n+    \"Authorization\": f\"Bearer {token}\",\n+    \"Content-Type\": \"application/json\",\n+}\n \n # Get existing Protection Groups\n print(\"\\n\ud83d\udce6 Getting existing Protection Groups...\")\n r = requests.get(f\"{API_ENDPOINT}/protection-groups\", headers=headers)\n r.raise_for_status()\n@@ -49,13 +52,19 @@\n         \"ServerIds\": servers,\n     }\n ]\n \n print(\"\\n\ud83d\ude80 Creating 'TEST' Recovery Plan...\")\n-payload = {\"PlanName\": \"TEST\", \"Description\": \"UI test - 1 wave\", \"Waves\": waves}\n+payload = {\n+    \"PlanName\": \"TEST\",\n+    \"Description\": \"UI test - 1 wave\",\n+    \"Waves\": waves,\n+}\n print(f\"DEBUG Payload: {payload}\")\n-r = requests.post(f\"{API_ENDPOINT}/recovery-plans\", headers=headers, json=payload)\n+r = requests.post(\n+    f\"{API_ENDPOINT}/recovery-plans\", headers=headers, json=payload\n+)\n if r.status_code != 201:\n     print(f\"\u274c Error: {r.status_code} - {r.text}\")\n r.raise_for_status()\n plan = r.json()\n print(f\"DEBUG Plan response: {plan}\")\n--- scripts/add-current-account.py\t2026-01-02 15:55:23.794467+00:00\n+++ scripts/add-current-account.py\t2026-01-02 15:55:36.781273+00:00\n@@ -60,11 +60,13 @@\n \n         # Check if account already exists\n         try:\n             response = table.get_item(Key={\"AccountId\": account_id})\n             if \"Item\" in response:\n-                print(f\"\u2705 Account {account_id} already exists as a target account\")\n+                print(\n+                    f\"\u2705 Account {account_id} already exists as a target account\"\n+                )\n                 return True\n         except Exception as e:\n             print(f\"Error checking existing account: {e}\")\n \n         # Create the account item\n@@ -86,11 +88,13 @@\n             f\"{account_name} ({account_id})\" if account_name else account_id\n         )\n         print(f\"\u2705 Successfully added {account_display} as a target account\")\n         print(f\"   - No cross-account role required (same account)\")\n         print(f\"   - Status: Active\")\n-        print(f\"   - You can now use the Dashboard and all DRS orchestration features\")\n+        print(\n+            f\"   - You can now use the Dashboard and all DRS orchestration features\"\n+        )\n \n         return True\n \n     except Exception as e:\n         print(f\"\u274c Error adding target account: {e}\")\n@@ -147,15 +151,21 @@\n \n     if success:\n         print(\"\\n\ud83c\udf89 Setup complete!\")\n         print(\"   1. Refresh your browser\")\n         print(\"   2. The Dashboard should now load without errors\")\n-        print(\"   3. You can start creating Protection Groups and Recovery Plans\")\n+        print(\n+            \"   3. You can start creating Protection Groups and Recovery Plans\"\n+        )\n         print(\"\\n\ud83d\udca1 Next steps:\")\n-        print(\"   - Go to Protection Groups to discover your DRS source servers\")\n+        print(\n+            \"   - Go to Protection Groups to discover your DRS source servers\"\n+        )\n         print(\"   - Create Recovery Plans to orchestrate disaster recovery\")\n-        print(\"   - Use the Settings gear icon to manage additional target accounts\")\n+        print(\n+            \"   - Use the Settings gear icon to manage additional target accounts\"\n+        )\n     else:\n         print(\"\\n\u274c Setup failed. Please check the error messages above.\")\n         sys.exit(1)\n \n \n--- tests/python/create_test_plan.py\t2026-01-02 15:55:23.572033+00:00\n+++ tests/python/create_test_plan.py\t2026-01-02 15:55:36.794462+00:00\n@@ -14,11 +14,14 @@\n API_URL = os.getenv(\n     \"API_URL\", \"https://9cowuz4azi.execute-api.us-east-1.amazonaws.com/test\"\n )\n TOKEN = os.getenv(\"COGNITO_TOKEN\")\n \n-headers = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n+headers = {\n+    \"Authorization\": f\"Bearer {TOKEN}\",\n+    \"Content-Type\": \"application/json\",\n+}\n \n print(\"\ud83d\udd0d Step 1: Getting Protection Groups...\")\n response = requests.get(f\"{API_URL}/protection-groups\", headers=headers)\n response.raise_for_status()\n pgs = response.json()\n@@ -29,19 +32,22 @@\n     if pg[\"name\"] in [\"WebServers\", \"AppServers\", \"DatabaseServers\"]:\n         pg_map[pg[\"name\"]] = pg[\"id\"]\n         print(f\"  \u2713 {pg['name']}: {pg['id']}\")\n \n if len(pg_map) != 3:\n-    print(f\"\u274c Error: Need all 3 Protection Groups. Found: {list(pg_map.keys())}\")\n+    print(\n+        f\"\u274c Error: Need all 3 Protection Groups. Found: {list(pg_map.keys())}\"\n+    )\n     exit(1)\n \n print(\"\\n\ud83d\udd0d Step 2: Getting servers from each Protection Group...\")\n waves_data = []\n \n # Wave 1: Web - Select 2 servers from WebServers\n response = requests.get(\n-    f'{API_URL}/protection-groups/{pg_map[\"WebServers\"]}/servers', headers=headers\n+    f'{API_URL}/protection-groups/{pg_map[\"WebServers\"]}/servers',\n+    headers=headers,\n )\n response.raise_for_status()\n web_servers = response.json()\n selected_web = [s[\"serverId\"] for s in web_servers[:2]]\n print(f\"  \u2713 WebServers: Selected 2 of {len(web_servers)} servers\")\n@@ -55,11 +61,12 @@\n     }\n )\n \n # Wave 2: App - Select 2 of 4 servers from AppServers\n response = requests.get(\n-    f'{API_URL}/protection-groups/{pg_map[\"AppServers\"]}/servers', headers=headers\n+    f'{API_URL}/protection-groups/{pg_map[\"AppServers\"]}/servers',\n+    headers=headers,\n )\n response.raise_for_status()\n app_servers = response.json()\n selected_app = [s[\"serverId\"] for s in app_servers[:2]]\n print(f\"  \u2713 AppServers: Selected 2 of {len(app_servers)} servers\")\n@@ -73,11 +80,12 @@\n     }\n )\n \n # Wave 3: Database - Select last 2 servers from DatabaseServers\n response = requests.get(\n-    f'{API_URL}/protection-groups/{pg_map[\"DatabaseServers\"]}/servers', headers=headers\n+    f'{API_URL}/protection-groups/{pg_map[\"DatabaseServers\"]}/servers',\n+    headers=headers,\n )\n response.raise_for_status()\n db_servers = response.json()\n selected_db = (\n     [s[\"serverId\"] for s in db_servers[-2:]]\n@@ -113,11 +121,13 @@\n     result = response.json()\n     print(f\"\\n\u2705 SUCCESS! Recovery Plan created:\")\n     print(f\"   Plan ID: {result.get('id', 'N/A')}\")\n     print(f\"   Plan Name: {result.get('name', 'TEST')}\")\n     print(f\"   Waves: {len(result.get('waves', []))}\")\n-    print(f\"\\n\ud83c\udf10 Check UI at: https://d1wfyuosowt0hl.cloudfront.net/recovery-plans\")\n+    print(\n+        f\"\\n\ud83c\udf10 Check UI at: https://d1wfyuosowt0hl.cloudfront.net/recovery-plans\"\n+    )\n     print(f\"\\n\u2713 Plan left in system - NOT deleted - you can verify in UI\")\n else:\n     print(f\"\\n\u274c ERROR: Failed to create Recovery Plan\")\n     print(f\"   Status: {response.status_code}\")\n     print(f\"   Response: {response.text}\")\n--- tests/python/create_real_test_data.py\t2026-01-02 15:52:46.185236+00:00\n+++ tests/python/create_real_test_data.py\t2026-01-02 15:55:36.802858+00:00\n@@ -27,27 +27,38 @@\n     AuthFlow=\"USER_PASSWORD_AUTH\",\n     AuthParameters={\"USERNAME\": USERNAME, \"PASSWORD\": PASSWORD},\n     ClientId=CLIENT_ID,\n )\n token = response[\"AuthenticationResult\"][\"IdToken\"]\n-headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n+headers = {\n+    \"Authorization\": f\"Bearer {token}\",\n+    \"Content-Type\": \"application/json\",\n+}\n \n # Clean up existing TEST data\n print(\"\\n\ud83e\uddf9 Cleaning up old test data...\")\n r = requests.get(f\"{API_ENDPOINT}/recovery-plans\", headers=headers)\n for plan in r.json().get(\"plans\", []):\n     if plan.get(\"name\") == \"TEST\":\n-        plan_id = plan.get(\"recoveryPlanId\") or plan.get(\"planId\") or plan.get(\"id\")\n+        plan_id = (\n+            plan.get(\"recoveryPlanId\") or plan.get(\"planId\") or plan.get(\"id\")\n+        )\n         print(f\"  Deleting old TEST plan: {plan_id}\")\n-        requests.delete(f\"{API_ENDPOINT}/recovery-plans/{plan_id}\", headers=headers)\n+        requests.delete(\n+            f\"{API_ENDPOINT}/recovery-plans/{plan_id}\", headers=headers\n+        )\n \n r = requests.get(f\"{API_ENDPOINT}/protection-groups\", headers=headers)\n for pg in r.json().get(\"groups\", []):\n     if pg.get(\"name\") in [\"WebServers\", \"AppServers\", \"DatabaseServers\"]:\n-        pg_id = pg.get(\"protectionGroupId\") or pg.get(\"groupId\") or pg.get(\"id\")\n+        pg_id = (\n+            pg.get(\"protectionGroupId\") or pg.get(\"groupId\") or pg.get(\"id\")\n+        )\n         print(f\"  Deleting old PG: {pg.get('name')} ({pg_id})\")\n-        requests.delete(f\"{API_ENDPOINT}/protection-groups/{pg_id}\", headers=headers)\n+        requests.delete(\n+            f\"{API_ENDPOINT}/protection-groups/{pg_id}\", headers=headers\n+        )\n \n # Create Protection Groups with REAL servers\n print(\"\\n\ud83d\udce6 Creating Protection Groups with REAL DRS servers...\")\n pg_data = {\n     \"WebServers\": REAL_SERVERS[:2],  # First 2 servers\n@@ -102,20 +113,24 @@\n     \"PlanName\": \"TEST\",\n     \"Description\": \"UI test - 3 waves with REAL DRS servers\",\n     \"Waves\": waves,\n }\n \n-r = requests.post(f\"{API_ENDPOINT}/recovery-plans\", headers=headers, json=payload)\n+r = requests.post(\n+    f\"{API_ENDPOINT}/recovery-plans\", headers=headers, json=payload\n+)\n if r.status_code != 201:\n     print(f\"\u274c Error: {r.status_code} - {r.text}\")\n     exit(1)\n \n plan = r.json()\n plan_id = plan.get(\"recoveryPlanId\") or plan.get(\"planId\") or plan.get(\"id\")\n \n print(f\"\\n\u2705 SUCCESS! TEST Recovery Plan created with REAL data\")\n print(f\"   Plan ID: {plan_id}\")\n-print(f\"   3 Protection Groups: WebServers (2), AppServers (2), DatabaseServers (2)\")\n+print(\n+    f\"   3 Protection Groups: WebServers (2), AppServers (2), DatabaseServers (2)\"\n+)\n print(f\"   3 Waves: WebTier \u2192 AppTier \u2192 DatabaseTier\")\n print(f\"\\n\ud83c\udf10 View in UI: https://d1wfyuosowt0hl.cloudfront.net/recovery-plans\")\n print(f\"\\n\u2713 All data uses REAL DRS servers from us-east-1\")\n print(f\"\u2713 Servers are in CONTINUOUS replication state\")\n--- scripts/monitor_lambda_drill.py\t2026-01-02 15:55:23.784462+00:00\n+++ scripts/monitor_lambda_drill.py\t2026-01-02 15:55:36.814419+00:00\n@@ -31,11 +31,15 @@\n                 or not isinstance(execution_id, str)\n                 or len(execution_id) > 100\n             ):\n                 print(\"\u274c Invalid execution_id\")\n                 return False\n-            if not plan_id or not isinstance(plan_id, str) or len(plan_id) > 100:\n+            if (\n+                not plan_id\n+                or not isinstance(plan_id, str)\n+                or len(plan_id) > 100\n+            ):\n                 print(\"\u274c Invalid plan_id\")\n                 return False\n \n             # Get execution status\n             response = dynamodb.get_item(\n@@ -59,11 +63,14 @@\n                 wave_status = wave.get(\"Status\", {}).get(\"S\", \"UNKNOWN\")\n                 job_id = wave.get(\"JobId\", {}).get(\"S\")\n                 servers = wave.get(\"Servers\", {}).get(\"L\", [])\n                 if servers:\n                     source_server_id = (\n-                        servers[0].get(\"M\", {}).get(\"SourceServerId\", {}).get(\"S\")\n+                        servers[0]\n+                        .get(\"M\", {})\n+                        .get(\"SourceServerId\", {})\n+                        .get(\"S\")\n                     )\n \n                 elapsed = int(time.time() - start_time)\n                 print(\n                     f\"[{elapsed}s] Execution: {status}, Wave: {wave_status}, Job: {job_id or 'N/A'}\"\n@@ -92,12 +99,16 @@\n                         )\n \n                         if instances[\"Reservations\"]:\n                             for reservation in instances[\"Reservations\"]:\n                                 for instance in reservation[\"Instances\"]:\n-                                    print(f\"   \u2705 Instance: {instance['InstanceId']}\")\n-                                    print(f\"      State: {instance['State']['Name']}\")\n+                                    print(\n+                                        f\"   \u2705 Instance: {instance['InstanceId']}\"\n+                                    )\n+                                    print(\n+                                        f\"      State: {instance['State']['Name']}\"\n+                                    )\n                                     print(\n                                         f\"      Launch Time: {instance['LaunchTime']}\"\n                                     )\n                                     print(\n                                         f\"      Private IP: {instance.get('PrivateIpAddress', 'N/A')}\"\n@@ -119,13 +130,17 @@\n     return False\n \n \n if __name__ == \"__main__\":\n     execution_id = (\n-        sys.argv[1] if len(sys.argv) > 1 else \"e11a2dbc-5279-4829-b23c-2d4862ca8c68\"\n+        sys.argv[1]\n+        if len(sys.argv) > 1\n+        else \"e11a2dbc-5279-4829-b23c-2d4862ca8c68\"\n     )\n     plan_id = (\n-        sys.argv[2] if len(sys.argv) > 2 else \"ba8b28e2-7568-4c03-bff0-9f289262c1a6\"\n+        sys.argv[2]\n+        if len(sys.argv) > 2\n+        else \"ba8b28e2-7568-4c03-bff0-9f289262c1a6\"\n     )\n \n     success = monitor_execution(execution_id, plan_id)\n     sys.exit(0 if success else 1)\n--- tests/python/e2e/get_auth_token.py\t2026-01-02 15:55:23.683960+00:00\n+++ tests/python/e2e/get_auth_token.py\t2026-01-02 15:55:36.822690+00:00\n@@ -42,9 +42,11 @@\n     if token:\n         print(f\"\u2705 ID Token retrieved successfully\")\n         print(f\"\\nSet environment variable:\")\n         print(f\"export ID_TOKEN='{token}'\")\n         print(f\"\\nOr run tests directly:\")\n-        print(f\"ID_TOKEN='{token}' python tests/python/e2e/test_recovery_plan_bugs.py\")\n+        print(\n+            f\"ID_TOKEN='{token}' python tests/python/e2e/test_recovery_plan_bugs.py\"\n+        )\n     else:\n         print(\"\u274c Failed to get ID token\")\n         exit(1)\n--- tests/python/e2e/test_protection_groups_fix.py\t2026-01-02 15:55:23.689104+00:00\n+++ tests/python/e2e/test_protection_groups_fix.py\t2026-01-02 15:55:36.838019+00:00\n@@ -44,11 +44,13 @@\n \n     print(\"\u2705 Authenticated successfully\")\n \n     # Call API\n     headers = {\"Authorization\": f\"Bearer {token}\"}\n-    response = requests.get(f\"{API_ENDPOINT}/protection-groups\", headers=headers)\n+    response = requests.get(\n+        f\"{API_ENDPOINT}/protection-groups\", headers=headers\n+    )\n \n     if response.status_code != 200:\n         print(f\"\u274c API call failed: {response.status_code}\")\n         print(f\"   Response: {response.text}\")\n         return False\n@@ -66,11 +68,13 @@\n     print(f\"\u2705 Found {len(pgs)} Protection Groups\")\n \n     # Check each PG has SourceServerIds\n     all_valid = True\n     for pg in pgs:\n-        pg_name = pg.get(\"name\") or pg.get(\"Name\") or pg.get(\"GroupName\", \"Unknown\")\n+        pg_name = (\n+            pg.get(\"name\") or pg.get(\"Name\") or pg.get(\"GroupName\", \"Unknown\")\n+        )\n         source_servers = pg.get(\"SourceServerIds\") or pg.get(\"sourceServerIds\")\n \n         if not source_servers:\n             print(f\"\u274c {pg_name}: Missing SourceServerIds field\")\n             all_valid = False\n--- lambda/tag_discovery.py\t2026-01-02 15:52:46.233939+00:00\n+++ lambda/tag_discovery.py\t2026-01-02 15:55:36.850955+00:00\n@@ -118,11 +118,13 @@\n                         \"fqdn\": id_hints.get(\"fqdn\", \"\"),\n                         \"replicationState\": replication_state,\n                         \"isHealthy\": is_healthy,\n                         \"tags\": server_tags,\n                         \"region\": region,\n-                        \"accountId\": server.get(\"sourceServerID\", \"\").split(\"/\")[0]\n+                        \"accountId\": server.get(\"sourceServerID\", \"\").split(\n+                            \"/\"\n+                        )[0]\n                         if \"/\" in server.get(\"sourceServerID\", \"\")\n                         else None,\n                         \"lastLaunchResult\": server.get(\n                             \"lastLaunchResult\", \"NOT_STARTED\"\n                         ),\n@@ -163,11 +165,13 @@\n         \"healthyCount\": healthy_count,\n         \"unhealthyCount\": unhealthy_count,\n     }\n \n \n-def discover_multi_account(tags: Dict[str, str], staging_accounts: List[Dict]) -> Dict:\n+def discover_multi_account(\n+    tags: Dict[str, str], staging_accounts: List[Dict]\n+) -> Dict:\n     \"\"\"\n     Discover DRS source servers across multiple staging accounts.\n     Uses cross-account IAM role assumption.\n \n     Args:\n@@ -218,16 +222,20 @@\n                     server_tags = server.get(\"tags\", {})\n \n                     if all(server_tags.get(k) == v for k, v in tags.items()):\n                         source_props = server.get(\"sourceProperties\", {})\n                         id_hints = source_props.get(\"identificationHints\", {})\n-                        replication_info = server.get(\"dataReplicationInfo\", {})\n+                        replication_info = server.get(\n+                            \"dataReplicationInfo\", {}\n+                        )\n                         replication_state = replication_info.get(\n                             \"dataReplicationState\", \"UNKNOWN\"\n                         )\n \n-                        is_healthy = replication_state in VALID_REPLICATION_STATES\n+                        is_healthy = (\n+                            replication_state in VALID_REPLICATION_STATES\n+                        )\n                         if is_healthy:\n                             healthy += 1\n                         else:\n                             unhealthy += 1\n \n@@ -268,13 +276,16 @@\n                 \"unhealthy\": 0,\n                 \"error\": str(e),\n             }\n \n     # Discover in parallel across accounts\n-    with ThreadPoolExecutor(max_workers=min(10, len(staging_accounts))) as executor:\n+    with ThreadPoolExecutor(\n+        max_workers=min(10, len(staging_accounts))\n+    ) as executor:\n         futures = {\n-            executor.submit(discover_in_account, acc): acc for acc in staging_accounts\n+            executor.submit(discover_in_account, acc): acc\n+            for acc in staging_accounts\n         }\n \n         for future in as_completed(futures):\n             result = future.result()\n             all_servers.extend(result[\"servers\"])\n@@ -328,11 +339,13 @@\n         sorted_waves[wave] = servers_by_wave[wave]\n \n     return sorted_waves\n \n \n-def resolve_protection_group(protection_group: Dict, region: str = None) -> Dict:\n+def resolve_protection_group(\n+    protection_group: Dict, region: str = None\n+) -> Dict:\n     \"\"\"\n     Resolve a Protection Group to actual server IDs.\n \n     For EXPLICIT mode: returns sourceServerIds as-is\n     For TAGS mode: queries DRS API to find matching servers\n--- scripts/test_drs_drill_trace.py\t2026-01-02 15:55:23.800292+00:00\n+++ scripts/test_drs_drill_trace.py\t2026-01-02 15:55:36.846613+00:00\n@@ -46,11 +46,13 @@\n             if sid in SOURCE_SERVER_IDS:\n                 print(f\"    \u2713 Target server found\")\n \n                 # Check launch configuration\n                 try:\n-                    launch_config = drs.get_launch_configuration(sourceServerID=sid)\n+                    launch_config = drs.get_launch_configuration(\n+                        sourceServerID=sid\n+                    )\n                     print(\n                         f\"    Launch disposition: {launch_config.get('launchDisposition', 'UNKNOWN')}\"\n                     )\n                     print(\n                         f\"    Target instance type: {launch_config.get('targetInstanceTypeRightSizingMethod', 'UNKNOWN')}\"\n@@ -69,11 +71,13 @@\n \n         print(f\"Calling drs.start_recovery():\")\n         print(f\"  sourceServers: {source_servers}\")\n         print(f\"  isDrill: {IS_DRILL}\")\n \n-        response = drs.start_recovery(sourceServers=source_servers, isDrill=IS_DRILL)\n+        response = drs.start_recovery(\n+            sourceServers=source_servers, isDrill=IS_DRILL\n+        )\n \n         job = response.get(\"job\", {})\n         job_id = job.get(\"jobID\")\n \n         print(f\"\\n\u2713 Job created: {job_id}\")\n@@ -132,19 +136,24 @@\n                     try:\n                         ec2_response = ec2.describe_instances(\n                             InstanceIds=[recovery_instance_id]\n                         )\n                         if ec2_response[\"Reservations\"]:\n-                            instance = ec2_response[\"Reservations\"][0][\"Instances\"][0]\n+                            instance = ec2_response[\"Reservations\"][0][\n+                                \"Instances\"\n+                            ][0]\n                             ec2_state = instance[\"State\"][\"Name\"]\n                             print(f\"    EC2 State: {ec2_state}\")\n \n                             # Check tags\n                             tags = {\n-                                t[\"Key\"]: t[\"Value\"] for t in instance.get(\"Tags\", [])\n+                                t[\"Key\"]: t[\"Value\"]\n+                                for t in instance.get(\"Tags\", [])\n                             }\n-                            has_drs_tag = \"AWSElasticDisasterRecoveryManaged\" in tags\n+                            has_drs_tag = (\n+                                \"AWSElasticDisasterRecoveryManaged\" in tags\n+                            )\n                             print(f\"    Has DRS Tag: {has_drs_tag}\")\n                     except Exception as e:\n                         print(f\"    \u26a0 Could not describe EC2 instance: {e}\")\n \n             # Check completion\n@@ -177,11 +186,13 @@\n                 state = ri.get(\"ec2InstanceState\")\n \n                 print(f\"\\n  {sid}:\")\n                 print(f\"    EC2 Instance: {ec2_id}\")\n                 print(f\"    State: {state}\")\n-                print(f\"    Recovery Instance ID: {ri.get('recoveryInstanceID')}\")\n+                print(\n+                    f\"    Recovery Instance ID: {ri.get('recoveryInstanceID')}\"\n+                )\n \n                 if not ec2_id:\n                     print(f\"    \u2717 NO EC2 INSTANCE CREATED\")\n                     return False\n                 else:\n--- scripts/manage-user-roles.py\t2026-01-02 15:55:23.790675+00:00\n+++ scripts/manage-user-roles.py\t2026-01-02 15:55:36.853923+00:00\n@@ -62,11 +62,13 @@\n                 GroupName=group_name,\n             )\n             print(f\"\u2705 Added user '{username}' to group '{group_name}'\")\n             return True\n         except Exception as e:\n-            print(f\"\u274c Error adding user '{username}' to group '{group_name}': {e}\")\n+            print(\n+                f\"\u274c Error adding user '{username}' to group '{group_name}': {e}\"\n+            )\n             return False\n \n     def remove_user_from_group(self, username: str, group_name: str) -> bool:\n         \"\"\"Remove user from a group\"\"\"\n         try:\n@@ -76,11 +78,13 @@\n                 GroupName=group_name,\n             )\n             print(f\"\u2705 Removed user '{username}' from group '{group_name}'\")\n             return True\n         except Exception as e:\n-            print(f\"\u274c Error removing user '{username}' from group '{group_name}': {e}\")\n+            print(\n+                f\"\u274c Error removing user '{username}' from group '{group_name}': {e}\"\n+            )\n             return False\n \n     def create_user(\n         self,\n         email: str,\n@@ -96,19 +100,25 @@\n                 {\"Name\": \"email\", \"Value\": email},\n                 {\"Name\": \"email_verified\", \"Value\": \"true\"},\n             ]\n \n             if given_name:\n-                user_attributes.append({\"Name\": \"given_name\", \"Value\": given_name})\n+                user_attributes.append(\n+                    {\"Name\": \"given_name\", \"Value\": given_name}\n+                )\n             if family_name:\n-                user_attributes.append({\"Name\": \"family_name\", \"Value\": family_name})\n+                user_attributes.append(\n+                    {\"Name\": \"family_name\", \"Value\": family_name}\n+                )\n             if department:\n                 user_attributes.append(\n                     {\"Name\": \"custom:department\", \"Value\": department}\n                 )\n             if job_title:\n-                user_attributes.append({\"Name\": \"custom:job_title\", \"Value\": job_title})\n+                user_attributes.append(\n+                    {\"Name\": \"custom:job_title\", \"Value\": job_title}\n+                )\n \n             self.cognito.admin_create_user(\n                 UserPoolId=self.user_pool_id,\n                 Username=email,\n                 UserAttributes=user_attributes,\n@@ -193,49 +203,67 @@\n             print(f\"  {description}\")\n             print()\n \n \n def main():\n-    parser = argparse.ArgumentParser(description=\"Manage DRS Orchestration User Roles\")\n-    parser.add_argument(\"--user-pool-id\", required=True, help=\"Cognito User Pool ID\")\n+    parser = argparse.ArgumentParser(\n+        description=\"Manage DRS Orchestration User Roles\"\n+    )\n+    parser.add_argument(\n+        \"--user-pool-id\", required=True, help=\"Cognito User Pool ID\"\n+    )\n     parser.add_argument(\n         \"--region\", default=\"us-east-1\", help=\"AWS Region (default: us-east-1)\"\n     )\n \n-    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n+    subparsers = parser.add_subparsers(\n+        dest=\"command\", help=\"Available commands\"\n+    )\n \n     # List users command\n     list_parser = subparsers.add_parser(\n         \"list-users\", help=\"List all users and their roles\"\n     )\n \n     # List roles command\n-    roles_parser = subparsers.add_parser(\"list-roles\", help=\"List available roles\")\n+    roles_parser = subparsers.add_parser(\n+        \"list-roles\", help=\"List available roles\"\n+    )\n \n     # Add user to role command\n     add_parser = subparsers.add_parser(\"add-role\", help=\"Add user to role\")\n-    add_parser.add_argument(\"--username\", required=True, help=\"Username (email)\")\n+    add_parser.add_argument(\n+        \"--username\", required=True, help=\"Username (email)\"\n+    )\n     add_parser.add_argument(\n         \"--role\",\n         required=True,\n         choices=list(DRS_ROLES.keys()),\n         help=\"Role name\",\n     )\n \n     # Remove user from role command\n-    remove_parser = subparsers.add_parser(\"remove-role\", help=\"Remove user from role\")\n-    remove_parser.add_argument(\"--username\", required=True, help=\"Username (email)\")\n+    remove_parser = subparsers.add_parser(\n+        \"remove-role\", help=\"Remove user from role\"\n+    )\n+    remove_parser.add_argument(\n+        \"--username\", required=True, help=\"Username (email)\"\n+    )\n     remove_parser.add_argument(\n         \"--role\",\n         required=True,\n         choices=list(DRS_ROLES.keys()),\n         help=\"Role name\",\n     )\n \n     # Create user command\n-    create_parser = subparsers.add_parser(\"create-user\", help=\"Create new user\")\n-    create_parser.add_argument(\"--email\", required=True, help=\"User email address\")\n+    create_parser = subparsers.add_parser(\n+        \"create-user\", help=\"Create new user\"\n+    )\n+    create_parser.add_argument(\n+        \"--email\", required=True, help=\"User email address\"\n+    )\n     create_parser.add_argument(\n         \"--temp-password\", required=True, help=\"Temporary password\"\n     )\n     create_parser.add_argument(\"--given-name\", help=\"First name\")\n     create_parser.add_argument(\"--family-name\", help=\"Last name\")\n@@ -247,11 +275,13 @@\n \n     # Set password command\n     password_parser = subparsers.add_parser(\n         \"set-password\", help=\"Set permanent password\"\n     )\n-    password_parser.add_argument(\"--username\", required=True, help=\"Username (email)\")\n+    password_parser.add_argument(\n+        \"--username\", required=True, help=\"Username (email)\"\n+    )\n     password_parser.add_argument(\n         \"--password\", required=True, help=\"New permanent password\"\n     )\n \n     args = parser.parse_args()\n--- tests/python/e2e/test_protection_group_crud.py\t2026-01-02 15:55:23.677195+00:00\n+++ tests/python/e2e/test_protection_group_crud.py\t2026-01-02 15:55:36.867562+00:00\n@@ -11,13 +11,11 @@\n import requests\n \n # Config - UPDATE THESE AFTER DEPLOYMENT\n USER_POOL_ID = \"us-east-1_jKbDOFre2\"  # From stack outputs\n CLIENT_ID = \"79e5u9lflt3hvbuug78mg9okn3\"  # From stack outputs\n-API_ENDPOINT = (\n-    \"https://n122r4122g.execute-api.us-east-1.amazonaws.com/test\"  # From stack outputs\n-)\n+API_ENDPOINT = \"https://n122r4122g.execute-api.us-east-1.amazonaws.com/test\"  # From stack outputs\n USERNAME = \"testuser@example.com\"\n PASSWORD = \"TestPass123!\"\n \n \n def get_auth_token():\n@@ -37,11 +35,14 @@\n \n def test_create_protection_group(token):\n     \"\"\"Test POST /protection-groups\"\"\"\n     print(\"\\n\ud83e\uddea Test 1: CREATE Protection Group (POST)\")\n \n-    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n+    headers = {\n+        \"Authorization\": f\"Bearer {token}\",\n+        \"Content-Type\": \"application/json\",\n+    }\n     data = {\n         \"GroupName\": f\"test-pg-{int(time.time())}\",\n         \"Description\": \"Test protection group for CRUD operations\",\n         \"Region\": \"us-east-1\",\n         \"AccountId\": \"123456789012\",\n@@ -88,11 +89,14 @@\n \n def test_update_protection_group(token, pg_id):\n     \"\"\"Test PUT /protection-groups/{id}\"\"\"\n     print(f\"\\n\ud83e\uddea Test 3: UPDATE Protection Group (PUT /{pg_id})\")\n \n-    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n+    headers = {\n+        \"Authorization\": f\"Bearer {token}\",\n+        \"Content-Type\": \"application/json\",\n+    }\n     data = {\n         \"description\": \"Updated description - testing PUT method\",\n         \"sourceServerIds\": [\n             \"s-1234567890abcdef0\",\n             \"s-1234567890abcdef1\",\n@@ -110,11 +114,13 @@\n \n     if response.status_code == 200:\n         pg = response.json()\n         print(f\"\u2705 UPDATE successful\")\n         print(f\"   Description: {pg['description']}\")\n-        print(f\"   Servers: {len(pg.get('sourceServerIds', []))} (added 1 server)\")\n+        print(\n+            f\"   Servers: {len(pg.get('sourceServerIds', []))} (added 1 server)\"\n+        )\n         print(f\"   Tags: {len(pg.get('tags', []))} (added 1 tag)\")\n         return True\n     else:\n         print(f\"\u274c UPDATE failed: {response.status_code}\")\n         print(f\"   Response: {response.text}\")\n@@ -142,11 +148,14 @@\n def test_name_uniqueness(token, existing_name):\n     \"\"\"Test name uniqueness validation\"\"\"\n     print(f\"\\n\ud83e\uddea Test 5: Name Uniqueness Validation\")\n     print(f\"   Attempting to create PG with existing name: {existing_name}\")\n \n-    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n+    headers = {\n+        \"Authorization\": f\"Bearer {token}\",\n+        \"Content-Type\": \"application/json\",\n+    }\n     data = {\n         \"GroupName\": existing_name,  # Use existing name - should fail\n         \"Description\": \"This should fail due to duplicate name\",\n         \"Region\": \"us-east-1\",\n         \"AccountId\": \"123456789012\",\n--- tests/python/e2e/test_recovery_plan_bugs.py\t2026-01-02 15:55:23.674571+00:00\n+++ tests/python/e2e/test_recovery_plan_bugs.py\t2026-01-02 15:55:36.876637+00:00\n@@ -9,11 +9,12 @@\n import pytest\n import requests\n \n # API Configuration\n API_BASE_URL = os.getenv(\n-    \"API_BASE_URL\", \"https://etv40zymeg.execute-api.us-east-1.amazonaws.com/test\"\n+    \"API_BASE_URL\",\n+    \"https://etv40zymeg.execute-api.us-east-1.amazonaws.com/test\",\n )\n API_ENDPOINT = f\"{API_BASE_URL}/recovery-plans\"\n \n # Test credentials (from .env.test or environment)\n ID_TOKEN = os.getenv(\"ID_TOKEN\", \"\")  # Must be set before running\n@@ -54,33 +55,39 @@\n     }\n \n \n def test_create_recovery_plan(auth_headers, test_plan_data):\n     \"\"\"Test: Create Recovery Plan via API.\"\"\"\n-    response = requests.post(API_ENDPOINT, headers=auth_headers, json=test_plan_data)\n+    response = requests.post(\n+        API_ENDPOINT, headers=auth_headers, json=test_plan_data\n+    )\n \n     assert response.status_code == 201, f\"Create failed: {response.text}\"\n     data = response.json()\n     assert \"recoveryPlanId\" in data\n \n     # Store for cleanup\n     plan_id = data[\"recoveryPlanId\"]\n     return plan_id\n \n \n-def test_edit_recovery_plan_validates_array_format(auth_headers, test_plan_data):\n+def test_edit_recovery_plan_validates_array_format(\n+    auth_headers, test_plan_data\n+):\n     \"\"\"\n     Test: P1 Wave Bug Fix - ServerIds remain arrays after edit.\n \n     Bug: transform_rp_to_camelcase() converted ServerIds to strings\n     Fix: Preserve ServerIds as arrays in transformation\n     \"\"\"\n     # Create plan\n     create_response = requests.post(\n         API_ENDPOINT, headers=auth_headers, json=test_plan_data\n     )\n-    assert create_response.status_code == 201, f\"Create failed: {create_response.text}\"\n+    assert (\n+        create_response.status_code == 201\n+    ), f\"Create failed: {create_response.text}\"\n     response_data = create_response.json()\n     # API returns PlanId, not recoveryPlanId\n     plan_id = (\n         response_data.get(\"PlanId\")\n         or response_data.get(\"recoveryPlanId\")\n@@ -88,11 +95,13 @@\n     )\n     assert plan_id, f\"No plan ID in response: {response_data}\"\n \n     try:\n         # Get plan for edit\n-        get_response = requests.get(f\"{API_ENDPOINT}/{plan_id}\", headers=auth_headers)\n+        get_response = requests.get(\n+            f\"{API_ENDPOINT}/{plan_id}\", headers=auth_headers\n+        )\n         assert (\n             get_response.status_code == 200\n         ), f\"GET failed: {get_response.status_code} - {get_response.text}\"\n         plan_data = get_response.json()\n \n@@ -118,22 +127,26 @@\n         # Edit plan (update description)\n         plan_data[\"description\"] = \"Updated via E2E test\"\n         edit_response = requests.put(\n             f\"{API_ENDPOINT}/{plan_id}\", headers=auth_headers, json=plan_data\n         )\n-        assert edit_response.status_code == 200, f\"Edit failed: {edit_response.text}\"\n+        assert (\n+            edit_response.status_code == 200\n+        ), f\"Edit failed: {edit_response.text}\"\n \n         # Verify ServerIds still arrays after edit\n         edited_data = edit_response.json()\n         edited_waves = edited_data.get(\"waves\", [])\n         for wave in edited_waves:\n             server_ids = wave.get(\"serverIds\", [])\n             assert isinstance(\n                 server_ids, list\n             ), f\"After edit: Wave '{wave.get('waveName')}' ServerIds became {type(server_ids).__name__}\"\n \n-        print(f\"\u2705 P1 WAVE BUG FIX VALIDATED AFTER EDIT: ServerIds remain arrays\")\n+        print(\n+            f\"\u2705 P1 WAVE BUG FIX VALIDATED AFTER EDIT: ServerIds remain arrays\"\n+        )\n \n     finally:\n         # Cleanup\n         requests.delete(f\"{API_ENDPOINT}/{plan_id}\", headers=auth_headers)\n \n@@ -151,22 +164,26 @@\n     )\n     assert create_response.status_code == 201\n     plan_id = create_response.json()[\"recoveryPlanId\"]\n \n     # Delete plan - should succeed without error\n-    delete_response = requests.delete(f\"{API_ENDPOINT}/{plan_id}\", headers=auth_headers)\n+    delete_response = requests.delete(\n+        f\"{API_ENDPOINT}/{plan_id}\", headers=auth_headers\n+    )\n \n     # CRITICAL VALIDATION: Delete succeeds (no GSI error)\n     assert delete_response.status_code in [\n         200,\n         204,\n     ], f\"Delete failed with {delete_response.status_code}: {delete_response.text}\"\n \n     print(f\"\u2705 P1 DELETE BUG FIX VALIDATED: Delete succeeded using scan\")\n \n     # Verify plan deleted - GET should return 404\n-    get_response = requests.get(f\"{API_ENDPOINT}/{plan_id}\", headers=auth_headers)\n+    get_response = requests.get(\n+        f\"{API_ENDPOINT}/{plan_id}\", headers=auth_headers\n+    )\n     assert get_response.status_code == 404, \"Plan should be deleted\"\n \n \n if __name__ == \"__main__\":\n     \"\"\"Run tests directly for quick validation.\"\"\"\n--- tests/python/monitor_execution.py\t2026-01-02 15:55:23.566534+00:00\n+++ tests/python/monitor_execution.py\t2026-01-02 15:55:36.904044+00:00\n@@ -90,11 +90,12 @@\n \n                     # Count launched servers\n                     launched = sum(\n                         1\n                         for s in servers\n-                        if s.get(\"M\", {}).get(\"Status\", {}).get(\"S\") == \"LAUNCHED\"\n+                        if s.get(\"M\", {}).get(\"Status\", {}).get(\"S\")\n+                        == \"LAUNCHED\"\n                     )\n \n                     status_icon = \"\u2705\" if wave_status == \"COMPLETED\" else \"\u23f3\"\n                     print(\n                         f\"  {status_icon} Wave {wave_id}: {wave_status} ({launched}/{server_count} servers)\"\n@@ -116,11 +117,13 @@\n \n if __name__ == \"__main__\":\n     if len(sys.argv) < 2:\n         print(\"Usage: python3 monitor_execution.py <execution-id>\")\n         print(\"\\nExample:\")\n-        print(\"  python3 monitor_execution.py 79d6f9cd-c092-437c-82e9-6ec47f42982b\")\n+        print(\n+            \"  python3 monitor_execution.py 79d6f9cd-c092-437c-82e9-6ec47f42982b\"\n+        )\n         sys.exit(1)\n \n     execution_id = sys.argv[1]\n     region = sys.argv[2] if len(sys.argv) > 2 else \"us-east-1\"\n \n--- tests/python/test_drs_validation.py\t2026-01-02 15:55:23.580025+00:00\n+++ tests/python/test_drs_validation.py\t2026-01-02 15:55:36.906501+00:00\n@@ -24,11 +24,14 @@\n     ClientId=USER_POOL_CLIENT_ID,\n     AuthFlow=\"USER_PASSWORD_AUTH\",\n     AuthParameters={\"USERNAME\": USERNAME, \"PASSWORD\": PASSWORD},\n )\n token = auth_response[\"AuthenticationResult\"][\"IdToken\"]\n-headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n+headers = {\n+    \"Authorization\": f\"Bearer {token}\",\n+    \"Content-Type\": \"application/json\",\n+}\n \n print(\"\\n\" + \"=\" * 70)\n print(\"TEST 1: Verify FAKE Server IDs Are REJECTED\")\n print(\"=\" * 70)\n \n@@ -61,11 +64,13 @@\n \n # Get real DRS servers\n print(\"\\n\ud83d\udd0d Getting real DRS servers...\")\n drs = boto3.client(\"drs\", region_name=\"us-east-1\")\n servers_response = drs.describe_source_servers()\n-real_servers = [s[\"sourceServerID\"] for s in servers_response.get(\"items\", [])][:2]\n+real_servers = [\n+    s[\"sourceServerID\"] for s in servers_response.get(\"items\", [])\n+][:2]\n \n print(f\"   Found {len(real_servers)} servers: {real_servers}\")\n \n real_payload = {\n     \"GroupName\": \"RealGroup-Test\",\n@@ -87,11 +92,13 @@\n     print(f\"   Created PG: {pg_id}\")\n     print(\"\\n\u2705 SUCCESS! Real server IDs were ACCEPTED\")\n \n     # Clean up\n     print(f\"\\n\ud83e\uddf9 Cleaning up test PG...\")\n-    requests.delete(f\"{API_ENDPOINT}/protection-groups/{pg_id}\", headers=headers)\n+    requests.delete(\n+        f\"{API_ENDPOINT}/protection-groups/{pg_id}\", headers=headers\n+    )\n else:\n     print(f\"   Body: {response.text}\")\n     print(f\"\\n\u274c FAILED! Expected 201 created, got {response.status_code}\")\n \n print(\"\\n\" + \"=\" * 70)\n--- tests/python/e2e/test_recovery_plan_api_crud.py\t2026-01-02 15:55:23.686976+00:00\n+++ tests/python/e2e/test_recovery_plan_api_crud.py\t2026-01-02 15:55:36.906316+00:00\n@@ -42,13 +42,18 @@\n             AuthFlow=\"USER_PASSWORD_AUTH\",\n             AuthParameters={\"USERNAME\": username, \"PASSWORD\": password},\n         )\n \n         self.id_token = auth_response[\"AuthenticationResult\"][\"IdToken\"]\n-        self.access_token = auth_response[\"AuthenticationResult\"][\"AccessToken\"]\n-\n-        return {\"Authorization\": self.id_token, \"Content-Type\": \"application/json\"}\n+        self.access_token = auth_response[\"AuthenticationResult\"][\n+            \"AccessToken\"\n+        ]\n+\n+        return {\n+            \"Authorization\": self.id_token,\n+            \"Content-Type\": \"application/json\",\n+        }\n \n \n class RecoveryPlanAPITester:\n     \"\"\"Tests Recovery Plan CRUD operations via API\"\"\"\n \n@@ -59,22 +64,26 @@\n         self.created_plan_id = None\n \n     def create_protection_group(self, name: str, num_servers: int) -> str:\n         \"\"\"Create a Protection Group with mock servers\"\"\"\n         # Generate mock server IDs\n-        server_ids = [f\"i-{name.lower()}{i:03d}\" for i in range(1, num_servers + 1)]\n+        server_ids = [\n+            f\"i-{name.lower()}{i:03d}\" for i in range(1, num_servers + 1)\n+        ]\n \n         payload = {\n             \"GroupName\": name,\n             \"Region\": REGION,\n             \"Description\": f\"Test Protection Group for {name}\",\n             \"AccountId\": \"123456789012\",  # Required by deployed API\n             \"sourceServerIds\": server_ids,\n         }\n \n         response = requests.post(\n-            f\"{self.api_endpoint}/protection-groups\", headers=self.headers, json=payload\n+            f\"{self.api_endpoint}/protection-groups\",\n+            headers=self.headers,\n+            json=payload,\n         )\n \n         assert (\n             response.status_code == 201\n         ), f\"Failed to create PG {name}: {response.text}\"\n@@ -123,19 +132,25 @@\n             \"Owner\": USERNAME,\n             \"Waves\": wave_configs,\n         }\n \n         response = requests.post(\n-            f\"{self.api_endpoint}/recovery-plans\", headers=self.headers, json=payload\n-        )\n-\n-        assert response.status_code == 201, f\"Failed to create plan: {response.text}\"\n+            f\"{self.api_endpoint}/recovery-plans\",\n+            headers=self.headers,\n+            json=payload,\n+        )\n+\n+        assert (\n+            response.status_code == 201\n+        ), f\"Failed to create plan: {response.text}\"\n \n         plan_data = response.json()\n         print(f\"  DEBUG - API Response: {json.dumps(plan_data, indent=2)}\")\n         self.created_plan_id = (\n-            plan_data.get(\"PlanId\") or plan_data.get(\"id\") or plan_data.get(\"planId\")\n+            plan_data.get(\"PlanId\")\n+            or plan_data.get(\"id\")\n+            or plan_data.get(\"planId\")\n         )\n \n         assert (\n             self.created_plan_id\n         ), f\"No plan ID in response! Keys: {list(plan_data.keys())}\"\n@@ -143,52 +158,64 @@\n         print(\n             f\"\u2705 Created Recovery Plan: {plan_name} ({self.created_plan_id}) with {len(waves)} waves\"\n         )\n         return self.created_plan_id\n \n-    def update_recovery_plan(self, plan_id: str, updated_waves: List[Dict]) -> Dict:\n+    def update_recovery_plan(\n+        self, plan_id: str, updated_waves: List[Dict]\n+    ) -> Dict:\n         \"\"\"Update a Recovery Plan by modifying wave configurations\"\"\"\n         payload = {\"waves\": updated_waves}\n \n         response = requests.put(\n             f\"{self.api_endpoint}/recovery-plans/{plan_id}\",\n             headers=self.headers,\n             json=payload,\n         )\n \n-        assert response.status_code == 200, f\"Failed to update plan: {response.text}\"\n+        assert (\n+            response.status_code == 200\n+        ), f\"Failed to update plan: {response.text}\"\n \n         print(f\"\u2705 Updated Recovery Plan: {plan_id}\")\n         return response.json()\n \n     def delete_recovery_plan(self, plan_id: str) -> None:\n         \"\"\"Delete a Recovery Plan\"\"\"\n         response = requests.delete(\n-            f\"{self.api_endpoint}/recovery-plans/{plan_id}\", headers=self.headers\n-        )\n-\n-        assert response.status_code == 200, f\"Failed to delete plan: {response.text}\"\n+            f\"{self.api_endpoint}/recovery-plans/{plan_id}\",\n+            headers=self.headers,\n+        )\n+\n+        assert (\n+            response.status_code == 200\n+        ), f\"Failed to delete plan: {response.text}\"\n \n         print(f\"\u2705 Deleted Recovery Plan: {plan_id}\")\n \n     def cleanup_protection_groups(self) -> None:\n         \"\"\"Clean up created Protection Groups\"\"\"\n         for pg_id in self.created_pg_ids:\n             response = requests.delete(\n-                f\"{self.api_endpoint}/protection-groups/{pg_id}\", headers=self.headers\n+                f\"{self.api_endpoint}/protection-groups/{pg_id}\",\n+                headers=self.headers,\n             )\n             if response.status_code == 200:\n                 print(f\"\u2705 Deleted Protection Group: {pg_id}\")\n             else:\n-                print(f\"\u26a0\ufe0f  Failed to delete Protection Group {pg_id}: {response.text}\")\n+                print(\n+                    f\"\u26a0\ufe0f  Failed to delete Protection Group {pg_id}: {response.text}\"\n+                )\n \n \n @pytest.fixture(scope=\"module\")\n def auth_headers():\n     \"\"\"Authenticate and get headers for API requests\"\"\"\n     authenticator = CognitoAuthenticator()\n-    headers = authenticator.authenticate(USERNAME, PASSWORD, USER_POOL_CLIENT_ID)\n+    headers = authenticator.authenticate(\n+        USERNAME, PASSWORD, USER_POOL_CLIENT_ID\n+    )\n     return headers\n \n \n @pytest.fixture(scope=\"module\")\n def api_tester(auth_headers):\n@@ -216,13 +243,19 @@\n     print(\"\\n\" + \"=\" * 80)\n     print(\"PART 1: Create Recovery Plan with 3 Waves\")\n     print(\"=\" * 80)\n \n     # Create Protection Groups\n-    web_pg_id = api_tester.create_protection_group(f\"WebServers-{timestamp}\", 2)\n-    app_pg_id = api_tester.create_protection_group(f\"AppServers-{timestamp}\", 2)\n-    db_pg_id = api_tester.create_protection_group(f\"DatabaseServers-{timestamp}\", 2)\n+    web_pg_id = api_tester.create_protection_group(\n+        f\"WebServers-{timestamp}\", 2\n+    )\n+    app_pg_id = api_tester.create_protection_group(\n+        f\"AppServers-{timestamp}\", 2\n+    )\n+    db_pg_id = api_tester.create_protection_group(\n+        f\"DatabaseServers-{timestamp}\", 2\n+    )\n \n     # Define waves with all servers from each PG\n     waves = [\n         {\n             \"name\": \"Web\",\n@@ -273,11 +306,13 @@\n             \"launchOrder\": 2,\n         },\n         {\n             \"waveName\": \"Database\",\n             \"protectionGroupId\": db_pg_id,\n-            \"serverIds\": [\"i-databaseservers001\"],  # Removed i-databaseservers002\n+            \"serverIds\": [\n+                \"i-databaseservers001\"\n+            ],  # Removed i-databaseservers002\n             \"launchOrder\": 3,\n         },\n     ]\n \n     # Update plan\n--- scripts/generate_quality_report.py\t2026-01-02 15:55:23.777570+00:00\n+++ scripts/generate_quality_report.py\t2026-01-02 15:55:36.911871+00:00\n@@ -77,11 +77,13 @@\n \n             return {\n                 \"tool\": \"black\",\n                 \"status\": \"passed\" if result.returncode == 0 else \"failed\",\n                 \"files_checked\": len(self.python_files),\n-                \"files_needing_format\": 0 if result.returncode == 0 else \"unknown\",\n+                \"files_needing_format\": 0\n+                if result.returncode == 0\n+                else \"unknown\",\n                 \"output\": result.stdout,\n                 \"errors\": result.stderr,\n                 \"return_code\": result.returncode,\n             }\n \n@@ -120,11 +122,13 @@\n                         parts = line.split(\":\", 3)\n                         if len(parts) >= 4:\n                             violations.append(\n                                 {\n                                     \"file\": parts[0],\n-                                    \"line\": int(parts[1]) if parts[1].isdigit() else 0,\n+                                    \"line\": int(parts[1])\n+                                    if parts[1].isdigit()\n+                                    else 0,\n                                     \"column\": int(parts[2])\n                                     if parts[2].isdigit()\n                                     else 0,\n                                     \"message\": parts[3].strip(),\n                                 }\n@@ -166,11 +170,13 @@\n \n             return {\n                 \"tool\": \"isort\",\n                 \"status\": \"passed\" if result.returncode == 0 else \"failed\",\n                 \"files_checked\": len(self.python_files),\n-                \"files_needing_sort\": 0 if result.returncode == 0 else \"unknown\",\n+                \"files_needing_sort\": 0\n+                if result.returncode == 0\n+                else \"unknown\",\n                 \"output\": result.stdout,\n                 \"errors\": result.stderr,\n                 \"return_code\": result.returncode,\n             }\n \n@@ -181,33 +187,43 @@\n                 \"error\": \"isort check timed out after 60 seconds\",\n             }\n         except Exception as e:\n             return {\"tool\": \"isort\", \"status\": \"error\", \"error\": str(e)}\n \n-    def calculate_compliance_metrics(self, results: Dict[str, Any]) -> Dict[str, Any]:\n+    def calculate_compliance_metrics(\n+        self, results: Dict[str, Any]\n+    ) -> Dict[str, Any]:\n         \"\"\"Calculate overall compliance metrics.\"\"\"\n         total_files = len(self.python_files)\n \n         # Count tools that passed\n         tools_passed = sum(\n-            1 for result in results.values() if result.get(\"status\") == \"passed\"\n+            1\n+            for result in results.values()\n+            if result.get(\"status\") == \"passed\"\n         )\n         total_tools = len(results)\n \n         # Calculate compliance percentage\n-        tool_compliance = (tools_passed / total_tools * 100) if total_tools > 0 else 0\n+        tool_compliance = (\n+            (tools_passed / total_tools * 100) if total_tools > 0 else 0\n+        )\n \n         # Get Flake8 violations for detailed metrics\n-        flake8_violations = results.get(\"flake8\", {}).get(\"total_violations\", 0)\n+        flake8_violations = results.get(\"flake8\", {}).get(\n+            \"total_violations\", 0\n+        )\n \n         return {\n             \"total_files_analyzed\": total_files,\n             \"tools_run\": total_tools,\n             \"tools_passed\": tools_passed,\n             \"tool_compliance_percentage\": round(tool_compliance, 2),\n             \"flake8_violations\": flake8_violations,\n-            \"overall_status\": \"PASSED\" if tools_passed == total_tools else \"FAILED\",\n+            \"overall_status\": \"PASSED\"\n+            if tools_passed == total_tools\n+            else \"FAILED\",\n             \"timestamp\": self.timestamp.isoformat(),\n             \"files_analyzed\": [str(f) for f in self.python_files],\n         }\n \n     def generate_json_report(\n@@ -315,35 +331,43 @@\n \n         # Generate tool sections\n         tool_sections = []\n         for tool_name, result in results.items():\n             status_class = (\n-                \"status-passed\" if result.get(\"status\") == \"passed\" else \"status-failed\"\n+                \"status-passed\"\n+                if result.get(\"status\") == \"passed\"\n+                else \"status-failed\"\n             )\n             status_text = result.get(\"status\", \"unknown\").upper()\n \n             content = f'<div class=\"tool-content\">'\n             content += f'<p><strong>Status:</strong> <span class=\"{status_class}\">{status_text}</span></p>'\n \n             if tool_name == \"flake8\" and result.get(\"violations\"):\n                 content += f'<p><strong>Total Violations:</strong> {len(result[\"violations\"])}</p>'\n                 content += '<div class=\"violations\">'\n-                for violation in result[\"violations\"][:10]:  # Show first 10 violations\n+                for violation in result[\"violations\"][\n+                    :10\n+                ]:  # Show first 10 violations\n                     content += f\"\"\"\n                     <div class=\"violation\">\n                         <div class=\"violation-file\">{violation[\"file\"]}:{violation[\"line\"]}:{violation[\"column\"]}</div>\n                         <div class=\"violation-message\">{violation[\"message\"]}</div>\n                     </div>\n                     \"\"\"\n                 if len(result[\"violations\"]) > 10:\n                     content += f'<p><em>... and {len(result[\"violations\"]) - 10} more violations</em></p>'\n                 content += \"</div>\"\n             elif result.get(\"status\") == \"passed\":\n-                content += '<div class=\"no-violations\">\u2713 No violations found</div>'\n+                content += (\n+                    '<div class=\"no-violations\">\u2713 No violations found</div>'\n+                )\n \n             if result.get(\"output\") and result.get(\"status\") != \"passed\":\n-                content += f'<h4>Output:</h4><pre>{result[\"output\"][:1000]}</pre>'\n+                content += (\n+                    f'<h4>Output:</h4><pre>{result[\"output\"][:1000]}</pre>'\n+                )\n \n             content += \"</div>\"\n \n             tool_sections.append(\n                 f\"\"\"\n@@ -353,11 +377,13 @@\n             </div>\n             \"\"\"\n             )\n \n         # Generate files list\n-        files_list = \"\\n\".join(f\"<li>{f}</li>\" for f in metrics[\"files_analyzed\"])\n+        files_list = \"\\n\".join(\n+            f\"<li>{f}</li>\" for f in metrics[\"files_analyzed\"]\n+        )\n \n         # Fill template\n         html_content = html_template.format(\n             timestamp=self.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n             overall_status=metrics[\"overall_status\"],\n@@ -441,14 +467,18 @@\n \n         # Generate reports\n         generated_files = {}\n \n         if \"json\" in formats:\n-            generated_files[\"json\"] = self.generate_json_report(results, metrics)\n+            generated_files[\"json\"] = self.generate_json_report(\n+                results, metrics\n+            )\n \n         if \"html\" in formats:\n-            generated_files[\"html\"] = self.generate_html_report(results, metrics)\n+            generated_files[\"html\"] = self.generate_html_report(\n+                results, metrics\n+            )\n \n         # Print summary\n         print(f\"\\n=== Quality Report Summary ===\")\n         print(f\"Overall Status: {metrics['overall_status']}\")\n         print(f\"Tool Compliance: {metrics['tool_compliance_percentage']}%\")\n@@ -480,11 +510,13 @@\n     formats = [f.strip().lower() for f in args.format.split(\",\")]\n     valid_formats = [\"json\", \"html\"]\n     formats = [f for f in formats if f in valid_formats]\n \n     if not formats:\n-        print(\"Error: No valid formats specified. Use 'json', 'html', or 'json,html'\")\n+        print(\n+            \"Error: No valid formats specified. Use 'json', 'html', or 'json,html'\"\n+        )\n         sys.exit(1)\n \n     # Generate reports\n     try:\n         reporter = QualityReporter(args.output_dir)\n--- tests/python/monitor_drs_drill.py\t2026-01-02 15:55:23.585032+00:00\n+++ tests/python/monitor_drs_drill.py\t2026-01-02 15:55:36.919336+00:00\n@@ -53,11 +53,13 @@\n             job = response[\"items\"][0]\n             job_status = job.get(\"status\")\n \n             # Print status\n             timestamp = datetime.now().strftime(\"%H:%M:%S\")\n-            print(f\"[{timestamp}] Poll #{poll_count} (T+{elapsed}s): {job_status}\")\n+            print(\n+                f\"[{timestamp}] Poll #{poll_count} (T+{elapsed}s): {job_status}\"\n+            )\n \n             # Show servers\n             servers = job.get(\"participatingServers\", [])\n             for server in servers:\n                 srv_id = server.get(\"sourceServerID\")\n--- tests/python/mocks/mock_drs_client.py\t2026-01-02 15:55:23.615075+00:00\n+++ tests/python/mocks/mock_drs_client.py\t2026-01-02 15:55:36.924488+00:00\n@@ -95,18 +95,24 @@\n                     \"dataReplicationInitiation\": {\n                         \"startDateTime\": \"2025-01-01T00:00:00Z\"\n                     },\n                 },\n                 \"lastLaunchResult\": \"NOT_STARTED\",\n-                \"lifeCycle\": {\"addedToServiceDateTime\": \"2025-01-01T00:00:00Z\"},\n+                \"lifeCycle\": {\n+                    \"addedToServiceDateTime\": \"2025-01-01T00:00:00Z\"\n+                },\n                 \"replicationDirection\": \"FAILOVER\",\n                 \"sourceProperties\": {\n                     \"cpus\": [{\"cores\": 2}],\n                     \"disks\": [{\"bytes\": 107374182400}],\n-                    \"identificationHints\": {\"hostname\": f\"server-{server_id[-8:]}\"},\n+                    \"identificationHints\": {\n+                        \"hostname\": f\"server-{server_id[-8:]}\"\n+                    },\n                     \"lastUpdatedDateTime\": \"2025-01-01T00:00:00Z\",\n-                    \"networkInterfaces\": [{\"ips\": [\"10.0.1.100\"], \"isPrimary\": True}],\n+                    \"networkInterfaces\": [\n+                        {\"ips\": [\"10.0.1.100\"], \"isPrimary\": True}\n+                    ],\n                     \"os\": {\"fullString\": \"Microsoft Windows Server 2019\"},\n                     \"ramBytes\": 8589934592,\n                     \"recommendedInstanceType\": \"t3.large\",\n                 },\n             }\n@@ -123,11 +129,13 @@\n         \"\"\"Simulate random errors based on error_rate.\"\"\"\n         if self.simulate_errors and self.error_rate > 0:\n             import random\n \n             if random.random() < self.error_rate:\n-                raise ServiceUnavailableException(\"Service temporarily unavailable\")\n+                raise ServiceUnavailableException(\n+                    \"Service temporarily unavailable\"\n+                )\n \n     def describe_source_servers(\n         self,\n         filters: Optional[Dict[str, List[str]]] = None,\n         maxResults: int = 200,\n@@ -159,11 +167,14 @@\n             items = list(self.source_servers.values())\n \n         # Apply pagination\n         items = items[:maxResults]\n \n-        return {\"items\": items, \"nextToken\": None}  # Simplified: no pagination in mock\n+        return {\n+            \"items\": items,\n+            \"nextToken\": None,\n+        }  # Simplified: no pagination in mock\n \n     def start_recovery(\n         self,\n         sourceServers: List[Dict[str, str]],\n         isDrill: bool = False,\n@@ -187,11 +198,13 @@\n         for server in sourceServers:\n             server_id = server.get(\"sourceServerID\")\n             recovery_snapshot_id = server.get(\"recoverySnapshotID\", \"LATEST\")\n \n             if server_id not in self.source_servers:\n-                raise ResourceNotFoundException(f\"Source server not found: {server_id}\")\n+                raise ResourceNotFoundException(\n+                    f\"Source server not found: {server_id}\"\n+                )\n \n             if recovery_snapshot_id != \"LATEST\":\n                 # In real DRS, you can specify snapshot IDs\n                 # For mock, we only support LATEST\n                 pass\n@@ -231,11 +244,13 @@\n         Simulate job status transitions over time.\n         In real implementation, this would happen asynchronously.\n         For testing, we'll track transition times.\n         \"\"\"\n         job = self.recovery_jobs[job_id]\n-        creation_time = datetime.fromisoformat(job[\"creationDateTime\"].replace(\"Z\", \"\"))\n+        creation_time = datetime.fromisoformat(\n+            job[\"creationDateTime\"].replace(\"Z\", \"\")\n+        )\n \n         # Store transition schedule\n         job[\"_transitions\"] = {\n             \"in_progress_at\": creation_time + timedelta(seconds=5),\n             \"completed_at\": creation_time + timedelta(seconds=30),\n@@ -266,18 +281,21 @@\n \n         # Filter jobs if requested\n         if filters and \"jobIDs\" in filters:\n             job_ids = filters[\"jobIDs\"]\n             items = [\n-                self.recovery_jobs[jid] for jid in job_ids if jid in self.recovery_jobs\n+                self.recovery_jobs[jid]\n+                for jid in job_ids\n+                if jid in self.recovery_jobs\n             ]\n         else:\n             items = list(self.recovery_jobs.values())\n \n         # Remove internal fields\n         items = [\n-            {k: v for k, v in job.items() if not k.startswith(\"_\")} for job in items\n+            {k: v for k, v in job.items() if not k.startswith(\"_\")}\n+            for job in items\n         ]\n \n         # Apply pagination\n         items = items[:maxResults]\n \n@@ -292,19 +310,25 @@\n                 continue\n \n             transitions = job[\"_transitions\"]\n \n             # Transition to IN_PROGRESS\n-            if job[\"status\"] == \"PENDING\" and now >= transitions[\"in_progress_at\"]:\n+            if (\n+                job[\"status\"] == \"PENDING\"\n+                and now >= transitions[\"in_progress_at\"]\n+            ):\n                 job[\"status\"] = \"IN_PROGRESS\"\n                 for server in job[\"participatingServers\"]:\n                     server[\"launchStatus\"] = \"IN_PROGRESS\"\n                     # Generate mock recovery instance ID\n                     server[\"recoveryInstanceID\"] = f\"i-{uuid.uuid4().hex[:17]}\"\n \n             # Transition to COMPLETED\n-            if job[\"status\"] == \"IN_PROGRESS\" and now >= transitions[\"completed_at\"]:\n+            if (\n+                job[\"status\"] == \"IN_PROGRESS\"\n+                and now >= transitions[\"completed_at\"]\n+            ):\n                 job[\"status\"] = \"COMPLETED\"\n                 job[\"endDateTime\"] = now.isoformat() + \"Z\"\n                 for server in job[\"participatingServers\"]:\n                     server[\"launchStatus\"] = \"LAUNCHED\"\n \n@@ -360,11 +384,13 @@\n     def remove_source_server(self, server_id: str):\n         \"\"\"Remove a source server from the mock (for testing).\"\"\"\n         if server_id in self.source_servers:\n             del self.source_servers[server_id]\n \n-    def simulate_job_failure(self, job_id: str, error_message: str = \"Recovery failed\"):\n+    def simulate_job_failure(\n+        self, job_id: str, error_message: str = \"Recovery failed\"\n+    ):\n         \"\"\"\n         Simulate a job failure (for testing error scenarios).\n \n         Args:\n             job_id: Job ID to fail\n--- tests/python/standalone_drs_drill.py\t2026-01-02 15:55:23.593580+00:00\n+++ tests/python/standalone_drs_drill.py\t2026-01-02 15:55:36.941380+00:00\n@@ -52,11 +52,13 @@\n             )\n             latest_snapshot = None\n \n         # Step 3: Start the recovery drill\n         print(\"\\nStep 3: Starting recovery drill...\")\n-        drill_job = start_recovery_drill(drs_client, source_server_id, latest_snapshot)\n+        drill_job = start_recovery_drill(\n+            drs_client, source_server_id, latest_snapshot\n+        )\n \n         if not drill_job:\n             print(\"Failed to start recovery drill\")\n             return None\n \n@@ -64,24 +66,32 @@\n         print(f\"Drill job started successfully!\")\n         print(f\"Job ID: {job_id}\")\n \n         # Step 4: Monitor the drill progress\n         print(\"\\nStep 4: Monitoring drill progress...\")\n-        recovery_instance = monitor_drill_progress(drs_client, job_id, source_server_id)\n+        recovery_instance = monitor_drill_progress(\n+            drs_client, job_id, source_server_id\n+        )\n \n         if recovery_instance:\n             print(f\"\\nDrill completed successfully!\")\n-            print(f\"Recovery Instance ID: {recovery_instance['recoveryInstanceID']}\")\n+            print(\n+                f\"Recovery Instance ID: {recovery_instance['recoveryInstanceID']}\"\n+            )\n             print(f\"EC2 Instance ID: {recovery_instance['ec2InstanceID']}\")\n-            print(f\"EC2 Instance State: {recovery_instance['ec2InstanceState']}\")\n+            print(\n+                f\"EC2 Instance State: {recovery_instance['ec2InstanceState']}\"\n+            )\n \n             # Step 5: Provide cleanup instructions\n             print_cleanup_instructions(recovery_instance[\"recoveryInstanceID\"])\n \n             return {\n                 \"job_id\": job_id,\n-                \"recovery_instance_id\": recovery_instance[\"recoveryInstanceID\"],\n+                \"recovery_instance_id\": recovery_instance[\n+                    \"recoveryInstanceID\"\n+                ],\n                 \"ec2_instance_id\": recovery_instance[\"ec2InstanceID\"],\n             }\n         else:\n             print(\"Drill failed or timed out\")\n             return None\n@@ -124,11 +134,13 @@\n         response = drs_client.describe_recovery_snapshots(\n             sourceServerID=source_server_id\n         )\n \n         # Filter to only completed snapshots (with timestamp)\n-        completed_snapshots = [s for s in response[\"items\"] if \"timestamp\" in s]\n+        completed_snapshots = [\n+            s for s in response[\"items\"] if \"timestamp\" in s\n+        ]\n \n         # Sort by timestamp (most recent first)\n         snapshots = sorted(\n             completed_snapshots, key=lambda x: x[\"timestamp\"], reverse=True\n         )\n@@ -148,11 +160,13 @@\n         # Prepare source server configuration\n         source_server_config = {\"sourceServerID\": source_server_id}\n \n         # Add recovery snapshot if available\n         if recovery_snapshot:\n-            source_server_config[\"recoverySnapshotID\"] = recovery_snapshot[\"snapshotID\"]\n+            source_server_config[\"recoverySnapshotID\"] = recovery_snapshot[\n+                \"snapshotID\"\n+            ]\n \n         # Start the recovery drill\n         response = drs_client.start_recovery(\n             sourceServers=[source_server_config],\n             isDrill=True,  # This is the key parameter for drill mode\n@@ -168,11 +182,13 @@\n     except ClientError as e:\n         print(f\"Error starting recovery drill: {e}\")\n         return None\n \n \n-def monitor_drill_progress(drs_client, job_id, source_server_id, timeout_minutes=30):\n+def monitor_drill_progress(\n+    drs_client, job_id, source_server_id, timeout_minutes=30\n+):\n     \"\"\"\n     Monitor the drill progress and wait for completion\n     \"\"\"\n     start_time = time.time()\n     timeout_seconds = timeout_minutes * 60\n@@ -180,11 +196,13 @@\n     print(f\"Monitoring job {job_id} (timeout: {timeout_minutes} minutes)\")\n \n     while time.time() - start_time < timeout_seconds:\n         try:\n             # Check job status\n-            job_response = drs_client.describe_jobs(filters={\"jobIDs\": [job_id]})\n+            job_response = drs_client.describe_jobs(\n+                filters={\"jobIDs\": [job_id]}\n+            )\n \n             if job_response[\"items\"]:\n                 job = job_response[\"items\"][0]\n                 status = job[\"status\"]\n \n@@ -200,23 +218,27 @@\n                         )\n \n                 if status == \"COMPLETED\":\n                     # Get recovery instance information\n                     print(\"\\nJob completed! Looking for recovery instance...\")\n-                    recovery_instances = drs_client.describe_recovery_instances(\n-                        filters={\"sourceServerIDs\": [source_server_id]}\n+                    recovery_instances = (\n+                        drs_client.describe_recovery_instances(\n+                            filters={\"sourceServerIDs\": [source_server_id]}\n+                        )\n                     )\n \n                     # Find the drill instance\n                     for instance in recovery_instances[\"items\"]:\n                         if (\n                             instance.get(\"isDrill\", False)\n                             and instance.get(\"jobID\") == job_id\n                         ):\n                             return instance\n \n-                    print(\"Warning: Job completed but no recovery instance found\")\n+                    print(\n+                        \"Warning: Job completed but no recovery instance found\"\n+                    )\n                     return None\n \n                 elif status in [\"FAILED\", \"TERMINATED\"]:\n                     print(f\"Job failed with status: {status}\")\n \n@@ -249,11 +271,13 @@\n     \"\"\"\n     print(\"\\n\" + \"=\" * 60)\n     print(\"IMPORTANT: CLEANUP INSTRUCTIONS\")\n     print(\"=\" * 60)\n     print(\"The drill has created recovery instances that will incur charges.\")\n-    print(\"Remember to terminate the drill instances when testing is complete.\")\n+    print(\n+        \"Remember to terminate the drill instances when testing is complete.\"\n+    )\n     print()\n     print(\"To terminate the drill instance:\")\n     print(f\"1. Via AWS CLI:\")\n     print(\n         f\"   aws drs terminate-recovery-instances --recovery-instance-ids {recovery_instance_id}\"\n--- tests/python/fixtures/recovery_plan_fixtures.py\t2026-01-02 15:55:23.664520+00:00\n+++ tests/python/fixtures/recovery_plan_fixtures.py\t2026-01-02 15:55:36.942755+00:00\n@@ -55,31 +55,37 @@\n         \"Waves\": [\n             {\n                 \"WaveNumber\": 1,\n                 \"WaveName\": \"Database Tier\",\n                 \"WaveDescription\": \"Database servers\",\n-                \"ServerIds\": [f\"s-db{i:03d}\" for i in range(1, servers_per_wave + 1)],\n+                \"ServerIds\": [\n+                    f\"s-db{i:03d}\" for i in range(1, servers_per_wave + 1)\n+                ],\n                 \"ExecutionType\": \"SEQUENTIAL\",\n                 \"ExecutionOrder\": list(range(1, servers_per_wave + 1)),\n                 \"Dependencies\": [],\n                 \"WaitTimeSeconds\": 60,\n             },\n             {\n                 \"WaveNumber\": 2,\n                 \"WaveName\": \"Application Tier\",\n                 \"WaveDescription\": \"Application servers\",\n-                \"ServerIds\": [f\"s-app{i:03d}\" for i in range(1, servers_per_wave + 1)],\n+                \"ServerIds\": [\n+                    f\"s-app{i:03d}\" for i in range(1, servers_per_wave + 1)\n+                ],\n                 \"ExecutionType\": \"PARALLEL\",\n                 \"ExecutionOrder\": [1] * servers_per_wave,\n                 \"Dependencies\": [\"Wave-1\"],\n                 \"WaitTimeSeconds\": 30,\n             },\n             {\n                 \"WaveNumber\": 3,\n                 \"WaveName\": \"Web Tier\",\n                 \"WaveDescription\": \"Web servers\",\n-                \"ServerIds\": [f\"s-web{i:03d}\" for i in range(1, servers_per_wave + 1)],\n+                \"ServerIds\": [\n+                    f\"s-web{i:03d}\" for i in range(1, servers_per_wave + 1)\n+                ],\n                 \"ExecutionType\": \"PARALLEL\",\n                 \"ExecutionOrder\": [1] * servers_per_wave,\n                 \"Dependencies\": [\"Wave-2\"],\n                 \"WaitTimeSeconds\": 0,\n             },\n@@ -105,11 +111,13 @@\n         \"Waves\": [\n             {\n                 \"WaveNumber\": 1,\n                 \"WaveName\": \"Parallel Wave\",\n                 \"WaveDescription\": \"All servers launch simultaneously\",\n-                \"ServerIds\": [f\"s-par{i:03d}\" for i in range(1, server_count + 1)],\n+                \"ServerIds\": [\n+                    f\"s-par{i:03d}\" for i in range(1, server_count + 1)\n+                ],\n                 \"ExecutionType\": \"PARALLEL\",\n                 \"ExecutionOrder\": [1] * server_count,\n                 \"Dependencies\": [],\n                 \"WaitTimeSeconds\": 0,\n             }\n@@ -135,51 +143,61 @@\n         \"Waves\": [\n             {\n                 \"WaveNumber\": 1,\n                 \"WaveName\": \"Foundation\",\n                 \"WaveDescription\": \"Core infrastructure\",\n-                \"ServerIds\": [f\"s-fnd{i:03d}\" for i in range(1, servers_per_wave + 1)],\n+                \"ServerIds\": [\n+                    f\"s-fnd{i:03d}\" for i in range(1, servers_per_wave + 1)\n+                ],\n                 \"ExecutionType\": \"SEQUENTIAL\",\n                 \"ExecutionOrder\": list(range(1, servers_per_wave + 1)),\n                 \"Dependencies\": [],\n                 \"WaitTimeSeconds\": 30,\n             },\n             {\n                 \"WaveNumber\": 2,\n                 \"WaveName\": \"Database\",\n                 \"WaveDescription\": \"Database tier\",\n-                \"ServerIds\": [f\"s-db{i:03d}\" for i in range(1, servers_per_wave + 1)],\n+                \"ServerIds\": [\n+                    f\"s-db{i:03d}\" for i in range(1, servers_per_wave + 1)\n+                ],\n                 \"ExecutionType\": \"SEQUENTIAL\",\n                 \"ExecutionOrder\": list(range(1, servers_per_wave + 1)),\n                 \"Dependencies\": [\"Wave-1\"],\n                 \"WaitTimeSeconds\": 60,\n             },\n             {\n                 \"WaveNumber\": 3,\n                 \"WaveName\": \"Application\",\n                 \"WaveDescription\": \"Application tier\",\n-                \"ServerIds\": [f\"s-app{i:03d}\" for i in range(1, servers_per_wave + 1)],\n+                \"ServerIds\": [\n+                    f\"s-app{i:03d}\" for i in range(1, servers_per_wave + 1)\n+                ],\n                 \"ExecutionType\": \"PARALLEL\",\n                 \"ExecutionOrder\": [1] * servers_per_wave,\n                 \"Dependencies\": [\"Wave-2\"],\n                 \"WaitTimeSeconds\": 45,\n             },\n             {\n                 \"WaveNumber\": 4,\n                 \"WaveName\": \"Web\",\n                 \"WaveDescription\": \"Web tier\",\n-                \"ServerIds\": [f\"s-web{i:03d}\" for i in range(1, servers_per_wave + 1)],\n+                \"ServerIds\": [\n+                    f\"s-web{i:03d}\" for i in range(1, servers_per_wave + 1)\n+                ],\n                 \"ExecutionType\": \"PARALLEL\",\n                 \"ExecutionOrder\": [1] * servers_per_wave,\n                 \"Dependencies\": [\"Wave-3\"],\n                 \"WaitTimeSeconds\": 30,\n             },\n             {\n                 \"WaveNumber\": 5,\n                 \"WaveName\": \"Edge\",\n                 \"WaveDescription\": \"Edge services and CDN\",\n-                \"ServerIds\": [f\"s-edg{i:03d}\" for i in range(1, servers_per_wave + 1)],\n+                \"ServerIds\": [\n+                    f\"s-edg{i:03d}\" for i in range(1, servers_per_wave + 1)\n+                ],\n                 \"ExecutionType\": \"PARALLEL\",\n                 \"ExecutionOrder\": [1] * servers_per_wave,\n                 \"Dependencies\": [\"Wave-4\"],\n                 \"WaitTimeSeconds\": 0,\n             },\n@@ -187,11 +205,13 @@\n         \"CreatedDate\": int(time.time()),\n         \"LastModifiedDate\": int(time.time()),\n     }\n \n \n-def create_mixed_execution_plan(plan_id: str = \"test-plan-mixed\") -> Dict[str, Any]:\n+def create_mixed_execution_plan(\n+    plan_id: str = \"test-plan-mixed\",\n+) -> Dict[str, Any]:\n     \"\"\"Create a plan with mixed sequential and parallel execution types.\"\"\"\n     return {\n         \"PlanId\": plan_id,\n         \"PlanName\": \"Mixed Execution Plan\",\n         \"Description\": \"Plan with both sequential and parallel waves\",\n@@ -309,11 +329,13 @@\n         \"CreatedDate\": int(time.time()),\n         \"LastModifiedDate\": int(time.time()),\n     }\n \n \n-def create_real_drs_server_plan(plan_id: str = \"test-plan-real-drs\") -> Dict[str, Any]:\n+def create_real_drs_server_plan(\n+    plan_id: str = \"test-plan-real-drs\",\n+) -> Dict[str, Any]:\n     \"\"\"Create a plan using real DRS server IDs from TEST account.\"\"\"\n     return {\n         \"PlanId\": plan_id,\n         \"PlanName\": \"Real DRS Servers Plan\",\n         \"Description\": \"Plan using actual DRS servers for integration testing\",\n@@ -386,7 +408,9 @@\n         \"wait_times\": create_plan_with_wait_times,\n         \"real_drs\": create_real_drs_server_plan,\n     }\n \n     if name not in fixtures:\n-        raise ValueError(f\"Unknown fixture: {name}. Available: {list(fixtures.keys())}\")\n+        raise ValueError(\n+            f\"Unknown fixture: {name}. Available: {list(fixtures.keys())}\"\n+        )\n     return fixtures[name](**kwargs)\n--- tests/python/e2e/test_recovery_plan_e2e.py\t2026-01-02 15:55:23.679883+00:00\n+++ tests/python/e2e/test_recovery_plan_e2e.py\t2026-01-02 15:55:36.937806+00:00\n@@ -45,11 +45,13 @@\n     print(\"=\" * 60)\n \n     # Get Protection Groups first\n     print(\"\\n\ud83d\udccb Step 1: Fetching Protection Groups...\")\n     headers = {\"Authorization\": f\"Bearer {token}\"}\n-    pg_response = requests.get(f\"{API_ENDPOINT}/protection-groups\", headers=headers)\n+    pg_response = requests.get(\n+        f\"{API_ENDPOINT}/protection-groups\", headers=headers\n+    )\n \n     if pg_response.status_code != 200:\n         print(f\"\u274c Failed to get Protection Groups: {pg_response.status_code}\")\n         return None, False\n \n@@ -167,40 +169,48 @@\n                 \"WaveId\": \"wave-0\",\n                 \"WaveName\": \"Web\",\n                 \"WaveDescription\": \"Web servers wave (1 server)\",\n                 \"ExecutionOrder\": 0,\n                 \"ProtectionGroupId\": web_pg[\"protectionGroupId\"],\n-                \"ServerIds\": [web_pg[\"sourceServerIds\"][0]],  # Only first server\n+                \"ServerIds\": [\n+                    web_pg[\"sourceServerIds\"][0]\n+                ],  # Only first server\n                 \"ExecutionType\": \"sequential\",\n                 \"Dependencies\": [],\n             },\n             {\n                 \"WaveId\": \"wave-1\",\n                 \"WaveName\": \"App\",\n                 \"WaveDescription\": \"Application servers wave (1 server)\",\n                 \"ExecutionOrder\": 1,\n                 \"ProtectionGroupId\": app_pg[\"protectionGroupId\"],\n-                \"ServerIds\": [app_pg[\"sourceServerIds\"][0]],  # Only first server\n+                \"ServerIds\": [\n+                    app_pg[\"sourceServerIds\"][0]\n+                ],  # Only first server\n                 \"ExecutionType\": \"sequential\",\n                 \"Dependencies\": [{\"DependsOnWaveId\": \"wave-0\"}],\n             },\n             {\n                 \"WaveId\": \"wave-2\",\n                 \"WaveName\": \"Database\",\n                 \"WaveDescription\": \"Database servers wave (1 server)\",\n                 \"ExecutionOrder\": 2,\n                 \"ProtectionGroupId\": db_pg[\"protectionGroupId\"],\n-                \"ServerIds\": [db_pg[\"sourceServerIds\"][0]],  # Only first server\n+                \"ServerIds\": [\n+                    db_pg[\"sourceServerIds\"][0]\n+                ],  # Only first server\n                 \"ExecutionType\": \"sequential\",\n                 \"Dependencies\": [{\"DependsOnWaveId\": \"wave-1\"}],\n             },\n         ],\n     }\n \n     headers = {\"Authorization\": f\"Bearer {token}\"}\n     update_response = requests.put(\n-        f\"{API_ENDPOINT}/recovery-plans/{plan_id}\", headers=headers, json=update_payload\n+        f\"{API_ENDPOINT}/recovery-plans/{plan_id}\",\n+        headers=headers,\n+        json=update_payload,\n     )\n \n     if update_response.status_code != 200:\n         print(f\"\u274c Failed to update plan: {update_response.status_code}\")\n         print(f\"   Response: {update_response.text}\")\n--- tests/python/unit/test_infrastructure_smoke.py\t2026-01-02 15:55:23.628037+00:00\n+++ tests/python/unit/test_infrastructure_smoke.py\t2026-01-02 15:55:36.943972+00:00\n@@ -40,9 +40,11 @@\n     with mock_dynamodb():\n         dynamodb = boto3.resource(\"dynamodb\", region_name=\"us-east-1\")\n         table = dynamodb.create_table(\n             TableName=\"test-table\",\n             KeySchema=[{\"AttributeName\": \"id\", \"KeyType\": \"HASH\"}],\n-            AttributeDefinitions=[{\"AttributeName\": \"id\", \"AttributeType\": \"S\"}],\n+            AttributeDefinitions=[\n+                {\"AttributeName\": \"id\", \"AttributeType\": \"S\"}\n+            ],\n             BillingMode=\"PAY_PER_REQUEST\",\n         )\n         assert table.table_name == \"test-table\"\n--- tests/python/test_with_lambda_invoke.py\t2026-01-02 15:55:23.600262+00:00\n+++ tests/python/test_with_lambda_invoke.py\t2026-01-02 15:55:36.947024+00:00\n@@ -64,11 +64,13 @@\n             logger.info(f\"Waves: {len(waves)}\")\n \n             for wave in waves:\n                 job_id = wave.get(\"JobId\", \"NO JOB ID\")\n                 status = wave.get(\"Status\", \"UNKNOWN\")\n-                logger.info(f\"  Wave {wave.get('WaveId')}: {status} (Job: {job_id})\")\n+                logger.info(\n+                    f\"  Wave {wave.get('WaveId')}: {status} (Job: {job_id})\"\n+                )\n \n             logger.info(\"\\n\u2705 TEST COMPLETED\")\n \n             return {\n                 \"success\": True,\n@@ -106,19 +108,24 @@\n         if not execution_id:\n             raise Exception(f\"No ExecutionId in response: {body}\")\n \n         return execution_id\n \n-    def _monitor_execution(self, execution_id: str, timeout: int = 300) -> dict:\n+    def _monitor_execution(\n+        self, execution_id: str, timeout: int = 300\n+    ) -> dict:\n         \"\"\"Monitor execution until complete or timeout.\"\"\"\n         start_time = time.time()\n         last_status = None\n \n         while time.time() - start_time < timeout:\n             response = self.dynamodb.get_item(\n                 TableName=self.execution_table,\n-                Key={\"ExecutionId\": {\"S\": execution_id}, \"PlanId\": {\"S\": self.plan_id}},\n+                Key={\n+                    \"ExecutionId\": {\"S\": execution_id},\n+                    \"PlanId\": {\"S\": self.plan_id},\n+                },\n             )\n \n             if \"Item\" not in response:\n                 logger.warning(\"Execution not found, waiting...\")\n                 time.sleep(5)\n@@ -149,11 +156,13 @@\n             job_id = wave.get(\"JobId\")\n \n             if job_id:\n                 logger.info(f\"  \u2705 Wave {wave_id}: JobId = {job_id}\")\n             else:\n-                logger.warning(f\"  \u26a0\ufe0f  Wave {wave_id}: NO JobId (BUG 1 present!)\")\n+                logger.warning(\n+                    f\"  \u26a0\ufe0f  Wave {wave_id}: NO JobId (BUG 1 present!)\"\n+                )\n \n         return waves\n \n     def _check_drs_jobs(self, waves: list) -> dict:\n         \"\"\"Check DRS job status.\"\"\"\n--- lambda/rbac_middleware.py\t2026-01-02 15:52:46.243159+00:00\n+++ lambda/rbac_middleware.py\t2026-01-02 15:55:36.944000+00:00\n@@ -26,15 +26,17 @@\n         \"DRSOrchestrationAdmin\"  # Full admin access to orchestration platform\n     )\n     DRS_RECOVERY_MANAGER = (\n         \"DRSRecoveryManager\"  # Can execute and manage all recovery operations\n     )\n-    DRS_PLAN_MANAGER = (\n-        \"DRSPlanManager\"  # Can create/modify recovery plans and protection groups\n+    DRS_PLAN_MANAGER = \"DRSPlanManager\"  # Can create/modify recovery plans and protection groups\n+    DRS_OPERATOR = (\n+        \"DRSOperator\"  # Can execute recovery operations but not modify plans\n     )\n-    DRS_OPERATOR = \"DRSOperator\"  # Can execute recovery operations but not modify plans\n-    DRS_READ_ONLY = \"DRSReadOnly\"  # View-only access for monitoring and compliance\n+    DRS_READ_ONLY = (\n+        \"DRSReadOnly\"  # View-only access for monitoring and compliance\n+    )\n \n     # Legacy AWS-style aliases for security test compatibility\n     AWS_ADMIN = \"aws:admin\"\n     AWS_ADMIN_LIMITED = \"aws:admin-limited\"\n     AWS_POWER_USER = \"aws:power-user\"\n@@ -178,12 +180,16 @@\n ENDPOINT_PERMISSIONS = {\n     # Protection Groups\n     (\"GET\", \"/protection-groups\"): [DRSPermission.VIEW_PROTECTION_GROUPS],\n     (\"POST\", \"/protection-groups\"): [DRSPermission.CREATE_PROTECTION_GROUPS],\n     (\"GET\", \"/protection-groups/{id}\"): [DRSPermission.VIEW_PROTECTION_GROUPS],\n-    (\"PUT\", \"/protection-groups/{id}\"): [DRSPermission.MODIFY_PROTECTION_GROUPS],\n-    (\"DELETE\", \"/protection-groups/{id}\"): [DRSPermission.DELETE_PROTECTION_GROUPS],\n+    (\"PUT\", \"/protection-groups/{id}\"): [\n+        DRSPermission.MODIFY_PROTECTION_GROUPS\n+    ],\n+    (\"DELETE\", \"/protection-groups/{id}\"): [\n+        DRSPermission.DELETE_PROTECTION_GROUPS\n+    ],\n     (\"POST\", \"/protection-groups/{id}\"): [\n         DRSPermission.MODIFY_PROTECTION_GROUPS\n     ],  # resolve endpoint\n     # Recovery Plans\n     (\"GET\", \"/recovery-plans\"): [DRSPermission.VIEW_RECOVERY_PLANS],\n@@ -199,17 +205,23 @@\n     (\"GET\", \"/executions\"): [DRSPermission.VIEW_EXECUTIONS],\n     (\"POST\", \"/executions\"): [DRSPermission.START_RECOVERY],\n     (\"DELETE\", \"/executions\"): [DRSPermission.STOP_RECOVERY],\n     (\"POST\", \"/executions/delete\"): [DRSPermission.STOP_RECOVERY],\n     (\"GET\", \"/executions/{executionId}\"): [DRSPermission.VIEW_EXECUTIONS],\n-    (\"POST\", \"/executions/{executionId}/cancel\"): [DRSPermission.STOP_RECOVERY],\n+    (\"POST\", \"/executions/{executionId}/cancel\"): [\n+        DRSPermission.STOP_RECOVERY\n+    ],\n     (\"POST\", \"/executions/{executionId}/pause\"): [DRSPermission.STOP_RECOVERY],\n-    (\"POST\", \"/executions/{executionId}/resume\"): [DRSPermission.START_RECOVERY],\n+    (\"POST\", \"/executions/{executionId}/resume\"): [\n+        DRSPermission.START_RECOVERY\n+    ],\n     (\"POST\", \"/executions/{executionId}/terminate-instances\"): [\n         DRSPermission.TERMINATE_INSTANCES\n     ],\n-    (\"GET\", \"/executions/{executionId}/job-logs\"): [DRSPermission.VIEW_EXECUTIONS],\n+    (\"GET\", \"/executions/{executionId}/job-logs\"): [\n+        DRSPermission.VIEW_EXECUTIONS\n+    ],\n     (\"GET\", \"/executions/{executionId}/termination-status\"): [\n         DRSPermission.VIEW_EXECUTIONS\n     ],\n     # Account Management (CRITICAL - these were missing!)\n     (\"GET\", \"/accounts/targets\"): [DRSPermission.VIEW_ACCOUNTS],\n@@ -336,11 +348,13 @@\n     \"\"\"Check if user has a specific permission\"\"\"\n     user_permissions = get_user_permissions(user)\n     return required_permission in user_permissions\n \n \n-def has_any_permission(user: Dict, required_permissions: List[DRSPermission]) -> bool:\n+def has_any_permission(\n+    user: Dict, required_permissions: List[DRSPermission]\n+) -> bool:\n     \"\"\"Check if user has any of the required permissions\"\"\"\n     user_permissions = get_user_permissions(user)\n     return any(perm in user_permissions for perm in required_permissions)\n \n \n@@ -350,11 +364,13 @@\n     normalized_path = path\n \n     # Replace common path parameters\n     import re\n \n-    normalized_path = re.sub(r\"/[a-f0-9-]{36}\", \"/{id}\", normalized_path)  # UUIDs\n+    normalized_path = re.sub(\n+        r\"/[a-f0-9-]{36}\", \"/{id}\", normalized_path\n+    )  # UUIDs\n     normalized_path = re.sub(\n         r\"/[a-zA-Z0-9-]+(?=/|$)\", \"/{id}\", normalized_path\n     )  # Generic IDs\n \n     # Handle specific patterns\n@@ -406,11 +422,13 @@\n         # Get required permissions for this endpoint\n         required_permissions = get_endpoint_permissions(method, path)\n \n         # If no specific permissions required, allow access (fallback)\n         if not required_permissions:\n-            print(f\"\u26a0\ufe0f No permissions defined for {method} {path}, allowing access\")\n+            print(\n+                f\"\u26a0\ufe0f No permissions defined for {method} {path}, allowing access\"\n+            )\n             return {\n                 \"authorized\": True,\n                 \"user\": user,\n                 \"reason\": \"No specific permissions required\",\n             }\n@@ -426,11 +444,13 @@\n         else:\n             return {\n                 \"authorized\": False,\n                 \"user\": user,\n                 \"reason\": f\"Missing required permissions: {[p.value for p in required_permissions]}\",\n-                \"user_permissions\": [p.value for p in get_user_permissions(user)],\n+                \"user_permissions\": [\n+                    p.value for p in get_user_permissions(user)\n+                ],\n                 \"user_roles\": [r.value for r in get_user_roles(user)],\n             }\n \n     except Exception as e:\n         print(f\"Error in authorization check: {e}\")\n--- tests/python/unit/test_fixtures.py\t2026-01-02 15:55:23.634111+00:00\n+++ tests/python/unit/test_fixtures.py\t2026-01-02 15:55:36.963283+00:00\n@@ -153,11 +153,13 @@\n         ]\n \n         fixtures = get_all_fixtures()\n         for name, plan in fixtures.items():\n             for field in required_fields:\n-                assert field in plan, f\"Fixture '{name}' missing field '{field}'\"\n+                assert (\n+                    field in plan\n+                ), f\"Fixture '{name}' missing field '{field}'\"\n \n     def test_all_waves_have_required_fields(self):\n         \"\"\"Test all waves have required fields.\"\"\"\n         required_wave_fields = [\n             \"WaveNumber\",\n--- tests/python/validate_setup.py\t2026-01-02 15:55:23.587975+00:00\n+++ tests/python/validate_setup.py\t2026-01-02 15:55:36.984402+00:00\n@@ -9,11 +9,13 @@\n \n def check_python_version():\n     \"\"\"Verify Python version is 3.12+\"\"\"\n     version = sys.version_info\n     if version.major == 3 and version.minor >= 12:\n-        print(f\"\u2705 Python version: {version.major}.{version.minor}.{version.micro}\")\n+        print(\n+            f\"\u2705 Python version: {version.major}.{version.minor}.{version.micro}\"\n+        )\n         return True\n     else:\n         print(\n             f\"\u274c Python version {version.major}.{version.minor} is too old. Need 3.12+\"\n         )\n@@ -44,11 +46,18 @@\n \n \n def check_directory_structure():\n     \"\"\"Verify test directory structure exists\"\"\"\n     base_dir = os.path.dirname(__file__)\n-    required_dirs = [\"unit\", \"integration\", \"e2e\", \"fixtures\", \"mocks\", \"utils\"]\n+    required_dirs = [\n+        \"unit\",\n+        \"integration\",\n+        \"e2e\",\n+        \"fixtures\",\n+        \"mocks\",\n+        \"utils\",\n+    ]\n \n     all_ok = True\n     for dir_name in required_dirs:\n         dir_path = os.path.join(base_dir, dir_name)\n         if os.path.isdir(dir_path):\n--- tests/security/demo_security_test.py\t2026-01-02 15:55:23.704826+00:00\n+++ tests/security/demo_security_test.py\t2026-01-02 15:55:36.991796+00:00\n@@ -132,11 +132,14 @@\n             \"high_findings\": 0,\n             \"medium_findings\": 0,\n             \"low_findings\": 0,\n         },\n         \"findings\": demo_findings,\n-        \"compliance_status\": {\"overall_status\": \"COMPLIANT\", \"compliance_score\": 100.0},\n+        \"compliance_status\": {\n+            \"overall_status\": \"COMPLIANT\",\n+            \"compliance_score\": 100.0,\n+        },\n     }\n \n     # Save demo report\n     reports_dir = Path(\"reports\")\n     reports_dir.mkdir(exist_ok=True)\n--- tests/python/automated_e2e_test.py\t2026-01-02 15:55:23.597092+00:00\n+++ tests/python/automated_e2e_test.py\t2026-01-02 15:55:36.996241+00:00\n@@ -20,11 +20,13 @@\n \n \n class AutomatedE2ETest:\n     \"\"\"End-to-end automated test orchestrator.\"\"\"\n \n-    def __init__(self, api_endpoint: str, plan_id: str, region: str = \"us-east-1\"):\n+    def __init__(\n+        self, api_endpoint: str, plan_id: str, region: str = \"us-east-1\"\n+    ):\n         \"\"\"\n         Initialize test orchestrator.\n \n         Args:\n             api_endpoint: API Gateway endpoint URL\n@@ -75,15 +77,19 @@\n \n         Returns:\n             Dictionary containing test results\n         \"\"\"\n         try:\n-            self.results[\"test_start_time\"] = datetime.now(timezone.utc).isoformat()\n+            self.results[\"test_start_time\"] = datetime.now(\n+                timezone.utc\n+            ).isoformat()\n             logger.info(\"=\" * 80)\n             logger.info(\"STARTING END-TO-END AUTOMATED TEST\")\n             logger.info(f\"Plan ID: {self.plan_id}\")\n-            logger.info(f\"Execution Type: {'DRILL' if is_drill else 'RECOVERY'}\")\n+            logger.info(\n+                f\"Execution Type: {'DRILL' if is_drill else 'RECOVERY'}\"\n+            )\n             logger.info(\"=\" * 80)\n \n             # Phase 0: Authenticate\n             logger.info(\"\\n[PHASE 0] Authenticating with Cognito...\")\n             self._authenticate()\n@@ -115,16 +121,26 @@\n             self._generate_report()\n \n             # Determine overall success\n             self.results[\"success\"] = (\n                 self.results[\"execution_status\"] == \"COMPLETED\"\n-                and all(w.get(\"status\") == \"COMPLETED\" for w in self.results[\"waves\"])\n-                and all(j.get(\"success\", False) for j in self.results[\"drs_jobs\"])\n-                and all(i.get(\"running\", False) for i in self.results[\"ec2_instances\"])\n-            )\n-\n-            self.results[\"test_end_time\"] = datetime.now(timezone.utc).isoformat()\n+                and all(\n+                    w.get(\"status\") == \"COMPLETED\"\n+                    for w in self.results[\"waves\"]\n+                )\n+                and all(\n+                    j.get(\"success\", False) for j in self.results[\"drs_jobs\"]\n+                )\n+                and all(\n+                    i.get(\"running\", False)\n+                    for i in self.results[\"ec2_instances\"]\n+                )\n+            )\n+\n+            self.results[\"test_end_time\"] = datetime.now(\n+                timezone.utc\n+            ).isoformat()\n \n             logger.info(\"\\n\" + \"=\" * 80)\n             if self.results[\"success\"]:\n                 logger.info(\"\u2705 TEST PASSED - All validations successful\")\n             else:\n@@ -134,23 +150,30 @@\n             logger.info(\"=\" * 80)\n \n             return self.results\n \n         except Exception as e:\n-            logger.error(f\"\u274c Test failed with exception: {str(e)}\", exc_info=True)\n+            logger.error(\n+                f\"\u274c Test failed with exception: {str(e)}\", exc_info=True\n+            )\n             self.results[\"errors\"].append(f\"Test exception: {str(e)}\")\n             self.results[\"success\"] = False\n-            self.results[\"test_end_time\"] = datetime.now(timezone.utc).isoformat()\n+            self.results[\"test_end_time\"] = datetime.now(\n+                timezone.utc\n+            ).isoformat()\n             return self.results\n \n     def _authenticate(self) -> None:\n         \"\"\"Authenticate with Cognito and get JWT token.\"\"\"\n         try:\n             response = self.cognito.initiate_auth(\n                 ClientId=self.user_pool_client_id,\n                 AuthFlow=\"USER_PASSWORD_AUTH\",\n-                AuthParameters={\"USERNAME\": self.username, \"PASSWORD\": self.password},\n+                AuthParameters={\n+                    \"USERNAME\": self.username,\n+                    \"PASSWORD\": self.password,\n+                },\n             )\n             self.auth_token = response[\"AuthenticationResult\"][\"IdToken\"]\n             logger.info(f\"  Token obtained (length: {len(self.auth_token)})\")\n         except Exception as e:\n             logger.error(f\"Failed to authenticate: {str(e)}\")\n@@ -179,11 +202,13 @@\n             }\n \n             logger.info(f\"POST {url}\")\n             logger.info(f\"Payload: {json.dumps(payload, indent=2)}\")\n \n-            response = requests.post(url, json=payload, headers=headers, timeout=30)\n+            response = requests.post(\n+                url, json=payload, headers=headers, timeout=30\n+            )\n             response.raise_for_status()\n \n             result = response.json()\n             execution_id = result.get(\"executionId\")\n \n@@ -220,11 +245,13 @@\n                         \"PlanId\": {\"S\": self.plan_id},\n                     },\n                 )\n \n                 if \"Item\" not in response:\n-                    logger.warning(f\"Execution not found in DynamoDB: {execution_id}\")\n+                    logger.warning(\n+                        f\"Execution not found in DynamoDB: {execution_id}\"\n+                    )\n                     time.sleep(self.poll_interval)\n                     continue\n \n                 execution = self._parse_dynamodb_item(response[\"Item\"])\n                 status = execution.get(\"Status\", \"UNKNOWN\")\n@@ -260,11 +287,13 @@\n         self.results[\"errors\"].append(\n             f\"Orchestration timeout after {self.max_wait_time}s\"\n         )\n         raise TimeoutError(f\"Execution monitoring timed out\")\n \n-    def _monitor_drs_jobs(self, execution_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    def _monitor_drs_jobs(\n+        self, execution_data: Dict[str, Any]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"\n         Monitor DRS jobs directly via DRS API.\n \n         Args:\n             execution_data: Execution data from DynamoDB\n@@ -279,32 +308,39 @@\n             job_id = wave.get(\"JobId\")\n             wave_id = wave.get(\"WaveId\", \"unknown\")\n \n             if not job_id:\n                 logger.warning(f\"  Wave {wave_id} has no JobId\")\n-                self.results[\"warnings\"].append(f\"Wave {wave_id} missing JobId\")\n+                self.results[\"warnings\"].append(\n+                    f\"Wave {wave_id} missing JobId\"\n+                )\n                 continue\n \n             logger.info(f\"  Monitoring DRS job {job_id} (Wave {wave_id})...\")\n \n             try:\n                 # Query DRS for job details\n                 response = self.drs.describe_jobs(filters={\"jobIDs\": [job_id]})\n \n                 if not response.get(\"items\"):\n                     logger.warning(f\"  No DRS job found for ID {job_id}\")\n-                    self.results[\"warnings\"].append(f\"DRS job {job_id} not found\")\n+                    self.results[\"warnings\"].append(\n+                        f\"DRS job {job_id} not found\"\n+                    )\n                     continue\n \n                 job = response[\"items\"][0]\n                 job_status = job.get(\"status\", \"UNKNOWN\")\n                 servers = job.get(\"participatingServers\", [])\n \n                 # Check server launch statuses\n-                all_launched = all(s.get(\"launchStatus\") == \"LAUNCHED\" for s in servers)\n+                all_launched = all(\n+                    s.get(\"launchStatus\") == \"LAUNCHED\" for s in servers\n+                )\n                 any_failed = any(\n-                    s.get(\"launchStatus\") in [\"LAUNCH_FAILED\", \"FAILED\", \"TERMINATED\"]\n+                    s.get(\"launchStatus\")\n+                    in [\"LAUNCH_FAILED\", \"FAILED\", \"TERMINATED\"]\n                     for s in servers\n                 )\n \n                 job_result = {\n                     \"job_id\": job_id,\n@@ -327,11 +363,13 @@\n                 drs_results.append(job_result)\n                 self.results[\"drs_jobs\"].append(job_result)\n \n                 # Log results\n                 if job_result[\"success\"]:\n-                    logger.info(f\"    \u2705 All servers LAUNCHED ({len(servers)} servers)\")\n+                    logger.info(\n+                        f\"    \u2705 All servers LAUNCHED ({len(servers)} servers)\"\n+                    )\n                 else:\n                     logger.error(\n                         f\"    \u274c Job failed - {sum(1 for s in servers if s.get('launchStatus') != 'LAUNCHED')} server(s) failed\"\n                     )\n                     self.results[\"errors\"].append(\n@@ -430,11 +468,13 @@\n         logger.info(f\"Execution Status: {self.results['execution_status']}\")\n         logger.info(f\"Test Duration: {self._calculate_duration()}\")\n \n         logger.info(f\"\\nWaves: {len(self.results['waves'])}\")\n         for wave in self.results[\"waves\"]:\n-            logger.info(f\"  - Wave {wave.get('wave_id')}: {wave.get('status')}\")\n+            logger.info(\n+                f\"  - Wave {wave.get('wave_id')}: {wave.get('status')}\"\n+            )\n \n         logger.info(f\"\\nDRS Jobs: {len(self.results['drs_jobs'])}\")\n         for job in self.results[\"drs_jobs\"]:\n             status = \"\u2705 SUCCESS\" if job.get(\"success\") else \"\u274c FAILED\"\n             logger.info(f\"  - Job {job.get('job_id')}: {status}\")\n@@ -457,11 +497,14 @@\n             for warning in self.results[\"warnings\"]:\n                 logger.warning(f\"  - {warning}\")\n \n     def _calculate_duration(self) -> str:\n         \"\"\"Calculate test duration.\"\"\"\n-        if not self.results[\"test_start_time\"] or not self.results[\"test_end_time\"]:\n+        if (\n+            not self.results[\"test_start_time\"]\n+            or not self.results[\"test_end_time\"]\n+        ):\n             return \"unknown\"\n \n         start = datetime.fromisoformat(self.results[\"test_start_time\"])\n         end = datetime.fromisoformat(self.results[\"test_end_time\"])\n         duration = (end - start).total_seconds()\n@@ -476,14 +519,18 @@\n         for key, value in item.items():\n             if \"S\" in value:\n                 result[key] = value[\"S\"]\n             elif \"N\" in value:\n                 result[key] = (\n-                    int(value[\"N\"]) if \".\" not in value[\"N\"] else float(value[\"N\"])\n+                    int(value[\"N\"])\n+                    if \".\" not in value[\"N\"]\n+                    else float(value[\"N\"])\n                 )\n             elif \"L\" in value:\n-                result[key] = [self._parse_dynamodb_value(v) for v in value[\"L\"]]\n+                result[key] = [\n+                    self._parse_dynamodb_value(v) for v in value[\"L\"]\n+                ]\n             elif \"M\" in value:\n                 result[key] = self._parse_dynamodb_item(value[\"M\"])\n             elif \"BOOL\" in value:\n                 result[key] = value[\"BOOL\"]\n         return result\n@@ -491,20 +538,24 @@\n     def _parse_dynamodb_value(self, value: Dict[str, Any]) -> Any:\n         \"\"\"Parse a single DynamoDB value.\"\"\"\n         if \"S\" in value:\n             return value[\"S\"]\n         elif \"N\" in value:\n-            return int(value[\"N\"]) if \".\" not in value[\"N\"] else float(value[\"N\"])\n+            return (\n+                int(value[\"N\"]) if \".\" not in value[\"N\"] else float(value[\"N\"])\n+            )\n         elif \"L\" in value:\n             return [self._parse_dynamodb_value(v) for v in value[\"L\"]]\n         elif \"M\" in value:\n             return self._parse_dynamodb_item(value[\"M\"])\n         elif \"BOOL\" in value:\n             return value[\"BOOL\"]\n         return value\n \n-    def _extract_wave_data(self, execution: Dict[str, Any]) -> List[Dict[str, Any]]:\n+    def _extract_wave_data(\n+        self, execution: Dict[str, Any]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Extract simplified wave data.\"\"\"\n         waves = []\n         for wave in execution.get(\"Waves\", []):\n             waves.append(\n                 {\n@@ -519,11 +570,13 @@\n \n def main():\n     \"\"\"Main entry point for automated testing.\"\"\"\n     import argparse\n \n-    parser = argparse.ArgumentParser(description=\"Run end-to-end automated test\")\n+    parser = argparse.ArgumentParser(\n+        description=\"Run end-to-end automated test\"\n+    )\n     parser.add_argument(\n         \"--api-endpoint\", required=True, help=\"API Gateway endpoint URL\"\n     )\n     parser.add_argument(\"--plan-id\", required=True, help=\"Recovery plan ID\")\n     parser.add_argument(\"--region\", default=\"us-east-1\", help=\"AWS region\")\n@@ -533,11 +586,13 @@\n \n     args = parser.parse_args()\n \n     # Run test\n     test = AutomatedE2ETest(\n-        api_endpoint=args.api_endpoint, plan_id=args.plan_id, region=args.region\n+        api_endpoint=args.api_endpoint,\n+        plan_id=args.plan_id,\n+        region=args.region,\n     )\n \n     results = test.run_test(is_drill=not args.recovery)\n \n     # Save results to file\n--- tests/python/unit/test_recovery_plan_delete.py\t2026-01-02 15:55:23.641175+00:00\n+++ tests/python/unit/test_recovery_plan_delete.py\t2026-01-02 15:55:37.016668+00:00\n@@ -20,11 +20,13 @@\n os.environ[\n     \"STATE_MACHINE_ARN\"\n ] = \"arn:aws:states:us-east-1:123456789012:stateMachine:test\"\n \n # Add lambda directory to path\n-sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\", \"lambda\"))\n+sys.path.insert(\n+    0, os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\", \"lambda\")\n+)\n \n from index import delete_recovery_plan\n \n \n class TestRecoveryPlanDelete:\n@@ -49,11 +51,13 @@\n         call_kwargs = mock_history_table.query.call_args[1]\n         assert call_kwargs[\"IndexName\"] == \"PlanIdIndex\"\n         assert call_kwargs[\"Limit\"] == 1\n \n         # Verify delete was called\n-        mock_plans_table.delete_item.assert_called_once_with(Key={\"PlanId\": plan_id})\n+        mock_plans_table.delete_item.assert_called_once_with(\n+            Key={\"PlanId\": plan_id}\n+        )\n \n         # Verify success response\n         assert result[\"statusCode\"] == 200\n         body = eval(result[\"body\"])\n         assert body[\"message\"] == \"Recovery Plan deleted successfully\"\n@@ -67,11 +71,16 @@\n         \"\"\"Test successful delete using scan fallback when GSI doesn't exist\"\"\"\n         plan_id = \"plan-456\"\n \n         # Mock GSI query failure, then scan success\n         mock_history_table.query.side_effect = ClientError(\n-            {\"Error\": {\"Code\": \"ValidationException\", \"Message\": \"Index not found\"}},\n+            {\n+                \"Error\": {\n+                    \"Code\": \"ValidationException\",\n+                    \"Message\": \"Index not found\",\n+                }\n+            },\n             \"Query\",\n         )\n         mock_history_table.scan.return_value = {\"Items\": []}\n         mock_plans_table.delete_item.return_value = {}\n \n@@ -84,11 +93,13 @@\n         mock_history_table.scan.assert_called_once()\n         scan_kwargs = mock_history_table.scan.call_args[1]\n         assert scan_kwargs[\"Limit\"] == 1\n \n         # Verify delete was called\n-        mock_plans_table.delete_item.assert_called_once_with(Key={\"PlanId\": plan_id})\n+        mock_plans_table.delete_item.assert_called_once_with(\n+            Key={\"PlanId\": plan_id}\n+        )\n \n         # Verify success response\n         assert result[\"statusCode\"] == 200\n         body = eval(result[\"body\"])\n         assert body[\"message\"] == \"Recovery Plan deleted successfully\"\n@@ -102,11 +113,15 @@\n         plan_id = \"plan-789\"\n \n         # Mock GSI query returning 1 active execution\n         mock_history_table.query.return_value = {\n             \"Items\": [\n-                {\"ExecutionId\": \"exec-123\", \"PlanId\": plan_id, \"Status\": \"RUNNING\"}\n+                {\n+                    \"ExecutionId\": \"exec-123\",\n+                    \"PlanId\": plan_id,\n+                    \"Status\": \"RUNNING\",\n+                }\n             ]\n         }\n \n         result = delete_recovery_plan(plan_id)\n \n@@ -171,12 +186,16 @@\n     def test_delete_error_handling(self, mock_plans_table, mock_history_table):\n         \"\"\"Test error handling when DynamoDB operations fail\"\"\"\n         plan_id = \"plan-error\"\n \n         # Mock both query and scan to fail (test complete failure path)\n-        mock_history_table.query.side_effect = Exception(\"DynamoDB connection failed\")\n-        mock_history_table.scan.side_effect = Exception(\"DynamoDB connection failed\")\n+        mock_history_table.query.side_effect = Exception(\n+            \"DynamoDB connection failed\"\n+        )\n+        mock_history_table.scan.side_effect = Exception(\n+            \"DynamoDB connection failed\"\n+        )\n \n         result = delete_recovery_plan(plan_id)\n \n         # Verify 500 error response\n         assert result[\"statusCode\"] == 500\n@@ -185,11 +204,13 @@\n         assert \"Failed to delete Recovery Plan\" in body[\"message\"]\n         assert body[\"planId\"] == plan_id\n \n     @patch(\"index.execution_history_table\")\n     @patch(\"index.recovery_plans_table\")\n-    def test_gsi_query_parameters_correct(self, mock_plans_table, mock_history_table):\n+    def test_gsi_query_parameters_correct(\n+        self, mock_plans_table, mock_history_table\n+    ):\n         \"\"\"Test that GSI query uses correct parameters\"\"\"\n         plan_id = \"plan-params\"\n \n         mock_history_table.query.return_value = {\"Items\": []}\n         mock_plans_table.delete_item.return_value = {}\n@@ -246,11 +267,13 @@\n         # This test verifies the function completes without errors\n         assert mock_plans_table.delete_item.called\n \n     @patch(\"index.execution_history_table\")\n     @patch(\"index.recovery_plans_table\")\n-    def test_delete_with_empty_items_list(self, mock_plans_table, mock_history_table):\n+    def test_delete_with_empty_items_list(\n+        self, mock_plans_table, mock_history_table\n+    ):\n         \"\"\"Test delete succeeds when Items list is empty (not missing)\"\"\"\n         plan_id = \"plan-empty-list\"\n \n         # Mock query returning empty list\n         mock_history_table.query.return_value = {\"Items\": []}\n@@ -261,11 +284,13 @@\n         assert result[\"statusCode\"] == 200\n         mock_plans_table.delete_item.assert_called_once()\n \n     @patch(\"index.execution_history_table\")\n     @patch(\"index.recovery_plans_table\")\n-    def test_delete_with_no_items_key(self, mock_plans_table, mock_history_table):\n+    def test_delete_with_no_items_key(\n+        self, mock_plans_table, mock_history_table\n+    ):\n         \"\"\"Test delete succeeds when response has no Items key\"\"\"\n         plan_id = \"plan-no-key\"\n \n         # Mock query returning response without Items key\n         mock_history_table.query.return_value = {}\n--- lambda/orchestration_stepfunctions.py\t2026-01-02 15:52:46.351211+00:00\n+++ lambda/orchestration_stepfunctions.py\t2026-01-02 15:55:37.024150+00:00\n@@ -90,21 +90,25 @@\n                 RoleArn=f\"arn:aws:iam::{account_id}:role/{role_name}\",\n                 RoleSessionName=session_name,\n             )\n \n             credentials = assumed_role[\"Credentials\"]\n-            print(f\"Successfully assumed role {role_name} in account {account_id}\")\n+            print(\n+                f\"Successfully assumed role {role_name} in account {account_id}\"\n+            )\n \n             return boto3.client(\n                 \"drs\",\n                 region_name=region,\n                 aws_access_key_id=credentials[\"AccessKeyId\"],\n                 aws_secret_access_key=credentials[\"SecretAccessKey\"],\n                 aws_session_token=credentials[\"SessionToken\"],\n             )\n         except Exception as e:\n-            print(f\"Failed to assume role {role_name} in account {account_id}: {e}\")\n+            print(\n+                f\"Failed to assume role {role_name} in account {account_id}: {e}\"\n+            )\n             raise\n \n     # Default: use current account credentials\n     return boto3.client(\"drs\", region_name=region)\n \n@@ -301,11 +305,13 @@\n \n     # Convert Decimal to int if needed (DynamoDB returns Decimal)\n     if isinstance(paused_before_wave, Decimal):\n         paused_before_wave = int(paused_before_wave)\n \n-    print(f\"\u23ef\ufe0f Resuming execution {execution_id}, starting wave {paused_before_wave}\")\n+    print(\n+        f\"\u23ef\ufe0f Resuming execution {execution_id}, starting wave {paused_before_wave}\"\n+    )\n \n     # Reset status to running\n     state[\"status\"] = \"running\"\n     state[\"wave_completed\"] = False\n     state[\"paused_before_wave\"] = None\n@@ -433,11 +439,13 @@\n         )\n         if \"Item\" not in pg_response:\n             print(f\"Protection Group {protection_group_id} not found\")\n             state[\"wave_completed\"] = True\n             state[\"status\"] = \"failed\"\n-            state[\"error\"] = f\"Protection Group {protection_group_id} not found\"\n+            state[\n+                \"error\"\n+            ] = f\"Protection Group {protection_group_id} not found\"\n             return\n \n         pg = pg_response[\"Item\"]\n         region = pg.get(\"Region\", \"us-east-1\")\n \n@@ -455,11 +463,13 @@\n             )\n             print(f\"Resolved {len(server_ids)} servers from tags\")\n         else:\n             # Fallback: Check if wave has explicit ServerIds (legacy support)\n             server_ids = wave.get(\"ServerIds\", [])\n-            print(f\"Using explicit ServerIds from wave: {len(server_ids)} servers\")\n+            print(\n+                f\"Using explicit ServerIds from wave: {len(server_ids)} servers\"\n+            )\n \n         if not server_ids:\n             print(\n                 f\"Wave {wave_number} has no servers (no tags matched or no servers found), marking complete\"\n             )\n@@ -574,11 +584,13 @@\n     update_time = state.get(\"current_wave_update_time\", 30)\n     total_wait = state.get(\"current_wave_total_wait_time\", 0) + update_time\n     max_wait = state.get(\"current_wave_max_wait_time\", 1800)\n     state[\"current_wave_total_wait_time\"] = total_wait\n \n-    print(f\"Checking status for job {job_id}, wait time: {total_wait}s / {max_wait}s\")\n+    print(\n+        f\"Checking status for job {job_id}, wait time: {total_wait}s / {max_wait}s\"\n+    )\n \n     # Check for timeout\n     if total_wait >= max_wait:\n         print(f\"\u274c Wave {wave_number} TIMEOUT\")\n         state[\"wave_completed\"] = True\n@@ -606,19 +618,24 @@\n         print(\n             f\"Job {job_id} status: {job_status}, servers: {len(participating_servers)}\"\n         )\n \n         if not participating_servers:\n-            if job_status in DRS_JOB_STATUS_WAIT_STATES or job_status == \"STARTED\":\n+            if (\n+                job_status in DRS_JOB_STATUS_WAIT_STATES\n+                or job_status == \"STARTED\"\n+            ):\n                 print(\"Job still initializing\")\n                 state[\"wave_completed\"] = False\n                 return state\n             elif job_status == \"COMPLETED\":\n                 print(f\"\u274c Job COMPLETED but no servers\")\n                 state[\"wave_completed\"] = True\n                 state[\"status\"] = \"failed\"\n-                state[\"error\"] = \"DRS job completed but no participating servers\"\n+                state[\n+                    \"error\"\n+                ] = \"DRS job completed but no participating servers\"\n                 return state\n             else:\n                 state[\"wave_completed\"] = False\n                 return state\n \n@@ -731,11 +748,13 @@\n         if launched_count > 0 and launched_count < total_servers:\n             current_wave_status = \"IN_PROGRESS\"\n \n         # Update wave status in DynamoDB if it has changed from STARTED\n         if current_wave_status != \"STARTED\":\n-            print(f\"Updating wave {wave_number} status to {current_wave_status}\")\n+            print(\n+                f\"Updating wave {wave_number} status to {current_wave_status}\"\n+            )\n             update_wave_in_dynamodb(\n                 execution_id,\n                 plan_id,\n                 wave_number,\n                 current_wave_status,\n@@ -745,11 +764,13 @@\n         # Check if job completed but no instances created\n         if job_status == \"COMPLETED\" and launched_count == 0:\n             print(f\"\u274c Job COMPLETED but no instances launched\")\n             state[\"wave_completed\"] = True\n             state[\"status\"] = \"failed\"\n-            state[\"error\"] = \"DRS job completed but no recovery instances created\"\n+            state[\n+                \"error\"\n+            ] = \"DRS job completed but no recovery instances created\"\n             update_wave_in_dynamodb(\n                 execution_id, plan_id, wave_number, \"FAILED\", server_statuses\n             )\n             return state\n \n@@ -759,53 +780,63 @@\n                 f\"\u2705 Wave {wave_number} COMPLETE - all {launched_count} servers launched\"\n             )\n \n             # Get EC2 instance IDs\n             try:\n-                source_server_ids = [s.get(\"SourceServerId\") for s in server_statuses]\n+                source_server_ids = [\n+                    s.get(\"SourceServerId\") for s in server_statuses\n+                ]\n                 ri_response = drs_client.describe_recovery_instances(\n                     filters={\"sourceServerIDs\": source_server_ids}\n                 )\n                 for ri in ri_response.get(\"items\", []):\n                     source_id = ri.get(\"sourceServerID\")\n                     for ss in server_statuses:\n                         if ss.get(\"SourceServerId\") == source_id:\n                             ss[\"EC2InstanceId\"] = ri.get(\"ec2InstanceID\")\n-                            ss[\"RecoveryInstanceID\"] = ri.get(\"recoveryInstanceID\")\n+                            ss[\"RecoveryInstanceID\"] = ri.get(\n+                                \"recoveryInstanceID\"\n+                            )\n                             break\n             except Exception as e:\n-                print(f\"Warning: Could not fetch recovery instance details: {e}\")\n+                print(\n+                    f\"Warning: Could not fetch recovery instance details: {e}\"\n+                )\n \n             state[\"wave_completed\"] = True\n             state[\"completed_waves\"] = state.get(\"completed_waves\", 0) + 1\n \n             # Capture recovery instance IDs and IPs for parent orchestrator\n             for ss in server_statuses:\n                 ec2_id = ss.get(\"EC2InstanceId\")\n                 if ec2_id:\n                     if ec2_id not in state.get(\"recovery_instance_ids\", []):\n-                        state.setdefault(\"recovery_instance_ids\", []).append(ec2_id)\n+                        state.setdefault(\"recovery_instance_ids\", []).append(\n+                            ec2_id\n+                        )\n \n             # Fetch private IPs from EC2\n             try:\n                 ec2_ids = [\n                     ss.get(\"EC2InstanceId\")\n                     for ss in server_statuses\n                     if ss.get(\"EC2InstanceId\")\n                 ]\n                 if ec2_ids:\n                     ec2_client = boto3.client(\"ec2\", region_name=region)\n-                    ec2_response = ec2_client.describe_instances(InstanceIds=ec2_ids)\n+                    ec2_response = ec2_client.describe_instances(\n+                        InstanceIds=ec2_ids\n+                    )\n                     for reservation in ec2_response.get(\"Reservations\", []):\n                         for instance in reservation.get(\"Instances\", []):\n                             private_ip = instance.get(\"PrivateIpAddress\")\n                             if private_ip and private_ip not in state.get(\n                                 \"recovery_instance_ips\", []\n                             ):\n-                                state.setdefault(\"recovery_instance_ips\", []).append(\n-                                    private_ip\n-                                )\n+                                state.setdefault(\n+                                    \"recovery_instance_ips\", []\n+                                ).append(private_ip)\n             except Exception as e:\n                 print(f\"Warning: Could not fetch EC2 IPs: {e}\")\n \n             # Update wave result\n             for wr in state.get(\"wave_results\", []):\n@@ -836,11 +867,13 @@\n                     state[\"all_waves_completed\"] = True\n                     state[\"status\"] = \"cancelled\"\n                     state[\"status_reason\"] = \"Execution cancelled by user\"\n                     state[\"end_time\"] = end_time\n                     if state.get(\"start_time\"):\n-                        state[\"duration_seconds\"] = end_time - state[\"start_time\"]\n+                        state[\"duration_seconds\"] = (\n+                            end_time - state[\"start_time\"]\n+                        )\n                     get_execution_history_table().update_item(\n                         Key={\"ExecutionId\": execution_id, \"PlanId\": plan_id},\n                         UpdateExpression=\"SET #status = :status, EndTime = :end\",\n                         ExpressionAttributeNames={\"#status\": \"Status\"},\n                         ExpressionAttributeValues={\n@@ -899,11 +932,13 @@\n                     },\n                     ConditionExpression=\"attribute_exists(ExecutionId)\",\n                 )\n \n         elif failed_count > 0:\n-            print(f\"\u274c Wave {wave_number} FAILED - {failed_count} servers failed\")\n+            print(\n+                f\"\u274c Wave {wave_number} FAILED - {failed_count} servers failed\"\n+            )\n             end_time = int(time.time())\n             state[\"wave_completed\"] = True\n             state[\"status\"] = \"failed\"\n             state[\n                 \"status_reason\"\n--- tests/python/unit/test_data_generator.py\t2026-01-02 15:55:23.618124+00:00\n+++ tests/python/unit/test_data_generator.py\t2026-01-02 15:55:37.035057+00:00\n@@ -98,11 +98,13 @@\n             assert wave[\"Dependencies\"] == []\n \n     def test_generate_recovery_plan_execution_types(self):\n         \"\"\"Test plan generation with specific execution types.\"\"\"\n         execution_types = [\"PARALLEL\", \"SEQUENTIAL\", \"PARALLEL\"]\n-        plan = generate_recovery_plan(wave_count=3, execution_types=execution_types)\n+        plan = generate_recovery_plan(\n+            wave_count=3, execution_types=execution_types\n+        )\n \n         for i, wave in enumerate(plan[\"Waves\"]):\n             assert wave[\"ExecutionType\"] == execution_types[i]\n \n     def test_generate_recovery_plan_wait_times(self):\n@@ -196,11 +198,13 @@\n             assert execution[\"Status\"] == status\n \n     def test_generate_execution_history_wave_count(self):\n         \"\"\"Test execution history with specific wave count.\"\"\"\n         wave_count = 5\n-        execution = generate_execution_history(\"plan-001\", wave_count=wave_count)\n+        execution = generate_execution_history(\n+            \"plan-001\", wave_count=wave_count\n+        )\n \n         assert len(execution[\"WaveStatuses\"]) == wave_count\n \n     def test_generate_execution_history_with_errors(self):\n         \"\"\"Test execution history with errors.\"\"\"\n@@ -220,11 +224,13 @@\n     def test_generate_execution_history_server_counts(self):\n         \"\"\"Test execution history has correct server counts.\"\"\"\n         wave_count = 3\n         servers_per_wave = 4\n         execution = generate_execution_history(\n-            \"plan-001\", wave_count=wave_count, servers_per_wave=servers_per_wave\n+            \"plan-001\",\n+            wave_count=wave_count,\n+            servers_per_wave=servers_per_wave,\n         )\n \n         assert execution[\"TotalServers\"] == wave_count * servers_per_wave\n         assert (\n             execution[\"SuccessfulServers\"] + execution[\"FailedServers\"]\n--- tests/python/unit/test_mock_drs_client.py\t2026-01-02 15:55:23.637233+00:00\n+++ tests/python/unit/test_mock_drs_client.py\t2026-01-02 15:55:37.055495+00:00\n@@ -106,14 +106,19 @@\n \n     def test_start_recovery_invalid_server(self):\n         \"\"\"Test start_recovery with invalid server ID.\"\"\"\n         client = MockDRSClient()\n \n-        with pytest.raises(ResourceNotFoundException, match=\"Source server not found\"):\n+        with pytest.raises(\n+            ResourceNotFoundException, match=\"Source server not found\"\n+        ):\n             client.start_recovery(\n                 sourceServers=[\n-                    {\"sourceServerID\": \"s-invalid\", \"recoverySnapshotID\": \"LATEST\"}\n+                    {\n+                        \"sourceServerID\": \"s-invalid\",\n+                        \"recoverySnapshotID\": \"LATEST\",\n+                    }\n                 ]\n             )\n \n     def test_describe_jobs_single(self):\n         \"\"\"Test describe_jobs returns created job.\"\"\"\n@@ -192,11 +197,13 @@\n         assert jobs[\"items\"][0][\"status\"] == \"COMPLETED\"\n         assert jobs[\"items\"][0][\"endDateTime\"] is not None\n \n     def test_throttling_simulation(self):\n         \"\"\"Test throttling simulation after N calls.\"\"\"\n-        client = MockDRSClient(simulate_throttling=True, throttle_after_calls=3)\n+        client = MockDRSClient(\n+            simulate_throttling=True, throttle_after_calls=3\n+        )\n \n         # First 3 calls should succeed\n         for i in range(3):\n             client.describe_source_servers()\n \n@@ -212,11 +219,12 @@\n         client = MockDRSClient(\n             simulate_errors=True, error_rate=1.0  # 100% error rate for testing\n         )\n \n         with pytest.raises(\n-            ServiceUnavailableException, match=\"Service temporarily unavailable\"\n+            ServiceUnavailableException,\n+            match=\"Service temporarily unavailable\",\n         ):\n             client.describe_source_servers()\n \n     def test_terminate_recovery_instances(self):\n         \"\"\"Test terminate_recovery_instances.\"\"\"\n@@ -232,12 +240,16 @@\n             ]\n         )\n \n         # Wait for instance to be created\n         time.sleep(6)\n-        jobs = client.describe_jobs(filters={\"jobIDs\": [response[\"job\"][\"jobID\"]]})\n-        instance_id = jobs[\"items\"][0][\"participatingServers\"][0][\"recoveryInstanceID\"]\n+        jobs = client.describe_jobs(\n+            filters={\"jobIDs\": [response[\"job\"][\"jobID\"]]}\n+        )\n+        instance_id = jobs[\"items\"][0][\"participatingServers\"][0][\n+            \"recoveryInstanceID\"\n+        ]\n \n         # Terminate instance\n         term_response = client.terminate_recovery_instances(\n             recoveryInstanceIDs=[instance_id]\n         )\n--- tests/python/unit/test_wave_transformation.py\t2026-01-02 15:55:23.630910+00:00\n+++ tests/python/unit/test_wave_transformation.py\t2026-01-02 15:55:37.057111+00:00\n@@ -18,11 +18,13 @@\n os.environ[\n     \"STATE_MACHINE_ARN\"\n ] = \"arn:aws:states:us-east-1:123456789012:stateMachine:test\"\n \n # Add lambda directory to path\n-sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\", \"lambda\"))\n+sys.path.insert(\n+    0, os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\", \"lambda\")\n+)\n \n from index import transform_rp_to_camelcase\n \n \n class TestWaveTransformation:\n@@ -249,11 +251,13 @@\n                     \"WaveName\": \"Wave 1\",\n                     \"ServerIds\": [\"s-123\"],\n                     \"ProtectionGroupId\": \"pg-789\",\n                     \"ExecutionType\": \"sequential\",\n                     \"Dependencies\": [\n-                        {\"DependsOnWaveId\": \"invalid-format\"},  # No hyphen-number\n+                        {\n+                            \"DependsOnWaveId\": \"invalid-format\"\n+                        },  # No hyphen-number\n                         {\"DependsOnWaveId\": \"wave-abc\"},  # Non-numeric\n                         {\"DependsOnWaveId\": \"\"},  # Empty\n                     ],\n                 }\n             ],\n--- tests/security/run_security_tests.py\t2026-01-02 15:55:23.708468+00:00\n+++ tests/security/run_security_tests.py\t2026-01-02 15:55:37.058144+00:00\n@@ -196,11 +196,13 @@\n     # Process findings for HTML\n     findings_html = \"\"\n     for risk_level in [\"CRITICAL\", \"HIGH\", \"MEDIUM\", \"LOW\"]:\n         findings = report_data.get(\"findings_by_risk\", {}).get(risk_level, [])\n         if findings:\n-            findings_html += f\"<h3>{risk_level} Risk Findings ({len(findings)})</h3>\"\n+            findings_html += (\n+                f\"<h3>{risk_level} Risk Findings ({len(findings)})</h3>\"\n+            )\n             for finding in findings:\n                 risk_class = f\"finding-{risk_level.lower()}\"\n                 findings_html += f\"\"\"\n                 <div class=\"finding {risk_class}\">\n                     <div class=\"finding-title\">{finding['title']}</div>\n@@ -241,21 +243,25 @@\n     html_content = html_template.format(\n         timestamp=report_data[\"metadata\"][\"timestamp\"],\n         session_id=report_data[\"metadata\"][\"test_session_id\"],\n         total_tests=report_data[\"executive_summary\"][\"total_tests\"],\n         success_rate=report_data[\"executive_summary\"][\"success_rate\"],\n-        critical_findings=report_data[\"executive_summary\"][\"critical_findings\"],\n+        critical_findings=report_data[\"executive_summary\"][\n+            \"critical_findings\"\n+        ],\n         high_findings=report_data[\"executive_summary\"][\"high_findings\"],\n         medium_findings=report_data[\"executive_summary\"][\"medium_findings\"],\n         low_findings=report_data[\"executive_summary\"][\"low_findings\"],\n         compliance_status=compliance_status,\n         compliance_score=report_data[\"compliance_status\"][\"compliance_score\"],\n         compliance_class=compliance_class,\n         recommendations_html=recommendations_html,\n         findings_html=findings_html,\n         roles_tested=len(report_data[\"test_coverage\"][\"roles_tested\"]),\n-        permissions_tested=len(report_data[\"test_coverage\"][\"permissions_tested\"]),\n+        permissions_tested=len(\n+            report_data[\"test_coverage\"][\"permissions_tested\"]\n+        ),\n     )\n \n     with open(output_file, \"w\") as f:\n         f.write(html_content)\n \n@@ -299,11 +305,13 @@\n \n async def main():\n     \"\"\"Main execution function\"\"\"\n     parser = argparse.ArgumentParser(description=\"Run RBAC Security Tests\")\n     parser.add_argument(\n-        \"--config\", default=\"tests/security/config.json\", help=\"Configuration file path\"\n+        \"--config\",\n+        default=\"tests/security/config.json\",\n+        help=\"Configuration file path\",\n     )\n     parser.add_argument(\n         \"--output-dir\",\n         default=\"tests/security/reports\",\n         help=\"Output directory for reports\",\n@@ -376,24 +384,34 @@\n         print(f\"Test Session ID: {report['metadata']['test_session_id']}\")\n         print(f\"Timestamp: {report['metadata']['timestamp']}\")\n         print(f\"Environment: {report['metadata']['test_environment']}\")\n         print()\n         print(\"\ud83d\udcca TEST RESULTS:\")\n-        print(f\"  Total Tests Executed: {report['executive_summary']['total_tests']}\")\n+        print(\n+            f\"  Total Tests Executed: {report['executive_summary']['total_tests']}\"\n+        )\n         print(f\"  Tests Passed: {report['executive_summary']['passed_tests']}\")\n         print(f\"  Tests Failed: {report['executive_summary']['failed_tests']}\")\n         print(f\"  Test Errors: {report['executive_summary']['error_tests']}\")\n-        print(f\"  Success Rate: {report['executive_summary']['success_rate']:.1f}%\")\n+        print(\n+            f\"  Success Rate: {report['executive_summary']['success_rate']:.1f}%\"\n+        )\n         print()\n         print(\"\ud83d\udea8 SECURITY FINDINGS:\")\n-        print(f\"  Critical Risk: {report['executive_summary']['critical_findings']}\")\n+        print(\n+            f\"  Critical Risk: {report['executive_summary']['critical_findings']}\"\n+        )\n         print(f\"  High Risk: {report['executive_summary']['high_findings']}\")\n-        print(f\"  Medium Risk: {report['executive_summary']['medium_findings']}\")\n+        print(\n+            f\"  Medium Risk: {report['executive_summary']['medium_findings']}\"\n+        )\n         print(f\"  Low Risk: {report['executive_summary']['low_findings']}\")\n         print()\n         print(\"\u2705 COMPLIANCE STATUS:\")\n-        print(f\"  Overall Status: {report['compliance_status']['overall_status']}\")\n+        print(\n+            f\"  Overall Status: {report['compliance_status']['overall_status']}\"\n+        )\n         print(\n             f\"  Compliance Score: {report['compliance_status']['compliance_score']:.1f}%\"\n         )\n         print(\n             f\"  AWS Security Standards: {report['compliance_status']['standards']['AWS_Security_Standards']}\"\n@@ -423,14 +441,18 @@\n             logger.error(\n                 f\"CRITICAL: Found {critical_findings} critical security issues!\"\n             )\n             exit_code = 2\n         elif high_findings > 0:\n-            logger.warning(f\"WARNING: Found {high_findings} high-risk security issues!\")\n+            logger.warning(\n+                f\"WARNING: Found {high_findings} high-risk security issues!\"\n+            )\n             exit_code = 1\n         else:\n-            logger.info(\"SUCCESS: No critical or high-risk security issues found!\")\n+            logger.info(\n+                \"SUCCESS: No critical or high-risk security issues found!\"\n+            )\n             exit_code = 0\n \n         return exit_code\n \n     except Exception as e:\n--- tests/python/utils/test_data_generator.py\t2026-01-02 15:55:23.658093+00:00\n+++ tests/python/utils/test_data_generator.py\t2026-01-02 15:55:37.075321+00:00\n@@ -59,11 +59,12 @@\n     if plan_id is None:\n         plan_id = f\"plan-{uuid.uuid4().hex[:12]}\"\n \n     if execution_types is None:\n         execution_types = [\n-            random.choice([\"SEQUENTIAL\", \"PARALLEL\"]) for _ in range(wave_count)\n+            random.choice([\"SEQUENTIAL\", \"PARALLEL\"])\n+            for _ in range(wave_count)\n         ]\n \n     if wait_times is None:\n         wait_times = [random.randint(0, 300) for _ in range(wave_count)]\n \n@@ -140,11 +141,13 @@\n     \"\"\"\n     if execution_id is None:\n         execution_id = str(uuid.uuid4())\n \n     start_time = int(time.time()) - random.randint(300, 3600)\n-    end_time = start_time + random.randint(60, 1800) if status != \"RUNNING\" else None\n+    end_time = (\n+        start_time + random.randint(60, 1800) if status != \"RUNNING\" else None\n+    )\n \n     # Generate wave statuses\n     wave_statuses = []\n     total_servers = wave_count * servers_per_wave\n     failed_count = 0\n@@ -183,11 +186,14 @@\n                 \"JobId\": job_id,\n                 \"RecoveryInstanceId\": f\"i-{uuid.uuid4().hex[:17]}\"\n                 if server_status == \"COMPLETED\"\n                 else None,\n                 \"StartTime\": start_time + (i * 60) + (j * 10),\n-                \"EndTime\": start_time + (i * 60) + (j * 10) + random.randint(30, 120),\n+                \"EndTime\": start_time\n+                + (i * 60)\n+                + (j * 10)\n+                + random.randint(30, 120),\n                 \"ErrorMessage\": error_message,\n             }\n             server_results.append(server_result)\n \n         wave_status = {\n@@ -312,19 +318,23 @@\n         \"Description\": f\"Generated protection group with {server_count} servers\",\n         \"ServerIds\": generate_server_ids(server_count),\n         \"Region\": \"us-east-1\",\n         \"AccountId\": \"123456789012\",\n         \"Tags\": {\n-            \"Environment\": random.choice([\"Production\", \"Staging\", \"Development\"]),\n+            \"Environment\": random.choice(\n+                [\"Production\", \"Staging\", \"Development\"]\n+            ),\n             \"Application\": random.choice([\"WebApp\", \"Database\", \"API\"]),\n         },\n         \"CreatedDate\": int(time.time()),\n         \"LastModifiedDate\": int(time.time()),\n     }\n \n \n-def generate_random_string(length: int = 10, include_special: bool = False) -> str:\n+def generate_random_string(\n+    length: int = 10, include_special: bool = False\n+) -> str:\n     \"\"\"\n     Generate a random string.\n \n     Args:\n         length: Length of the string\n@@ -419,11 +429,13 @@\n \n     Returns:\n         List of generated execution records\n     \"\"\"\n     if plan_ids is None:\n-        plan_ids = [f\"plan-{uuid.uuid4().hex[:12]}\" for _ in range(count // 10)]\n+        plan_ids = [\n+            f\"plan-{uuid.uuid4().hex[:12]}\" for _ in range(count // 10)\n+        ]\n \n     executions = []\n     for _ in range(count):\n         plan_id = random.choice(plan_ids)\n \n--- tests/python/unit/test_drs_service_limits.py\t2026-01-02 15:55:23.625166+00:00\n+++ tests/python/unit/test_drs_service_limits.py\t2026-01-02 15:55:37.092291+00:00\n@@ -17,17 +17,21 @@\n from unittest.mock import MagicMock, patch\n \n import pytest\n \n # Add lambda directory to path for imports\n-sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\", \"lambda\"))\n+sys.path.insert(\n+    0, os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\", \"lambda\")\n+)\n \n # Mock environment variables before importing\n os.environ[\"PROTECTION_GROUPS_TABLE\"] = \"test-protection-groups\"\n os.environ[\"RECOVERY_PLANS_TABLE\"] = \"test-recovery-plans\"\n os.environ[\"EXECUTION_HISTORY_TABLE\"] = \"test-execution-history\"\n-os.environ[\"STATE_MACHINE_ARN\"] = \"arn:aws:states:us-east-1:123456789:stateMachine:test\"\n+os.environ[\n+    \"STATE_MACHINE_ARN\"\n+] = \"arn:aws:states:us-east-1:123456789:stateMachine:test\"\n \n \n class TestDRSLimitsConstants:\n     \"\"\"Test DRS_LIMITS constants match AWS documented limits.\"\"\"\n \n@@ -99,11 +103,14 @@\n         \"\"\"Wave with exactly 100 servers should be valid.\"\"\"\n         from index import validate_wave_sizes\n \n         plan = {\n             \"Waves\": [\n-                {\"WaveName\": \"Wave 1\", \"ServerIds\": [f\"s-{i}\" for i in range(100)]}\n+                {\n+                    \"WaveName\": \"Wave 1\",\n+                    \"ServerIds\": [f\"s-{i}\" for i in range(100)],\n+                }\n             ]\n         }\n         errors = validate_wave_sizes(plan)\n         assert len(errors) == 0\n \n@@ -111,11 +118,14 @@\n         \"\"\"Wave with 101 servers should return error.\"\"\"\n         from index import validate_wave_sizes\n \n         plan = {\n             \"Waves\": [\n-                {\"WaveName\": \"Wave 1\", \"ServerIds\": [f\"s-{i}\" for i in range(101)]}\n+                {\n+                    \"WaveName\": \"Wave 1\",\n+                    \"ServerIds\": [f\"s-{i}\" for i in range(101)],\n+                }\n             ]\n         }\n         errors = validate_wave_sizes(plan)\n         assert len(errors) == 1\n         assert errors[0][\"type\"] == \"WAVE_SIZE_EXCEEDED\"\n@@ -126,11 +136,14 @@\n         \"\"\"Wave with 200 servers should return error with correct count.\"\"\"\n         from index import validate_wave_sizes\n \n         plan = {\n             \"Waves\": [\n-                {\"WaveName\": \"Big Wave\", \"ServerIds\": [f\"s-{i}\" for i in range(200)]}\n+                {\n+                    \"WaveName\": \"Big Wave\",\n+                    \"ServerIds\": [f\"s-{i}\" for i in range(200)],\n+                }\n             ]\n         }\n         errors = validate_wave_sizes(plan)\n         assert len(errors) == 1\n         assert errors[0][\"serverCount\"] == 200\n@@ -140,13 +153,22 @@\n         \"\"\"Multiple waves with one exceeding limit should return one error.\"\"\"\n         from index import validate_wave_sizes\n \n         plan = {\n             \"Waves\": [\n-                {\"WaveName\": \"Wave 1\", \"ServerIds\": [f\"s-{i}\" for i in range(50)]},\n-                {\"WaveName\": \"Wave 2\", \"ServerIds\": [f\"s-{i}\" for i in range(150)]},\n-                {\"WaveName\": \"Wave 3\", \"ServerIds\": [f\"s-{i}\" for i in range(30)]},\n+                {\n+                    \"WaveName\": \"Wave 1\",\n+                    \"ServerIds\": [f\"s-{i}\" for i in range(50)],\n+                },\n+                {\n+                    \"WaveName\": \"Wave 2\",\n+                    \"ServerIds\": [f\"s-{i}\" for i in range(150)],\n+                },\n+                {\n+                    \"WaveName\": \"Wave 3\",\n+                    \"ServerIds\": [f\"s-{i}\" for i in range(30)],\n+                },\n             ]\n         }\n         errors = validate_wave_sizes(plan)\n         assert len(errors) == 1\n         assert errors[0][\"wave\"] == \"Wave 2\"\n@@ -156,12 +178,18 @@\n         \"\"\"Multiple waves all exceeding limit should return multiple errors.\"\"\"\n         from index import validate_wave_sizes\n \n         plan = {\n             \"Waves\": [\n-                {\"WaveName\": \"Wave 1\", \"ServerIds\": [f\"s-{i}\" for i in range(101)]},\n-                {\"WaveName\": \"Wave 2\", \"ServerIds\": [f\"s-{i}\" for i in range(150)]},\n+                {\n+                    \"WaveName\": \"Wave 1\",\n+                    \"ServerIds\": [f\"s-{i}\" for i in range(101)],\n+                },\n+                {\n+                    \"WaveName\": \"Wave 2\",\n+                    \"ServerIds\": [f\"s-{i}\" for i in range(150)],\n+                },\n             ]\n         }\n         errors = validate_wave_sizes(plan)\n         assert len(errors) == 2\n \n@@ -335,11 +363,13 @@\n             }\n         ]\n         mock_drs.get_paginator.return_value = mock_paginator\n         mock_boto_client.return_value = mock_drs\n \n-        result = validate_servers_in_all_jobs(\"us-east-1\", 100)  # Would be 550 total\n+        result = validate_servers_in_all_jobs(\n+            \"us-east-1\", 100\n+        )  # Would be 550 total\n \n         assert result[\"valid\"] is False\n         assert result[\"currentServersInJobs\"] == 450\n         assert result[\"totalAfterNew\"] == 550\n         assert result[\"maxServers\"] == 500\n@@ -406,11 +436,13 @@\n                 },\n             ]\n         }\n         mock_boto_client.return_value = mock_drs\n \n-        result = validate_server_replication_states(\"us-east-1\", [\"s-1\", \"s-2\"])\n+        result = validate_server_replication_states(\n+            \"us-east-1\", [\"s-1\", \"s-2\"]\n+        )\n \n         assert result[\"valid\"] is True\n         assert result[\"healthyCount\"] == 2\n         assert result[\"unhealthyCount\"] == 0\n \n@@ -422,11 +454,13 @@\n         mock_drs = MagicMock()\n         mock_drs.describe_source_servers.return_value = {\n             \"items\": [\n                 {\n                     \"sourceServerID\": \"s-1\",\n-                    \"dataReplicationInfo\": {\"dataReplicationState\": \"DISCONNECTED\"},\n+                    \"dataReplicationInfo\": {\n+                        \"dataReplicationState\": \"DISCONNECTED\"\n+                    },\n                     \"lifeCycle\": {\"state\": \"DISCONNECTED\"},\n                     \"sourceProperties\": {\n                         \"identificationHints\": {\"hostname\": \"server1\"}\n                     },\n                 }\n@@ -437,11 +471,13 @@\n         result = validate_server_replication_states(\"us-east-1\", [\"s-1\"])\n \n         assert result[\"valid\"] is False\n         assert result[\"unhealthyCount\"] == 1\n         assert result[\"unhealthyServers\"][0][\"serverId\"] == \"s-1\"\n-        assert result[\"unhealthyServers\"][0][\"replicationState\"] == \"DISCONNECTED\"\n+        assert (\n+            result[\"unhealthyServers\"][0][\"replicationState\"] == \"DISCONNECTED\"\n+        )\n \n     @patch(\"index.boto3.client\")\n     def test_empty_server_list(self, mock_boto_client):\n         \"\"\"Should be valid for empty server list.\"\"\"\n         from index import validate_server_replication_states\n@@ -480,11 +516,13 @@\n                 },\n             ]\n         }\n         mock_boto_client.return_value = mock_drs\n \n-        result = validate_server_replication_states(\"us-east-1\", [\"s-1\", \"s-2\"])\n+        result = validate_server_replication_states(\n+            \"us-east-1\", [\"s-1\", \"s-2\"]\n+        )\n \n         assert result[\"valid\"] is False\n         assert result[\"healthyCount\"] == 1\n         assert result[\"unhealthyCount\"] == 1\n \n--- tests/security/rbac_security_tests.py\t2026-01-02 15:55:23.701235+00:00\n+++ tests/security/rbac_security_tests.py\t2026-01-02 15:55:37.254418+00:00\n@@ -23,11 +23,12 @@\n import jwt\n import requests\n \n # Configure logging\n logging.basicConfig(\n-    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n+    level=logging.INFO,\n+    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n )\n logger = logging.getLogger(__name__)\n \n \n class RiskLevel(Enum):\n@@ -96,21 +97,25 @@\n         \"\"\"Load test configuration\"\"\"\n         try:\n             with open(config_file, \"r\") as f:\n                 return json.load(f)\n         except FileNotFoundError:\n-            logger.warning(f\"Config file {config_file} not found, using defaults\")\n+            logger.warning(\n+                f\"Config file {config_file} not found, using defaults\"\n+            )\n             return {\n                 \"api_base_url\": \"https://api.example.com\",\n                 \"aws_region\": \"us-east-1\",\n                 \"cognito_user_pool_id\": \"us-east-1_example\",\n                 \"cognito_client_id\": \"example_client_id\",\n             }\n \n     def _load_permission_matrix(self) -> Dict[str, Dict[str, bool]]:\n         \"\"\"Load the permission matrix for role validation\"\"\"\n-        matrix_file = Path(\"tests/security/test_scenarios/permission_matrix.json\")\n+        matrix_file = Path(\n+            \"tests/security/test_scenarios/permission_matrix.json\"\n+        )\n         if matrix_file.exists():\n             with open(matrix_file, \"r\") as f:\n                 return json.load(f)\n \n         # Default permission matrix\n@@ -221,39 +226,43 @@\n                 \"aws:read-only\",\n             ]\n \n             for role in roles:\n                 # Use email format for username (Cognito requirement)\n-                base_username = (\n-                    f\"security-test-{role.split(':')[1]}-{self.test_session_id[:8]}\"\n-                )\n+                base_username = f\"security-test-{role.split(':')[1]}-{self.test_session_id[:8]}\"\n                 email = f\"{base_username}@example.com\"\n \n                 user = TestUser(\n                     username=email,  # Use email as username for Cognito\n                     email=email,\n                     role=role,\n-                    permissions=list(self.permission_matrix.get(role, {}).keys()),\n+                    permissions=list(\n+                        self.permission_matrix.get(role, {}).keys()\n+                    ),\n                 )\n \n                 # Create user in Cognito\n                 if await self._create_test_user(user):\n                     self.test_users[role] = user\n                     logger.info(f\"Created test user for role: {role}\")\n                 else:\n-                    logger.error(f\"Failed to create test user for role: {role}\")\n+                    logger.error(\n+                        f\"Failed to create test user for role: {role}\"\n+                    )\n                     return False\n \n             # Wait for user creation to propagate\n             await asyncio.sleep(5)\n \n             # Authenticate all test users\n             for role, user in self.test_users.items():\n                 if await self._authenticate_user(user):\n                     logger.info(f\"Authenticated test user for role: {role}\")\n                 else:\n-                    logger.error(f\"Failed to authenticate test user for role: {role}\")\n+                    logger.error(\n+                        f\"Failed to authenticate test user for role: {role}\"\n+                    )\n                     return False\n \n             logger.info(\"Test environment setup completed successfully\")\n             return True\n \n@@ -292,21 +301,25 @@\n                 self.cognito_client.admin_add_user_to_group(\n                     UserPoolId=self.user_pool_id,\n                     Username=user.username,\n                     GroupName=group_name,\n                 )\n-                logger.info(f\"Added user {user.username} to group {group_name}\")\n+                logger.info(\n+                    f\"Added user {user.username} to group {group_name}\"\n+                )\n             except Exception as group_error:\n                 logger.warning(\n                     f\"Failed to add user to group {group_name}: {str(group_error)}\"\n                 )\n                 # Continue anyway - the user was created successfully\n \n             return True\n \n         except Exception as e:\n-            logger.error(f\"Failed to create test user {user.username}: {str(e)}\")\n+            logger.error(\n+                f\"Failed to create test user {user.username}: {str(e)}\"\n+            )\n             return False\n \n     async def _authenticate_user(self, user: TestUser) -> bool:\n         \"\"\"Authenticate a test user and get JWT token\"\"\"\n         try:\n@@ -322,11 +335,13 @@\n \n             user.token = response[\"AuthenticationResult\"][\"IdToken\"]\n             return True\n \n         except Exception as e:\n-            logger.error(f\"Failed to authenticate user {user.username}: {str(e)}\")\n+            logger.error(\n+                f\"Failed to authenticate user {user.username}: {str(e)}\"\n+            )\n             return False\n \n     async def run_security_tests(self) -> List[SecurityFinding]:\n         \"\"\"Run all security tests\"\"\"\n         logger.info(\"Starting comprehensive security testing...\")\n@@ -336,11 +351,13 @@\n         await self._run_privilege_escalation_tests()\n         await self._run_api_security_tests()\n         await self._run_data_access_tests()\n         await self._run_ui_security_tests()\n \n-        logger.info(f\"Security testing completed. Found {len(self.findings)} findings.\")\n+        logger.info(\n+            f\"Security testing completed. Found {len(self.findings)} findings.\"\n+        )\n         return self.findings\n \n     async def _run_permission_boundary_tests(self):\n         \"\"\"Test permission boundaries for all roles\"\"\"\n         logger.info(\"Running permission boundary tests...\")\n@@ -432,12 +449,18 @@\n                 \"POST\",\n                 \"/executions/test-execution/terminate-instances\",\n             ),\n             \"view_executions\": (\"GET\", \"/executions\"),\n             \"create_protection_groups\": (\"POST\", \"/protection-groups\"),\n-            \"delete_protection_groups\": (\"DELETE\", \"/protection-groups/test-group\"),\n-            \"modify_protection_groups\": (\"PUT\", \"/protection-groups/test-group\"),\n+            \"delete_protection_groups\": (\n+                \"DELETE\",\n+                \"/protection-groups/test-group\",\n+            ),\n+            \"modify_protection_groups\": (\n+                \"PUT\",\n+                \"/protection-groups/test-group\",\n+            ),\n             \"view_protection_groups\": (\"GET\", \"/protection-groups\"),\n             \"create_recovery_plans\": (\"POST\", \"/recovery-plans\"),\n             \"delete_recovery_plans\": (\"DELETE\", \"/recovery-plans/test-plan\"),\n             \"modify_recovery_plans\": (\"PUT\", \"/recovery-plans/test-plan\"),\n             \"view_recovery_plans\": (\"GET\", \"/recovery-plans\"),\n@@ -562,11 +585,13 @@\n                         affected_roles=[\"all\"],\n                         timestamp=datetime.datetime.utcnow().isoformat(),\n                     )\n                 )\n             else:\n-                logger.debug(\"JWT manipulation test passed - modified tokens rejected\")\n+                logger.debug(\n+                    \"JWT manipulation test passed - modified tokens rejected\"\n+                )\n \n         except Exception as e:\n             logger.error(f\"Error in JWT manipulation test: {str(e)}\")\n \n     async def _test_role_modification(self):\n@@ -700,11 +725,14 @@\n                     params=test.get(\"params\", {}),\n                     timeout=10,\n                 )\n \n                 # Check if tampering revealed unauthorized data\n-                if response.status_code == 200 and \"admin\" in response.text.lower():\n+                if (\n+                    response.status_code == 200\n+                    and \"admin\" in response.text.lower()\n+                ):\n                     self.findings.append(\n                         SecurityFinding(\n                             test_id=f\"{test_id}-{test['url']}\",\n                             title=\"Parameter Tampering Vulnerability\",\n                             description=f\"Parameter tampering on {test['url']} revealed unauthorized data\",\n@@ -843,11 +871,13 @@\n         test_id = \"DAC-001\"\n \n         try:\n             # This would require setting up test accounts\n             # For now, we'll simulate the test\n-            logger.info(\"Cross-account access test would require test account setup\")\n+            logger.info(\n+                \"Cross-account access test would require test account setup\"\n+            )\n \n         except Exception as e:\n             logger.error(f\"Error in cross-account access test: {str(e)}\")\n \n     async def _test_data_leakage(self):\n@@ -872,11 +902,13 @@\n                 \"/executions\",\n             ]\n \n             for endpoint in endpoints:\n                 response = requests.get(\n-                    f\"{self.api_base_url}{endpoint}\", headers=headers, timeout=10\n+                    f\"{self.api_base_url}{endpoint}\",\n+                    headers=headers,\n+                    timeout=10,\n                 )\n \n                 if response.status_code == 200:\n                     # Check for sensitive data patterns\n                     sensitive_patterns = [\n@@ -897,11 +929,14 @@\n                                     test_id=f\"{test_id}-{endpoint}\",\n                                     title=\"Sensitive Data Exposure\",\n                                     description=f\"Sensitive data pattern '{pattern}' found in {endpoint} response\",\n                                     risk_level=RiskLevel.MEDIUM,\n                                     status=TestStatus.FAIL,\n-                                    details={\"endpoint\": endpoint, \"pattern\": pattern},\n+                                    details={\n+                                        \"endpoint\": endpoint,\n+                                        \"pattern\": pattern,\n+                                    },\n                                     remediation=\"Remove sensitive data from API responses\",\n                                     affected_roles=[\"aws:read-only\"],\n                                     timestamp=datetime.datetime.utcnow().isoformat(),\n                                 )\n                             )\n@@ -913,11 +948,13 @@\n         \"\"\"Test UI security controls\"\"\"\n         logger.info(\"Running UI security tests...\")\n \n         # Note: UI security tests would typically require browser automation\n         # This is a placeholder for the framework\n-        logger.info(\"UI security tests would require browser automation framework\")\n+        logger.info(\n+            \"UI security tests would require browser automation framework\"\n+        )\n \n     async def cleanup_test_environment(self):\n         \"\"\"Clean up test environment\"\"\"\n         logger.info(\"Cleaning up test environment...\")\n \n@@ -950,14 +987,22 @@\n             if risk not in findings_by_risk:\n                 findings_by_risk[risk] = []\n             findings_by_risk[risk].append(asdict(finding))\n \n         # Calculate summary statistics\n-        total_tests = len([f for f in self.findings if f.status != TestStatus.SKIP])\n-        passed_tests = len([f for f in self.findings if f.status == TestStatus.PASS])\n-        failed_tests = len([f for f in self.findings if f.status == TestStatus.FAIL])\n-        error_tests = len([f for f in self.findings if f.status == TestStatus.ERROR])\n+        total_tests = len(\n+            [f for f in self.findings if f.status != TestStatus.SKIP]\n+        )\n+        passed_tests = len(\n+            [f for f in self.findings if f.status == TestStatus.PASS]\n+        )\n+        failed_tests = len(\n+            [f for f in self.findings if f.status == TestStatus.FAIL]\n+        )\n+        error_tests = len(\n+            [f for f in self.findings if f.status == TestStatus.ERROR]\n+        )\n \n         # Generate report\n         report = {\n             \"metadata\": {\n                 \"test_session_id\": self.test_session_id,\n@@ -983,11 +1028,14 @@\n             \"detailed_findings\": [asdict(f) for f in self.findings],\n             \"test_coverage\": {\n                 \"roles_tested\": list(self.test_users.keys()),\n                 \"permissions_tested\": list(\n                     set().union(\n-                        *[user.permissions for user in self.test_users.values()]\n+                        *[\n+                            user.permissions\n+                            for user in self.test_users.values()\n+                        ]\n                     )\n                 ),\n                 \"test_categories\": [\n                     \"Permission Boundary Tests\",\n                     \"Privilege Escalation Tests\",\n@@ -1008,11 +1056,13 @@\n \n         # Analyze findings and generate recommendations\n         critical_findings = [\n             f for f in self.findings if f.risk_level == RiskLevel.CRITICAL\n         ]\n-        high_findings = [f for f in self.findings if f.risk_level == RiskLevel.HIGH]\n+        high_findings = [\n+            f for f in self.findings if f.risk_level == RiskLevel.HIGH\n+        ]\n \n         if critical_findings:\n             recommendations.append(\n                 {\n                     \"priority\": \"IMMEDIATE\",\n@@ -1060,14 +1110,20 @@\n \n         return recommendations\n \n     def _assess_compliance(self) -> Dict[str, Any]:\n         \"\"\"Assess compliance with security standards\"\"\"\n-        total_tests = len([f for f in self.findings if f.status != TestStatus.SKIP])\n-        passed_tests = len([f for f in self.findings if f.status == TestStatus.PASS])\n-\n-        compliance_score = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n+        total_tests = len(\n+            [f for f in self.findings if f.status != TestStatus.SKIP]\n+        )\n+        passed_tests = len(\n+            [f for f in self.findings if f.status == TestStatus.PASS]\n+        )\n+\n+        compliance_score = (\n+            (passed_tests / total_tests * 100) if total_tests > 0 else 0\n+        )\n \n         # Determine compliance status\n         if compliance_score >= 95:\n             status = \"COMPLIANT\"\n         elif compliance_score >= 80:\n@@ -1081,17 +1137,21 @@\n             \"overall_status\": status,\n             \"compliance_score\": compliance_score,\n             \"standards\": {\n                 \"AWS_Security_Standards\": status,\n                 \"Least_Privilege_Principle\": \"COMPLIANT\"\n-                if not any(f.risk_level == RiskLevel.CRITICAL for f in self.findings)\n+                if not any(\n+                    f.risk_level == RiskLevel.CRITICAL for f in self.findings\n+                )\n                 else \"NON_COMPLIANT\",\n                 \"Defense_in_Depth\": \"COMPLIANT\"\n                 if compliance_score >= 80\n                 else \"NON_COMPLIANT\",\n             },\n-            \"gaps\": [f.title for f in self.findings if f.status == TestStatus.FAIL],\n+            \"gaps\": [\n+                f.title for f in self.findings if f.status == TestStatus.FAIL\n+            ],\n         }\n \n \n async def main():\n     \"\"\"Main execution function\"\"\"\n@@ -1108,13 +1168,11 @@\n \n         # Generate report\n         report = tester.generate_security_report()\n \n         # Save report\n-        report_file = (\n-            f\"tests/security/reports/security_report_{tester.test_session_id}.json\"\n-        )\n+        report_file = f\"tests/security/reports/security_report_{tester.test_session_id}.json\"\n         Path(report_file).parent.mkdir(parents=True, exist_ok=True)\n \n         with open(report_file, \"w\") as f:\n             json.dump(report, f, indent=2)\n \n@@ -1126,17 +1184,23 @@\n         print(\"=\" * 60)\n         print(f\"Total Tests: {report['executive_summary']['total_tests']}\")\n         print(f\"Passed: {report['executive_summary']['passed_tests']}\")\n         print(f\"Failed: {report['executive_summary']['failed_tests']}\")\n         print(f\"Errors: {report['executive_summary']['error_tests']}\")\n-        print(f\"Success Rate: {report['executive_summary']['success_rate']:.1f}%\")\n+        print(\n+            f\"Success Rate: {report['executive_summary']['success_rate']:.1f}%\"\n+        )\n         print(f\"\\nFindings by Risk Level:\")\n-        print(f\"  Critical: {report['executive_summary']['critical_findings']}\")\n+        print(\n+            f\"  Critical: {report['executive_summary']['critical_findings']}\"\n+        )\n         print(f\"  High: {report['executive_summary']['high_findings']}\")\n         print(f\"  Medium: {report['executive_summary']['medium_findings']}\")\n         print(f\"  Low: {report['executive_summary']['low_findings']}\")\n-        print(f\"\\nCompliance Status: {report['compliance_status']['overall_status']}\")\n+        print(\n+            f\"\\nCompliance Status: {report['compliance_status']['overall_status']}\"\n+        )\n         print(\n             f\"Compliance Score: {report['compliance_status']['compliance_score']:.1f}%\"\n         )\n         print(\"=\" * 60)\n \n--- lambda/index.py\t2026-01-02 15:52:48.973001+00:00\n+++ lambda/index.py\t2026-01-02 15:55:39.292892+00:00\n@@ -167,11 +167,13 @@\n                 )\n         except Exception as e:\n             # In test environment, use environment variable fallback\n             current_account_id = os.environ.get(\"AWS_ACCOUNT_ID\")\n             if not current_account_id:\n-                raise ValueError(f\"Unable to determine current account ID: {e}\")\n+                raise ValueError(\n+                    f\"Unable to determine current account ID: {e}\"\n+                )\n             print(\n                 f\"Using AWS_ACCOUNT_ID environment variable for current account: {current_account_id}\"\n             )\n \n         waves = plan.get(\"Waves\", [])\n@@ -184,11 +186,13 @@\n             if not pg_id:\n                 continue\n \n             try:\n                 # Get Protection Group to check its AccountId\n-                pg_result = protection_groups_table.get_item(Key={\"GroupId\": pg_id})\n+                pg_result = protection_groups_table.get_item(\n+                    Key={\"GroupId\": pg_id}\n+                )\n                 if \"Item\" in pg_result:\n                     pg = pg_result[\"Item\"]\n                     account_id = pg.get(\"AccountId\")\n                     if account_id and account_id.strip():\n                         all_target_account_ids.add(account_id.strip())\n@@ -211,11 +215,13 @@\n         if not all_target_account_ids or (\n             len(all_target_account_ids) == 1\n             and current_account_id in all_target_account_ids\n         ):\n             # All protection groups are in current account (or no protection groups found)\n-            print(f\"All Protection Groups are in current account {current_account_id}\")\n+            print(\n+                f\"All Protection Groups are in current account {current_account_id}\"\n+            )\n             return {\n                 \"AccountId\": current_account_id,\n                 \"AssumeRoleName\": None,  # No role assumption needed for current account\n                 \"isCurrentAccount\": True,\n             }\n@@ -231,11 +237,13 @@\n                 )\n                 if \"Item\" in account_result:\n                     account_config = account_result[\"Item\"]\n                     assume_role_name = (\n                         account_config.get(\"AssumeRoleName\")\n-                        or account_config.get(\"CrossAccountRoleArn\", \"\").split(\"/\")[-1]\n+                        or account_config.get(\"CrossAccountRoleArn\", \"\").split(\n+                            \"/\"\n+                        )[-1]\n                     )\n \n                     print(\n                         f\"Using target account {target_account_id} with role {assume_role_name}\"\n                     )\n@@ -252,11 +260,13 @@\n                 print(\n                     f\"Error getting target account configuration for {target_account_id}: {e}\"\n                 )\n \n         # Fallback: target account found but no configuration - assume standard role name\n-        print(f\"Using target account {target_account_id} with default role name\")\n+        print(\n+            f\"Using target account {target_account_id} with default role name\"\n+        )\n         return {\n             \"AccountId\": target_account_id,\n             \"AssumeRoleName\": \"DRSOrchestrationCrossAccountRole\",  # Default role name\n             \"isCurrentAccount\": False,\n         }\n@@ -325,11 +335,13 @@\n             aws_access_key_id=credentials[\"AccessKeyId\"],\n             aws_secret_access_key=credentials[\"SecretAccessKey\"],\n             aws_session_token=credentials[\"SessionToken\"],\n         )\n \n-        print(f\"Successfully created cross-account DRS client for account {account_id}\")\n+        print(\n+            f\"Successfully created cross-account DRS client for account {account_id}\"\n+        )\n         return drs_client\n \n     except Exception as e:\n         # FIX #3: Don't fall back silently - raise clear error messages\n         error_msg = f\"Failed to assume cross-account role for account {account_id}: {e}\"\n@@ -348,11 +360,13 @@\n \n         print(f\"Cross-account role assumption failed: {error_msg}\")\n         raise RuntimeError(error_msg)\n \n \n-def response(status_code: int, body: Any, headers: Optional[Dict] = None) -> Dict:\n+def response(\n+    status_code: int, body: Any, headers: Optional[Dict] = None\n+) -> Dict:\n     \"\"\"Generate API Gateway response with CORS headers\"\"\"\n     default_headers = {\n         \"Content-Type\": \"application/json\",\n         \"Access-Control-Allow-Origin\": \"*\",\n         \"Access-Control-Allow-Headers\": \"Content-Type,Authorization\",\n@@ -408,18 +422,22 @@\n         # Get all jobs (no filter = all jobs)\n         # Note: describe_jobs without filters returns recent jobs\n         jobs_response = drs_client.describe_jobs(maxResults=100)\n \n         jobs_found = jobs_response.get(\"items\", [])\n-        print(f\"[DRS Job Check] Found {len(jobs_found)} total jobs in {region}\")\n+        print(\n+            f\"[DRS Job Check] Found {len(jobs_found)} total jobs in {region}\"\n+        )\n \n         for job in jobs_found:\n             job_id = job.get(\"jobID\")\n             job_status = job.get(\"status\", \"\")\n             job_type = job.get(\"type\", \"\")\n \n-            print(f\"[DRS Job Check] Job {job_id}: type={job_type}, status={job_status}\")\n+            print(\n+                f\"[DRS Job Check] Job {job_id}: type={job_type}, status={job_status}\"\n+            )\n \n             # Only check active LAUNCH jobs (not TERMINATE, CREATE_CONVERTED_SNAPSHOT, etc.)\n             if job_type == \"LAUNCH\" and job_status in DRS_ACTIVE_JOB_STATUSES:\n                 # Get servers participating in this job\n                 for server in job.get(\"participatingServers\", []):\n@@ -427,11 +445,13 @@\n                     if server_id:\n                         servers_in_drs_jobs[server_id] = {\n                             \"jobId\": job_id,\n                             \"jobStatus\": job_status,\n                             \"jobType\": job_type,\n-                            \"launchStatus\": server.get(\"launchStatus\", \"UNKNOWN\"),\n+                            \"launchStatus\": server.get(\n+                                \"launchStatus\", \"UNKNOWN\"\n+                            ),\n                         }\n                         print(\n                             f\"[DRS Job Check] Server {server_id} in active job {job_id}\"\n                         )\n \n@@ -509,11 +529,13 @@\n         for plan_id in plan_ids_with_pg:\n             active_executions = get_active_executions_for_plan(plan_id)\n             if active_executions:\n                 exec_info = active_executions[0]\n                 # Get plan name\n-                plan = next((p for p in all_plans if p.get(\"PlanId\") == plan_id), {})\n+                plan = next(\n+                    (p for p in all_plans if p.get(\"PlanId\") == plan_id), {}\n+                )\n                 return {\n                     \"executionId\": exec_info.get(\"ExecutionId\"),\n                     \"planId\": plan_id,\n                     \"planName\": plan.get(\"PlanName\", \"Unknown\"),\n                     \"status\": exec_info.get(\"Status\"),\n@@ -545,13 +567,17 @@\n                 ]:\n                     print(\n                         f\"StatusIndex GSI not available for status {status}: {error_code}\"\n                     )\n                 else:\n-                    print(f\"DynamoDB error querying StatusIndex for {status}: {e}\")\n+                    print(\n+                        f\"DynamoDB error querying StatusIndex for {status}: {e}\"\n+                    )\n             except Exception as e:\n-                print(f\"Unexpected error querying StatusIndex for {status}: {e}\")\n+                print(\n+                    f\"Unexpected error querying StatusIndex for {status}: {e}\"\n+                )\n \n         # If no results from GSI, fallback to scan\n         if not active_executions:\n             result = execution_history_table.scan()\n             all_executions = result.get(\"Items\", [])\n@@ -612,11 +638,13 @@\n \n         # For ALL active executions, resolve servers from the recovery plan's protection groups\n         # This ensures we catch conflicts even if ServerStatuses isn't populated yet\n         if plan_id not in plan_cache:\n             try:\n-                plan_result = recovery_plans_table.get_item(Key={\"PlanId\": plan_id})\n+                plan_result = recovery_plans_table.get_item(\n+                    Key={\"PlanId\": plan_id}\n+                )\n                 plan_cache[plan_id] = plan_result.get(\"Item\", {})\n             except Exception as e:\n                 print(f\"Error fetching plan {plan_id}: {e}\")\n                 plan_cache[plan_id] = {}\n \n@@ -637,11 +665,13 @@\n                 or (wave.get(\"ProtectionGroupIds\", []) or [None])[0]\n             )\n \n             if pg_id:\n                 # Resolve servers from Protection Group (handles both tags and explicit IDs)\n-                server_ids = resolve_pg_servers_for_conflict_check(pg_id, pg_cache)\n+                server_ids = resolve_pg_servers_for_conflict_check(\n+                    pg_id, pg_cache\n+                )\n                 for server_id in server_ids:\n                     # Only add if not already tracked (ServerStatuses takes precedence)\n                     if server_id not in servers_in_use:\n                         servers_in_use[server_id] = {\n                             \"executionId\": execution_id,\n@@ -689,11 +719,13 @@\n             # Resolve servers by tags\n             resolved = query_drs_servers_by_tags(\n                 region, selection_tags, account_context\n             )\n             server_ids = [\n-                s.get(\"sourceServerID\") for s in resolved if s.get(\"sourceServerID\")\n+                s.get(\"sourceServerID\")\n+                for s in resolved\n+                if s.get(\"sourceServerID\")\n             ]\n         elif source_server_ids:\n             # Use explicit server IDs\n             server_ids = source_server_ids\n         else:\n@@ -716,11 +748,13 @@\n     Returns list of conflicts with details.\n \n     For tag-based Protection Groups, resolves servers at check time.\n     Also checks DRS directly for active LAUNCH jobs (catches stale DynamoDB records).\n     \"\"\"\n-    print(f\"[Conflict Check] Starting conflict check for plan {plan.get('PlanId')}\")\n+    print(\n+        f\"[Conflict Check] Starting conflict check for plan {plan.get('PlanId')}\"\n+    )\n     print(f\"[Conflict Check] Account context: {account_context}\")\n \n     servers_in_use = get_servers_in_active_executions()\n     print(\n         f\"[Conflict Check] Servers in active executions: {list(servers_in_use.keys())}\"\n@@ -743,11 +777,13 @@\n \n         if pg_id:\n             # Get PG to find region\n             if pg_id not in pg_cache:\n                 try:\n-                    pg_result = protection_groups_table.get_item(Key={\"GroupId\": pg_id})\n+                    pg_result = protection_groups_table.get_item(\n+                        Key={\"GroupId\": pg_id}\n+                    )\n                     pg_cache[pg_id] = pg_result.get(\"Item\", {})\n                 except Exception as e:\n                     print(f\"[Conflict Check] Error fetching PG {pg_id}: {e}\")\n                     pg_cache[pg_id] = {}\n \n@@ -774,19 +810,27 @@\n \n             for server_id in server_ids:\n                 # Check DynamoDB execution conflicts\n                 if server_id in servers_in_use:\n                     conflict_info = servers_in_use[server_id]\n-                    print(f\"[Conflict Check] Server {server_id} has execution conflict\")\n+                    print(\n+                        f\"[Conflict Check] Server {server_id} has execution conflict\"\n+                    )\n                     conflicts.append(\n                         {\n                             \"serverId\": server_id,\n                             \"waveName\": wave_name,\n-                            \"conflictingExecutionId\": conflict_info[\"executionId\"],\n+                            \"conflictingExecutionId\": conflict_info[\n+                                \"executionId\"\n+                            ],\n                             \"conflictingPlanId\": conflict_info[\"planId\"],\n-                            \"conflictingWaveName\": conflict_info.get(\"waveName\"),\n-                            \"conflictingStatus\": conflict_info.get(\"executionStatus\"),\n+                            \"conflictingWaveName\": conflict_info.get(\n+                                \"waveName\"\n+                            ),\n+                            \"conflictingStatus\": conflict_info.get(\n+                                \"executionStatus\"\n+                            ),\n                             \"conflictSource\": \"execution\",\n                         }\n                     )\n                 # Check DRS job conflicts (even if not in DynamoDB)\n                 elif server_id in drs_servers_by_region.get(region, {}):\n@@ -798,11 +842,13 @@\n                         {\n                             \"serverId\": server_id,\n                             \"waveName\": wave_name,\n                             \"conflictingJobId\": drs_info[\"jobId\"],\n                             \"conflictingJobStatus\": drs_info[\"jobStatus\"],\n-                            \"conflictingLaunchStatus\": drs_info.get(\"launchStatus\"),\n+                            \"conflictingLaunchStatus\": drs_info.get(\n+                                \"launchStatus\"\n+                            ),\n                             \"conflictSource\": \"drs_job\",\n                         }\n                     )\n \n     print(f\"[Conflict Check] Total conflicts found: {len(conflicts)}\")\n@@ -861,21 +907,27 @@\n                 or (wave.get(\"ProtectionGroupIds\", []) or [None])[0]\n             )\n \n             if pg_id:\n                 # Resolve servers from Protection Group tags\n-                server_ids = resolve_pg_servers_for_conflict_check(pg_id, pg_cache)\n+                server_ids = resolve_pg_servers_for_conflict_check(\n+                    pg_id, pg_cache\n+                )\n \n                 for server_id in server_ids:\n                     if server_id in servers_in_use:\n                         conflict_info = servers_in_use[server_id]\n                         # Don't count as conflict if it's this plan's own execution\n                         if conflict_info[\"planId\"] != plan_id:\n                             conflicting_servers.append(server_id)\n-                            conflicting_execution_id = conflict_info[\"executionId\"]\n+                            conflicting_execution_id = conflict_info[\n+                                \"executionId\"\n+                            ]\n                             conflicting_plan_id = conflict_info[\"planId\"]\n-                            conflicting_status = conflict_info.get(\"executionStatus\")\n+                            conflicting_status = conflict_info.get(\n+                                \"executionStatus\"\n+                            )\n \n         # Also check DRS jobs for this plan's regions\n         drs_conflicting_servers = []\n         drs_conflicting_job_id = None\n         pg_metadata_cache = {}  # Separate cache for PG metadata (region, etc.)\n@@ -899,26 +951,30 @@\n                 pg_metadata = pg_metadata_cache.get(pg_id, {})\n                 region = pg_metadata.get(\"Region\", \"us-east-1\")\n \n                 # Lazy load DRS jobs per region\n                 if region not in drs_servers_by_region:\n-                    drs_servers_by_region[region] = get_servers_in_active_drs_jobs(\n+                    drs_servers_by_region[\n                         region\n-                    )\n-\n-                server_ids = resolve_pg_servers_for_conflict_check(pg_id, pg_cache)\n+                    ] = get_servers_in_active_drs_jobs(region)\n+\n+                server_ids = resolve_pg_servers_for_conflict_check(\n+                    pg_id, pg_cache\n+                )\n                 for server_id in server_ids:\n                     # Skip if already in execution conflict\n                     if (\n                         server_id not in conflicting_servers\n                         and server_id in drs_servers_by_region.get(region, {})\n                     ):\n                         drs_info = drs_servers_by_region[region][server_id]\n                         drs_conflicting_servers.append(server_id)\n                         drs_conflicting_job_id = drs_info[\"jobId\"]\n \n-        all_conflicting = list(set(conflicting_servers + drs_conflicting_servers))\n+        all_conflicting = list(\n+            set(conflicting_servers + drs_conflicting_servers)\n+        )\n \n         if all_conflicting:\n             # Build reason message\n             if conflicting_servers and drs_conflicting_servers:\n                 reason = f\"{len(set(conflicting_servers))} server(s) in execution, {len(set(drs_conflicting_servers))} in DRS jobs\"\n@@ -1002,11 +1058,13 @@\n                     active_jobs.append(\n                         {\n                             \"jobId\": job.get(\"jobID\"),\n                             \"status\": job.get(\"status\"),\n                             \"type\": job.get(\"type\"),\n-                            \"serverCount\": len(job.get(\"participatingServers\", [])),\n+                            \"serverCount\": len(\n+                                job.get(\"participatingServers\", [])\n+                            ),\n                         }\n                     )\n \n         current_count = len(active_jobs)\n         available_slots = DRS_LIMITS[\"MAX_CONCURRENT_JOBS\"] - current_count\n@@ -1107,11 +1165,13 @@\n             \"currentServersInJobs\": None,\n             \"maxServers\": DRS_LIMITS[\"MAX_SERVERS_IN_ALL_JOBS\"],\n         }\n \n \n-def validate_server_replication_states(region: str, server_ids: List[str]) -> Dict:\n+def validate_server_replication_states(\n+    region: str, server_ids: List[str]\n+) -> Dict:\n     \"\"\"\n     Validate that all servers have healthy replication state for recovery.\n     Returns validation result with unhealthy servers list.\n     \"\"\"\n     if not server_ids:\n@@ -1139,11 +1199,13 @@\n             for server in response.get(\"items\", []):\n                 server_id = server.get(\"sourceServerID\")\n                 replication_state = server.get(\"dataReplicationInfo\", {}).get(\n                     \"dataReplicationState\", \"UNKNOWN\"\n                 )\n-                lifecycle_state = server.get(\"lifeCycle\", {}).get(\"state\", \"UNKNOWN\")\n+                lifecycle_state = server.get(\"lifeCycle\", {}).get(\n+                    \"state\", \"UNKNOWN\"\n+                )\n \n                 if (\n                     replication_state in INVALID_REPLICATION_STATES\n                     or lifecycle_state == \"STOPPED\"\n                 ):\n@@ -1276,14 +1338,18 @@\n \n         # Determine capacity status\n         if replicating_servers >= DRS_LIMITS[\"MAX_REPLICATING_SERVERS\"]:\n             status = \"CRITICAL\"\n             message = f\"Account at hard limit: {replicating_servers}/{DRS_LIMITS['MAX_REPLICATING_SERVERS']} replicating servers in {region}\"\n-        elif replicating_servers >= DRS_LIMITS[\"CRITICAL_REPLICATING_THRESHOLD\"]:\n+        elif (\n+            replicating_servers >= DRS_LIMITS[\"CRITICAL_REPLICATING_THRESHOLD\"]\n+        ):\n             status = \"WARNING\"\n             message = f\"Approaching hard limit: {replicating_servers}/{DRS_LIMITS['MAX_REPLICATING_SERVERS']} replicating servers in {region}\"\n-        elif replicating_servers >= DRS_LIMITS[\"WARNING_REPLICATING_THRESHOLD\"]:\n+        elif (\n+            replicating_servers >= DRS_LIMITS[\"WARNING_REPLICATING_THRESHOLD\"]\n+        ):\n             status = \"INFO\"\n             message = f\"Monitor capacity: {replicating_servers}/{DRS_LIMITS['MAX_REPLICATING_SERVERS']} replicating servers in {region}\"\n         else:\n             status = \"OK\"\n             message = f\"Capacity OK: {replicating_servers}/{DRS_LIMITS['MAX_REPLICATING_SERVERS']} replicating servers in {region}\"\n@@ -1324,11 +1390,13 @@\n             return {\n                 \"totalSourceServers\": 0,\n                 \"replicatingServers\": 0,\n                 \"maxReplicatingServers\": DRS_LIMITS[\"MAX_REPLICATING_SERVERS\"],\n                 \"maxSourceServers\": DRS_LIMITS[\"MAX_SOURCE_SERVERS\"],\n-                \"availableReplicatingSlots\": DRS_LIMITS[\"MAX_REPLICATING_SERVERS\"],\n+                \"availableReplicatingSlots\": DRS_LIMITS[\n+                    \"MAX_REPLICATING_SERVERS\"\n+                ],\n                 \"status\": \"NOT_INITIALIZED\",\n                 \"message\": f\"DRS not initialized in {region}. Initialize DRS in the AWS Console to use this region.\",\n             }\n         elif (\n             \"UnrecognizedClientException\" in error_str\n@@ -1337,11 +1405,13 @@\n             return {\n                 \"totalSourceServers\": 0,\n                 \"replicatingServers\": 0,\n                 \"maxReplicatingServers\": DRS_LIMITS[\"MAX_REPLICATING_SERVERS\"],\n                 \"maxSourceServers\": DRS_LIMITS[\"MAX_SOURCE_SERVERS\"],\n-                \"availableReplicatingSlots\": DRS_LIMITS[\"MAX_REPLICATING_SERVERS\"],\n+                \"availableReplicatingSlots\": DRS_LIMITS[\n+                    \"MAX_REPLICATING_SERVERS\"\n+                ],\n                 \"status\": \"NOT_INITIALIZED\",\n                 \"message\": f\"DRS not initialized in {region}. Initialize DRS in the AWS Console to use this region.\",\n             }\n         elif (\n             \"AccessDeniedException\" in error_str\n@@ -1393,19 +1463,24 @@\n \n         # CRITICAL: Check for EventBridge-triggered tag sync BEFORE authentication\n         if path == \"/drs/tag-sync\" and http_method == \"POST\":\n             # Enhanced EventBridge validation with multiple security checks\n             source_ip = (\n-                event.get(\"requestContext\", {}).get(\"identity\", {}).get(\"sourceIp\", \"\")\n+                event.get(\"requestContext\", {})\n+                .get(\"identity\", {})\n+                .get(\"sourceIp\", \"\")\n             )\n             invocation_source = event.get(\"invocationSource\", \"\")\n             user_agent = event.get(\"headers\", {}).get(\"User-Agent\", \"\")\n \n             # Multiple validation criteria for EventBridge requests\n             is_eventbridge_request = (\n                 # Primary indicators\n-                (source_ip == \"eventbridge\" or invocation_source == \"EVENTBRIDGE\")\n+                (\n+                    source_ip == \"eventbridge\"\n+                    or invocation_source == \"EVENTBRIDGE\"\n+                )\n                 and\n                 # Additional security checks\n                 (\n                     # EventBridge requests typically have no Authorization header\n                     not event.get(\"headers\", {}).get(\"Authorization\")\n@@ -1444,11 +1519,13 @@\n         print(f\"Checking authentication for path: {path}\")\n \n         # Skip authentication check for health endpoint\n         if path != \"/health\":\n             # Validate authentication - check for Cognito authorizer context\n-            auth_context = event.get(\"requestContext\", {}).get(\"authorizer\", {})\n+            auth_context = event.get(\"requestContext\", {}).get(\n+                \"authorizer\", {}\n+            )\n             claims = auth_context.get(\"claims\", {})\n             print(\n                 f\"Auth validation - path: {path}, auth_context: {auth_context}, claims: {claims}\"\n             )\n \n@@ -1473,11 +1550,13 @@\n                 return response(\n                     403,\n                     {\n                         \"error\": \"Forbidden\",\n                         \"message\": auth_result[\"reason\"],\n-                        \"requiredPermission\": auth_result.get(\"required_permission\"),\n+                        \"requiredPermission\": auth_result.get(\n+                            \"required_permission\"\n+                        ),\n                         \"userRoles\": auth_result.get(\"user_roles\", []),\n                     },\n                 )\n \n             print(\n@@ -1506,11 +1585,13 @@\n             plan_id = path_parameters.get(\"id\")\n             body[\"PlanId\"] = plan_id\n             if \"InitiatedBy\" not in body:\n                 body[\"InitiatedBy\"] = \"system\"\n             return execute_recovery_plan(body)\n-        elif \"/check-existing-instances\" in path and path.startswith(\"/recovery-plans\"):\n+        elif \"/check-existing-instances\" in path and path.startswith(\n+            \"/recovery-plans\"\n+        ):\n             print(\"Matched /recovery-plans check-existing-instances route\")\n             # Handle /recovery-plans/{planId}/check-existing-instances endpoint\n             plan_id = path_parameters.get(\"id\")\n             return check_existing_recovery_instances(plan_id)\n         elif path.startswith(\"/recovery-plans\"):\n@@ -1533,13 +1614,17 @@\n             return handle_drs_quotas(query_parameters)\n         elif path.startswith(\"/drs/accounts\"):\n             print(f\"Matched /drs/accounts route, calling handle_drs_accounts\")\n             return handle_drs_accounts(query_parameters)\n         elif path.startswith(\"/accounts/targets\"):\n-            return handle_target_accounts(path, http_method, body, query_parameters)\n+            return handle_target_accounts(\n+                path, http_method, body, query_parameters\n+            )\n         elif path == \"/accounts/current\" and http_method == \"GET\":\n-            print(f\"Matched /accounts/current route, calling get_current_account_info\")\n+            print(\n+                f\"Matched /accounts/current route, calling get_current_account_info\"\n+            )\n             return get_current_account_info()\n         elif path == \"/drs/tag-sync\" and http_method == \"POST\":\n             print(\"Matched /drs/tag-sync route - manual tag sync request\")\n             return handle_drs_tag_sync(body)\n         elif path.startswith(\"/ec2/\"):\n@@ -1550,39 +1635,49 @@\n             return handle_config(http_method, path, body, query_parameters)\n         elif path == \"/user/permissions\" and http_method == \"GET\":\n             print(\"Matched /user/permissions route\")\n             return handle_user_permissions(event)\n         else:\n-            print(f\"No route matched for path: '{path}' - checking all conditions:\")\n+            print(\n+                f\"No route matched for path: '{path}' - checking all conditions:\"\n+            )\n             print(f\"  path == '/health': {path == '/health'}\")\n             print(\n                 f\"  path.startswith('/protection-groups'): {path.startswith('/protection-groups')}\"\n             )\n             print(\n                 f\"  path.startswith('/recovery-plans'): {path.startswith('/recovery-plans')}\"\n             )\n-            print(f\"  path.startswith('/executions'): {path.startswith('/executions')}\")\n+            print(\n+                f\"  path.startswith('/executions'): {path.startswith('/executions')}\"\n+            )\n             print(\n                 f\"  path.startswith('/drs/source-servers'): {path.startswith('/drs/source-servers')}\"\n             )\n-            print(f\"  path.startswith('/drs/quotas'): {path.startswith('/drs/quotas')}\")\n+            print(\n+                f\"  path.startswith('/drs/quotas'): {path.startswith('/drs/quotas')}\"\n+            )\n             print(\n                 f\"  path.startswith('/drs/accounts'): {path.startswith('/drs/accounts')}\"\n             )\n             print(\n                 f\"  path.startswith('/accounts/targets'): {path.startswith('/accounts/targets')}\"\n             )\n             print(f\"  path.startswith('/ec2/'): {path.startswith('/ec2/')}\")\n-            print(f\"  path.startswith('/config'): {path.startswith('/config')}\")\n+            print(\n+                f\"  path.startswith('/config'): {path.startswith('/config')}\"\n+            )\n             return response(404, {\"error\": \"Not Found\", \"path\": path})\n \n     except Exception as e:\n         print(f\"Error: {str(e)}\")\n         import traceback\n \n         traceback.print_exc()\n-        return response(500, {\"error\": \"Internal Server Error\", \"message\": str(e)})\n+        return response(\n+            500, {\"error\": \"Internal Server Error\", \"message\": str(e)}\n+        )\n \n \n # ============================================================================\n # Protection Groups Handlers\n # ============================================================================\n@@ -1646,11 +1741,13 @@\n                 \"AccountId\": body.get(\"AccountId\"),\n                 \"AssumeRoleName\": body.get(\"AssumeRoleName\"),\n             }\n \n         # Query DRS for servers matching tags\n-        resolved_servers = query_drs_servers_by_tags(region, tags, account_context)\n+        resolved_servers = query_drs_servers_by_tags(\n+            region, tags, account_context\n+        )\n \n         return response(\n             200,\n             {\n                 \"region\": region,\n@@ -1779,12 +1876,16 @@\n                 # Debug: Log the source properties structure for this server\n                 print(\n                     f\"DEBUG: Server {server.get('sourceServerID')} source properties keys: {list(source_props.keys())}\"\n                 )\n                 print(f\"DEBUG: CPUs data: {cpus}\")\n-                print(f\"DEBUG: RAM bytes: {source_props.get('ramBytes', 'NOT_FOUND')}\")\n-                print(f\"DEBUG: Disks data: {source_props.get('disks', 'NOT_FOUND')}\")\n+                print(\n+                    f\"DEBUG: RAM bytes: {source_props.get('ramBytes', 'NOT_FOUND')}\"\n+                )\n+                print(\n+                    f\"DEBUG: Disks data: {source_props.get('disks', 'NOT_FOUND')}\"\n+                )\n \n                 for cpu in cpus:\n                     cpu_info.append(\n                         {\n                             \"modelName\": cpu.get(\"modelName\", \"Unknown\"),\n@@ -1812,11 +1913,13 @@\n                             if disk_bytes\n                             else 0,\n                         }\n                     )\n                 total_disk_gib = (\n-                    round(total_disk_bytes / (1024**3), 1) if total_disk_bytes else 0\n+                    round(total_disk_bytes / (1024**3), 1)\n+                    if total_disk_bytes\n+                    else 0\n                 )\n \n                 # Extract OS info\n                 os_info = source_props.get(\"os\", {})\n                 os_string = os_info.get(\"fullString\", \"\")\n@@ -1961,16 +2064,20 @@\n         selection_tags = body.get(\"ServerSelectionTags\", {})\n         source_server_ids = body.get(\"SourceServerIds\", [])\n \n         # Must have at least one selection method\n         has_tags = isinstance(selection_tags, dict) and len(selection_tags) > 0\n-        has_servers = isinstance(source_server_ids, list) and len(source_server_ids) > 0\n+        has_servers = (\n+            isinstance(source_server_ids, list) and len(source_server_ids) > 0\n+        )\n \n         if not has_tags and not has_servers:\n             return response(\n                 400,\n-                {\"error\": \"Either ServerSelectionTags or SourceServerIds is required\"},\n+                {\n+                    \"error\": \"Either ServerSelectionTags or SourceServerIds is required\"\n+                },\n             )\n \n         # Validate unique name (case-insensitive, global across all users)\n         if not validate_unique_pg_name(name):\n             return response(\n@@ -1982,11 +2089,13 @@\n                 },\n             )\n \n         # If using tags, check for tag conflicts with other PGs\n         if has_tags:\n-            tag_conflicts = check_tag_conflicts_for_create(selection_tags, region)\n+            tag_conflicts = check_tag_conflicts_for_create(\n+                selection_tags, region\n+            )\n             if tag_conflicts:\n                 return response(\n                     409,\n                     {\n                         \"error\": \"TAG_CONFLICT\",\n@@ -2001,11 +2110,13 @@\n             regional_drs = boto3.client(\"drs\", region_name=region)\n             try:\n                 drs_response = regional_drs.describe_source_servers(\n                     filters={\"sourceServerIDs\": source_server_ids}\n                 )\n-                found_ids = {s[\"sourceServerID\"] for s in drs_response.get(\"items\", [])}\n+                found_ids = {\n+                    s[\"sourceServerID\"] for s in drs_response.get(\"items\", [])\n+                }\n                 missing = set(source_server_ids) - found_ids\n                 if missing:\n                     return response(\n                         400, {\"error\": f\"Invalid server IDs: {list(missing)}\"}\n                     )\n@@ -2096,11 +2207,13 @@\n                     }\n                 resolved = query_drs_servers_by_tags(\n                     region, selection_tags, account_context\n                 )\n                 server_ids_to_apply = [\n-                    s.get(\"sourceServerID\") for s in resolved if s.get(\"sourceServerID\")\n+                    s.get(\"sourceServerID\")\n+                    for s in resolved\n+                    if s.get(\"sourceServerID\")\n                 ]\n \n             # Apply LaunchConfig to DRS/EC2 immediately\n             if server_ids_to_apply and launch_config:\n                 launch_config_apply_results = apply_launch_config_to_servers(\n@@ -2139,11 +2252,13 @@\n         # Transform to camelCase for frontend\n         response_item = transform_pg_to_camelcase(item)\n \n         # Include LaunchConfig apply results if applicable\n         if launch_config_apply_results:\n-            response_item[\"launchConfigApplyResults\"] = launch_config_apply_results\n+            response_item[\n+                \"launchConfigApplyResults\"\n+            ] = launch_config_apply_results\n \n         return response(201, response_item)\n \n     except Exception as e:\n         print(f\"Error creating Protection Group: {str(e)}\")\n@@ -2185,11 +2300,13 @@\n                     # Include groups that match account or have no account specified (legacy)\n                     filtered_groups.append(group)\n             groups = filtered_groups\n \n         # Transform to camelCase (no server enrichment - tags are resolved at execution time)\n-        camelcase_groups = [transform_pg_to_camelcase(group) for group in groups]\n+        camelcase_groups = [\n+            transform_pg_to_camelcase(group) for group in groups\n+        ]\n \n         return response(\n             200, {\"groups\": camelcase_groups, \"count\": len(camelcase_groups)}\n         )\n \n@@ -2274,11 +2391,13 @@\n                     },\n                 )\n \n         # Prevent region changes\n         if \"Region\" in body and body[\"Region\"] != existing_group.get(\"Region\"):\n-            return response(400, {\"error\": \"Cannot change region after creation\"})\n+            return response(\n+                400, {\"error\": \"Cannot change region after creation\"}\n+            )\n \n         # Validate name if provided\n         if \"GroupName\" in body:\n             name = body[\"GroupName\"]\n \n@@ -2322,11 +2441,14 @@\n                     )\n \n         # Validate tags if provided\n         if \"ServerSelectionTags\" in body:\n             selection_tags = body[\"ServerSelectionTags\"]\n-            if not isinstance(selection_tags, dict) or len(selection_tags) == 0:\n+            if (\n+                not isinstance(selection_tags, dict)\n+                or len(selection_tags) == 0\n+            ):\n                 return response(\n                     400,\n                     {\n                         \"error\": \"INVALID_TAGS\",\n                         \"message\": \"ServerSelectionTags must be a non-empty object with tag key-value pairs\",\n@@ -2349,21 +2471,26 @@\n                 )\n \n         # Validate server IDs if provided\n         if \"SourceServerIds\" in body:\n             source_server_ids = body[\"SourceServerIds\"]\n-            if not isinstance(source_server_ids, list) or len(source_server_ids) == 0:\n+            if (\n+                not isinstance(source_server_ids, list)\n+                or len(source_server_ids) == 0\n+            ):\n                 return response(\n                     400,\n                     {\n                         \"error\": \"INVALID_SERVERS\",\n                         \"message\": \"SourceServerIds must be a non-empty array\",\n                     },\n                 )\n \n             # Check for conflicts with other PGs (excluding this one)\n-            conflicts = check_server_conflicts_for_update(source_server_ids, group_id)\n+            conflicts = check_server_conflicts_for_update(\n+                source_server_ids, group_id\n+            )\n             if conflicts:\n                 return response(\n                     409,\n                     {\n                         \"error\": \"SERVER_CONFLICT\",\n@@ -2372,11 +2499,13 @@\n                     },\n                 )\n \n         # Build update expression with version increment (optimistic locking)\n         new_version = int(current_version) + 1\n-        update_expression = \"SET LastModifiedDate = :timestamp, Version = :new_version\"\n+        update_expression = (\n+            \"SET LastModifiedDate = :timestamp, Version = :new_version\"\n+        )\n         expression_values = {\n             \":timestamp\": int(time.time()),\n             \":new_version\": new_version,\n             \":current_version\": int(current_version),\n         }\n@@ -2452,17 +2581,21 @@\n                     region,\n                     existing_group[\"ServerSelectionTags\"],\n                     account_context,\n                 )\n                 server_ids = [\n-                    s.get(\"sourceServerID\") for s in resolved if s.get(\"sourceServerID\")\n+                    s.get(\"sourceServerID\")\n+                    for s in resolved\n+                    if s.get(\"sourceServerID\")\n                 ]\n \n             # Apply LaunchConfig to DRS/EC2 immediately\n             if server_ids and launch_config:\n                 # Get group name (use updated name if provided, else existing)\n-                pg_name = body.get(\"GroupName\", existing_group.get(\"GroupName\", \"\"))\n+                pg_name = body.get(\n+                    \"GroupName\", existing_group.get(\"GroupName\", \"\")\n+                )\n                 launch_config_apply_results = apply_launch_config_to_servers(\n                     server_ids,\n                     launch_config,\n                     region,\n                     protection_group_id=group_id,\n@@ -2519,11 +2652,13 @@\n         # Transform to camelCase for frontend\n         response_item = transform_pg_to_camelcase(result[\"Attributes\"])\n \n         # Include LaunchConfig apply results if applicable\n         if launch_config_apply_results:\n-            response_item[\"launchConfigApplyResults\"] = launch_config_apply_results\n+            response_item[\n+                \"launchConfigApplyResults\"\n+            ] = launch_config_apply_results\n \n         print(f\"Updated Protection Group: {group_id}\")\n         return response(200, response_item)\n \n     except Exception as e:\n@@ -2577,11 +2712,13 @@\n \n         # Delete the group\n         protection_groups_table.delete_item(Key={\"GroupId\": group_id})\n \n         print(f\"Deleted Protection Group: {group_id}\")\n-        return response(200, {\"message\": \"Protection Group deleted successfully\"})\n+        return response(\n+            200, {\"message\": \"Protection Group deleted successfully\"}\n+        )\n \n     except Exception as e:\n         print(f\"Error deleting Protection Group: {str(e)}\")\n         return response(500, {\"error\": str(e)})\n \n@@ -2791,21 +2928,25 @@\n                     Limit=1,  # Get only the latest execution\n                 )\n \n                 if execution_result.get(\"Items\"):\n                     latest_execution = execution_result[\"Items\"][0]\n-                    plan[\"LastExecutionStatus\"] = latest_execution.get(\"Status\")\n+                    plan[\"LastExecutionStatus\"] = latest_execution.get(\n+                        \"Status\"\n+                    )\n                     plan[\"LastStartTime\"] = latest_execution.get(\"StartTime\")\n                     plan[\"LastEndTime\"] = latest_execution.get(\"EndTime\")\n                 else:\n                     # No executions found for this plan\n                     plan[\"LastExecutionStatus\"] = None\n                     plan[\"LastStartTime\"] = None\n                     plan[\"LastEndTime\"] = None\n \n             except Exception as e:\n-                print(f\"Error querying execution history for plan {plan_id}: {str(e)}\")\n+                print(\n+                    f\"Error querying execution history for plan {plan_id}: {str(e)}\"\n+                )\n                 # Set null values on error\n                 plan[\"LastExecutionStatus\"] = None\n                 plan[\"LastStartTime\"] = None\n                 plan[\"LastEndTime\"] = None\n \n@@ -2856,11 +2997,14 @@\n                 has_matching_tag = False\n                 for wave in plan_waves:\n                     pg_id = wave.get(\"ProtectionGroupId\")\n                     if pg_id and pg_id in pg_tags_map:\n                         pg_tags = pg_tags_map[pg_id]\n-                        if tag_key in pg_tags and pg_tags[tag_key] == tag_value:\n+                        if (\n+                            tag_key in pg_tags\n+                            and pg_tags[tag_key] == tag_value\n+                        ):\n                             has_matching_tag = True\n                             break\n                 if not has_matching_tag:\n                     continue\n \n@@ -2869,11 +3013,13 @@\n         # Transform to camelCase for frontend\n         camelcase_plans = []\n         for plan in filtered_plans:\n             camelcase_plans.append(transform_rp_to_camelcase(plan))\n \n-        return response(200, {\"plans\": camelcase_plans, \"count\": len(camelcase_plans)})\n+        return response(\n+            200, {\"plans\": camelcase_plans, \"count\": len(camelcase_plans)}\n+        )\n \n     except Exception as e:\n         print(f\"Error listing Recovery Plans: {str(e)}\")\n         return response(500, {\"error\": str(e)})\n \n@@ -3003,19 +3149,23 @@\n                             \"error\": \"INVALID_WAVE_DATA\",\n                             \"message\": f\"Wave {idx} has invalid ServerIds format (must be array)\",\n                             \"waveIndex\": idx,\n                         },\n                     )\n-                print(f\"Wave {idx}: {wave.get('WaveName')} - {len(server_ids)} servers\")\n+                print(\n+                    f\"Wave {idx}: {wave.get('WaveName')} - {len(server_ids)} servers\"\n+                )\n \n             validation_error = validate_waves(body[\"Waves\"])\n             if validation_error:\n                 return response(400, {\"error\": validation_error})\n \n         # Build update expression with version increment (optimistic locking)\n         new_version = int(current_version) + 1\n-        update_expression = \"SET LastModifiedDate = :timestamp, Version = :new_version\"\n+        update_expression = (\n+            \"SET LastModifiedDate = :timestamp, Version = :new_version\"\n+        )\n         expression_values = {\n             \":timestamp\": int(time.time()),\n             \":new_version\": new_version,\n             \":current_version\": int(current_version),\n         }\n@@ -3147,11 +3297,13 @@\n         for wave in plan.get(\"Waves\", []):\n             pg_id = wave.get(\"ProtectionGroupId\")\n             if not pg_id:\n                 continue\n \n-            pg_result = protection_groups_table.get_item(Key={\"GroupId\": pg_id})\n+            pg_result = protection_groups_table.get_item(\n+                Key={\"GroupId\": pg_id}\n+            )\n             pg = pg_result.get(\"Item\", {})\n             if not pg:\n                 continue\n \n             # Get region from protection group\n@@ -3184,11 +3336,13 @@\n                         print(f\"Resolved {len(resolved)} servers from tags\")\n                         for server in resolved:\n                             server_id = server.get(\"sourceServerID\")\n                             if server_id:\n                                 all_server_ids.add(server_id)\n-                                print(f\"Added server {server_id} to check list\")\n+                                print(\n+                                    f\"Added server {server_id} to check list\"\n+                                )\n                     except Exception as e:\n                         print(f\"Error resolving tags for PG {pg_id}: {e}\")\n \n         print(\n             f\"Total servers to check for recovery instances: {len(all_server_ids)}: {all_server_ids}\"\n@@ -3247,32 +3401,38 @@\n                             for exec_item in exec_items:\n                                 exec_waves = exec_item.get(\"Waves\", [])\n                                 found = False\n                                 for wave in exec_waves:\n                                     # Check ServerStatuses array (correct structure)\n-                                    for server in wave.get(\"ServerStatuses\", []):\n+                                    for server in wave.get(\n+                                        \"ServerStatuses\", []\n+                                    ):\n                                         # Match by source server ID (most reliable)\n                                         if (\n                                             server.get(\"SourceServerId\")\n                                             == source_server_id\n                                         ):\n                                             source_execution = exec_item.get(\n                                                 \"ExecutionId\"\n                                             )\n                                             # Get plan name\n-                                            exec_plan_id = exec_item.get(\"PlanId\")\n+                                            exec_plan_id = exec_item.get(\n+                                                \"PlanId\"\n+                                            )\n                                             if exec_plan_id:\n-                                                plan_lookup = (\n-                                                    recovery_plans_table.get_item(\n-                                                        Key={\"PlanId\": exec_plan_id}\n+                                                plan_lookup = recovery_plans_table.get_item(\n+                                                    Key={\n+                                                        \"PlanId\": exec_plan_id\n+                                                    }\n+                                                )\n+                                                source_plan_name = (\n+                                                    plan_lookup.get(\n+                                                        \"Item\", {}\n+                                                    ).get(\n+                                                        \"PlanName\",\n+                                                        exec_plan_id,\n                                                     )\n-                                                )\n-                                                source_plan_name = plan_lookup.get(\n-                                                    \"Item\", {}\n-                                                ).get(\n-                                                    \"PlanName\",\n-                                                    exec_plan_id,\n                                                 )\n                                             found = True\n                                             break\n                                     if found:\n                                         break\n@@ -3306,11 +3466,13 @@\n                     inst[\"ec2InstanceId\"]\n                     for inst in existing_instances\n                     if inst.get(\"ec2InstanceId\")\n                 ]\n                 if ec2_ids:\n-                    ec2_response = ec2_client.describe_instances(InstanceIds=ec2_ids)\n+                    ec2_response = ec2_client.describe_instances(\n+                        InstanceIds=ec2_ids\n+                    )\n                     ec2_details = {}\n                     for reservation in ec2_response.get(\"Reservations\", []):\n                         for instance in reservation.get(\"Instances\", []):\n                             inst_id = instance.get(\"InstanceId\")\n                             name_tag = next(\n@@ -3324,11 +3486,13 @@\n                             ec2_details[inst_id] = {\n                                 \"name\": name_tag,\n                                 \"privateIp\": instance.get(\"PrivateIpAddress\"),\n                                 \"publicIp\": instance.get(\"PublicIpAddress\"),\n                                 \"instanceType\": instance.get(\"InstanceType\"),\n-                                \"launchTime\": instance.get(\"LaunchTime\").isoformat()\n+                                \"launchTime\": instance.get(\n+                                    \"LaunchTime\"\n+                                ).isoformat()\n                                 if instance.get(\"LaunchTime\")\n                                 else None,\n                             }\n                     # Merge EC2 details into existing_instances\n                     for inst in existing_instances:\n@@ -3397,11 +3561,14 @@\n         return get_termination_job_status(execution_id, job_ids, region)\n     elif execution_id and \"/job-logs\" in full_path:\n         job_id = query_params.get(\"jobId\")\n         return get_job_log_items(execution_id, job_id)\n     elif (\n-        method == \"POST\" and \"/delete\" in full_path and body and \"executionIds\" in body\n+        method == \"POST\"\n+        and \"/delete\" in full_path\n+        and body\n+        and \"executionIds\" in body\n     ):\n         # Delete specific executions by IDs (selective operation) - NEW POST route\n         execution_ids = body.get(\"executionIds\", [])\n         return delete_executions_by_ids(execution_ids)\n     elif method == \"POST\" and not execution_id:\n@@ -3409,11 +3576,16 @@\n     elif method == \"GET\" and execution_id:\n         return get_execution_details(execution_id)\n     elif method == \"GET\":\n         # List all executions with optional pagination\n         return list_executions(query_params)\n-    elif method == \"DELETE\" and not execution_id and body and \"executionIds\" in body:\n+    elif (\n+        method == \"DELETE\"\n+        and not execution_id\n+        and body\n+        and \"executionIds\" in body\n+    ):\n         # Delete specific executions by IDs (selective operation) - LEGACY DELETE route\n         execution_ids = body.get(\"executionIds\", [])\n         return delete_executions_by_ids(execution_ids)\n     elif method == \"DELETE\" and not execution_id:\n         # Delete completed executions only (bulk operation)\n@@ -3463,11 +3635,13 @@\n                     \"field\": \"InitiatedBy\",\n                 },\n             )\n \n         plan_id = body[\"PlanId\"]\n-        execution_type = body[\"ExecutionType\"].upper() if body[\"ExecutionType\"] else \"\"\n+        execution_type = (\n+            body[\"ExecutionType\"].upper() if body[\"ExecutionType\"] else \"\"\n+        )\n \n         # Validate execution type\n         if execution_type not in [\"DRILL\", \"RECOVERY\"]:\n             return response(\n                 400,\n@@ -3520,19 +3694,25 @@\n                 },\n             )\n \n         # BLOCK: Check for server conflicts with other running executions OR active DRS jobs\n         # Parse account context from request\n-        account_context = body.get(\"AccountContext\") or body.get(\"accountContext\")\n+        account_context = body.get(\"AccountContext\") or body.get(\n+            \"accountContext\"\n+        )\n         server_conflicts = check_server_conflicts(plan, account_context)\n         if server_conflicts:\n             # Separate execution conflicts from DRS job conflicts\n             execution_conflicts = [\n-                c for c in server_conflicts if c.get(\"conflictSource\") == \"execution\"\n+                c\n+                for c in server_conflicts\n+                if c.get(\"conflictSource\") == \"execution\"\n             ]\n             drs_job_conflicts = [\n-                c for c in server_conflicts if c.get(\"conflictSource\") == \"drs_job\"\n+                c\n+                for c in server_conflicts\n+                if c.get(\"conflictSource\") == \"drs_job\"\n             ]\n \n             # Group execution conflicts by execution\n             conflict_executions = {}\n             for conflict in execution_conflicts:\n@@ -3542,11 +3722,13 @@\n                         \"executionId\": exec_id,\n                         \"planId\": conflict.get(\"conflictingPlanId\"),\n                         \"servers\": [],\n                     }\n                 if exec_id:\n-                    conflict_executions[exec_id][\"servers\"].append(conflict[\"serverId\"])\n+                    conflict_executions[exec_id][\"servers\"].append(\n+                        conflict[\"serverId\"]\n+                    )\n \n             # Group DRS job conflicts by job\n             conflict_drs_jobs = {}\n             for conflict in drs_job_conflicts:\n                 job_id = conflict.get(\"conflictingJobId\")\n@@ -3555,11 +3737,13 @@\n                         \"jobId\": job_id,\n                         \"jobStatus\": conflict.get(\"conflictingJobStatus\"),\n                         \"servers\": [],\n                     }\n                 if job_id:\n-                    conflict_drs_jobs[job_id][\"servers\"].append(conflict[\"serverId\"])\n+                    conflict_drs_jobs[job_id][\"servers\"].append(\n+                        conflict[\"serverId\"]\n+                    )\n \n             # Build appropriate error message\n             if drs_job_conflicts and not execution_conflicts:\n                 message = f\"{len(drs_job_conflicts)} server(s) are being processed by active DRS jobs\"\n             elif execution_conflicts and not drs_job_conflicts:\n@@ -3571,11 +3755,13 @@\n                 409,\n                 {\n                     \"error\": \"SERVER_CONFLICT\",\n                     \"message\": message,\n                     \"conflicts\": server_conflicts,\n-                    \"conflictingExecutions\": list(conflict_executions.values()),\n+                    \"conflictingExecutions\": list(\n+                        conflict_executions.values()\n+                    ),\n                     \"conflictingDrsJobs\": list(conflict_drs_jobs.values()),\n                 },\n             )\n \n         # ============================================================\n@@ -3584,11 +3770,13 @@\n \n         # Get region from first wave's protection group\n         first_wave = plan.get(\"Waves\", [{}])[0]\n         pg_id = first_wave.get(\"ProtectionGroupId\")\n         if pg_id:\n-            pg_result = protection_groups_table.get_item(Key={\"GroupId\": pg_id})\n+            pg_result = protection_groups_table.get_item(\n+                Key={\"GroupId\": pg_id}\n+            )\n             region = pg_result.get(\"Item\", {}).get(\"Region\", \"us-east-1\")\n         else:\n             region = \"us-east-1\"  # Default fallback\n \n         # Collect all server IDs from all waves\n@@ -3635,25 +3823,33 @@\n                     \"error\": \"SERVERS_IN_JOBS_LIMIT_EXCEEDED\",\n                     \"message\": servers_in_jobs_result.get(\"message\"),\n                     \"currentServersInJobs\": servers_in_jobs_result.get(\n                         \"currentServersInJobs\"\n                     ),\n-                    \"newServerCount\": servers_in_jobs_result.get(\"newServerCount\"),\n-                    \"totalAfterNew\": servers_in_jobs_result.get(\"totalAfterNew\"),\n+                    \"newServerCount\": servers_in_jobs_result.get(\n+                        \"newServerCount\"\n+                    ),\n+                    \"totalAfterNew\": servers_in_jobs_result.get(\n+                        \"totalAfterNew\"\n+                    ),\n                     \"maxServers\": servers_in_jobs_result.get(\"maxServers\"),\n                 },\n             )\n \n         # 4. Validate server replication states\n-        replication_result = validate_server_replication_states(region, all_server_ids)\n+        replication_result = validate_server_replication_states(\n+            region, all_server_ids\n+        )\n         if not replication_result.get(\"valid\"):\n             return response(\n                 400,\n                 {\n                     \"error\": \"UNHEALTHY_SERVER_REPLICATION\",\n                     \"message\": replication_result.get(\"message\"),\n-                    \"unhealthyServers\": replication_result.get(\"unhealthyServers\"),\n+                    \"unhealthyServers\": replication_result.get(\n+                        \"unhealthyServers\"\n+                    ),\n                     \"healthyCount\": replication_result.get(\"healthyCount\"),\n                     \"unhealthyCount\": replication_result.get(\"unhealthyCount\"),\n                 },\n             )\n \n@@ -3666,11 +3862,13 @@\n         print(f\"Creating async execution {execution_id} for plan {plan_id}\")\n \n         # Create initial execution history record with PENDING status\n         # Store PlanName directly so it's preserved even if plan is later deleted\n         # Determine account context early so we can store it for resume\n-        account_context = body.get(\"AccountContext\") or body.get(\"accountContext\")\n+        account_context = body.get(\"AccountContext\") or body.get(\n+            \"accountContext\"\n+        )\n         if not account_context:\n             # Derive from plan if not provided in request\n             account_context = determine_target_account_context(plan)\n \n         history_item = {\n@@ -3818,11 +4016,13 @@\n             stateMachineArn=state_machine_arn,\n             name=sfn_name,\n             input=json.dumps(sfn_input, cls=DecimalEncoder),\n         )\n \n-        print(f\"Step Functions execution started: {sfn_response['executionArn']}\")\n+        print(\n+            f\"Step Functions execution started: {sfn_response['executionArn']}\"\n+        )\n \n         # Update DynamoDB with Step Functions execution ARN\n         execution_history_table.update_item(\n             Key={\"ExecutionId\": execution_id, \"PlanId\": plan_id},\n             UpdateExpression=\"SET StateMachineArn = :arn, #status = :status\",\n@@ -3977,11 +4177,13 @@\n                 \"Status\": \"INITIATED\",\n                 \"Servers\": [],\n                 \"StartTime\": int(time.time()),\n             }\n \n-        print(f\"Initiating recovery for {len(server_ids)} servers in region {region}\")\n+        print(\n+            f\"Initiating recovery for {len(server_ids)} servers in region {region}\"\n+        )\n \n         # CRITICAL FIX: Launch ALL servers in wave with ONE DRS API call\n         # This gives us ONE job ID per wave (which poller expects)\n         # STEP 6: Pass metadata to start_drs_recovery_for_wave\n         wave_job_result = start_drs_recovery_for_wave(\n@@ -4062,19 +4264,23 @@\n     drs_client = boto3.client(\"drs\", region_name=region)\n     configs = {}\n \n     for server_id in server_ids:\n         try:\n-            response = drs_client.get_launch_configuration(sourceServerID=server_id)\n+            response = drs_client.get_launch_configuration(\n+                sourceServerID=server_id\n+            )\n \n             configs[server_id] = {\n                 \"targetInstanceTypeRightSizingMethod\": response.get(\n                     \"targetInstanceTypeRightSizingMethod\", \"BASIC\"\n                 ),\n                 \"copyPrivateIp\": response.get(\"copyPrivateIp\", False),\n                 \"copyTags\": response.get(\"copyTags\", True),\n-                \"launchDisposition\": response.get(\"launchDisposition\", \"STARTED\"),\n+                \"launchDisposition\": response.get(\n+                    \"launchDisposition\", \"STARTED\"\n+                ),\n                 \"bootMode\": response.get(\"bootMode\", \"USE_DEFAULT\"),\n             }\n \n             print(\n                 f\"[Launch Config] {server_id}: rightSizing={configs[server_id]['targetInstanceTypeRightSizingMethod']}\"\n@@ -4158,11 +4364,13 @@\n         )\n         launch_configs = get_server_launch_configurations(region, server_ids)\n \n         # STEP 3: Build sourceServers array (simplified - DRS uses latest snapshot automatically)\n         source_servers = [{\"sourceServerID\": sid} for sid in server_ids]\n-        print(f\"[DRS API] Built sourceServers array for {len(server_ids)} servers\")\n+        print(\n+            f\"[DRS API] Built sourceServers array for {len(server_ids)} servers\"\n+        )\n \n         # CRITICAL FIX: Do NOT pass tags to start_recovery()\n         # The reference implementation (drs-plan-automation) does NOT use tags\n         # CLI without tags works, code with tags fails (conversion skipped)\n         # Tags were causing DRS to skip the CONVERSION phase entirely\n@@ -4198,11 +4406,13 @@\n \n         print(f\"[DRS API] \u2705 Job created successfully\")\n         print(f\"[DRS API]   Job ID: {job_id}\")\n         print(f\"[DRS API]   Status: {job_status}\")\n         print(f\"[DRS API]   Type: {job_type}\")\n-        print(f\"[DRS API]   Servers: {len(server_ids)} (all share this job ID)\")\n+        print(\n+            f\"[DRS API]   Servers: {len(server_ids)} (all share this job ID)\"\n+        )\n \n         # Build server results array (all servers share same job ID)\n         server_results = []\n         for server_id in server_ids:\n             server_results.append(\n@@ -4310,11 +4520,13 @@\n     max_retries = 3\n     base_delay = 30  # Base delay in seconds\n \n     for attempt in range(max_retries):\n         try:\n-            return start_drs_recovery(server_id, region, is_drill, execution_id)\n+            return start_drs_recovery(\n+                server_id, region, is_drill, execution_id\n+            )\n         except ClientError as e:\n             error_code = e.response[\"Error\"][\"Code\"]\n \n             # Only retry on ConflictException\n             if error_code == \"ConflictException\" and attempt < max_retries - 1:\n@@ -4386,11 +4598,13 @@\n             ScanIndexForward=False,  # Sort by StartTime descending\n         )\n \n         executions = result.get(\"Items\", [])\n \n-        return response(200, {\"executions\": executions, \"count\": len(executions)})\n+        return response(\n+            200, {\"executions\": executions, \"count\": len(executions)}\n+        )\n \n     except Exception as e:\n         print(f\"Error getting execution history: {str(e)}\")\n         return response(500, {\"error\": str(e)})\n \n@@ -4439,23 +4653,25 @@\n                     if plan_id:\n                         plan_result = recovery_plans_table.get_item(\n                             Key={\"PlanId\": plan_id}\n                         )\n                         if \"Item\" in plan_result:\n-                            execution[\"RecoveryPlanName\"] = plan_result[\"Item\"].get(\n-                                \"PlanName\", \"Unknown\"\n-                            )\n+                            execution[\"RecoveryPlanName\"] = plan_result[\n+                                \"Item\"\n+                            ].get(\"PlanName\", \"Unknown\")\n                         else:\n                             execution[\"RecoveryPlanName\"] = \"Deleted Plan\"\n                     else:\n                         execution[\"RecoveryPlanName\"] = \"Unknown\"\n \n                 # Determine selection mode from protection groups\n                 plan_id = execution.get(\"PlanId\")\n                 selection_mode = \"PLAN\"  # Default to plan-based\n                 if plan_id:\n-                    plan_result = recovery_plans_table.get_item(Key={\"PlanId\": plan_id})\n+                    plan_result = recovery_plans_table.get_item(\n+                        Key={\"PlanId\": plan_id}\n+                    )\n                     if \"Item\" in plan_result:\n                         plan = plan_result[\"Item\"]\n                         waves = plan.get(\"Waves\", [])\n                         pg_ids = set()\n                         for wave in waves:\n@@ -4474,11 +4690,13 @@\n                                     tags = pg.get(\"ServerSelectionTags\", {})\n                                     if tags and len(tags) > 0:\n                                         selection_mode = \"TAGS\"\n                                         break  # Found tag-based, no need to check more\n                             except Exception as pg_err:\n-                                print(f\"Error checking PG {pg_id}: {str(pg_err)}\")\n+                                print(\n+                                    f\"Error checking PG {pg_id}: {str(pg_err)}\"\n+                                )\n \n                 execution[\"SelectionMode\"] = selection_mode\n             except Exception as e:\n                 print(\n                     f\"Error enriching execution {execution.get('ExecutionId')}: {str(e)}\"\n@@ -4497,20 +4715,24 @@\n                 has_active_jobs = False\n                 waves = execution.get(\"Waves\", [])\n                 print(f\"  Found {len(waves)} waves to check for active jobs\")\n                 for wave in waves:\n                     job_id = wave.get(\"JobId\")\n-                    print(f\"  Wave JobId: {job_id}, Region: {wave.get('Region')}\")\n+                    print(\n+                        f\"  Wave JobId: {job_id}, Region: {wave.get('Region')}\"\n+                    )\n                     if job_id:\n                         region = wave.get(\"Region\", \"us-east-1\")\n                         try:\n                             drs_client = create_drs_client(region)\n                             job_response = drs_client.describe_jobs(\n                                 filters={\"jobIDs\": [job_id]}\n                             )\n                             jobs = job_response.get(\"items\", [])\n-                            job_status = jobs[0].get(\"status\") if jobs else \"NOT_FOUND\"\n+                            job_status = (\n+                                jobs[0].get(\"status\") if jobs else \"NOT_FOUND\"\n+                            )\n                             print(f\"  DRS job {job_id} status: {job_status}\")\n                             if jobs and jobs[0].get(\"status\") in [\n                                 \"PENDING\",\n                                 \"STARTED\",\n                             ]:\n@@ -4523,11 +4745,13 @@\n                 print(f\"  HasActiveDrsJobs set to: {has_active_jobs}\")\n             else:\n                 execution[\"HasActiveDrsJobs\"] = False\n \n             # Transform to camelCase for frontend\n-            transformed_executions.append(transform_execution_to_camelcase(execution))\n+            transformed_executions.append(\n+                transform_execution_to_camelcase(execution)\n+            )\n \n         # Build response with pagination\n         response_data = {\n             \"items\": transformed_executions,\n             \"count\": len(transformed_executions),\n@@ -4587,22 +4811,26 @@\n \n                     # Get source account ID from staging area info or ARN\n                     source_account_id = \"\"\n                     staging_area = server.get(\"stagingArea\", {})\n                     if staging_area:\n-                        source_account_id = staging_area.get(\"stagingAccountID\", \"\")\n+                        source_account_id = staging_area.get(\n+                            \"stagingAccountID\", \"\"\n+                        )\n                     # Fallback: extract from ARN if available\n                     if not source_account_id:\n                         arn = server.get(\"arn\", \"\")\n                         if arn:\n                             # ARN format: arn:aws:drs:region:account:source-server/id\n                             arn_parts = arn.split(\":\")\n                             if len(arn_parts) >= 5:\n                                 source_account_id = arn_parts[4]\n \n                     # Extract source IP from network interfaces\n-                    network_interfaces = source_props.get(\"networkInterfaces\", [])\n+                    network_interfaces = source_props.get(\n+                        \"networkInterfaces\", []\n+                    )\n                     source_ip = \"\"\n                     if network_interfaces:\n                         # Get first private IP from first interface\n                         first_iface = network_interfaces[0]\n                         ips = first_iface.get(\"ips\", [])\n@@ -4615,11 +4843,13 @@\n                     replicated_disks = data_rep_info.get(\"replicatedDisks\", [])\n                     if replicated_disks:\n                         # Extract region from device name or use staging area region\n                         staging_area = server.get(\"stagingArea\", {})\n                         source_region = (\n-                            staging_area.get(\"stagingSourceServerArn\", \"\").split(\":\")[3]\n+                            staging_area.get(\n+                                \"stagingSourceServerArn\", \"\"\n+                            ).split(\":\")[3]\n                             if staging_area.get(\"stagingSourceServerArn\")\n                             else \"\"\n                         )\n \n                     # Fallback: get source region from sourceProperties\n@@ -4642,13 +4872,13 @@\n                         \"sourceInstanceId\": source_instance_id,\n                         \"sourceAccountId\": source_account_id,\n                         \"sourceIp\": source_ip,\n                         \"sourceRegion\": source_region\n                         or region,  # Fallback to target region if not found\n-                        \"replicationState\": server.get(\"dataReplicationInfo\", {}).get(\n-                            \"dataReplicationState\", \"UNKNOWN\"\n-                        ),\n+                        \"replicationState\": server.get(\n+                            \"dataReplicationInfo\", {}\n+                        ).get(\"dataReplicationState\", \"UNKNOWN\"),\n                         \"lastLaunchResult\": server.get(\n                             \"lastLaunchResult\", \"NOT_STARTED\"\n                         ),\n                     }\n \n@@ -4668,11 +4898,13 @@\n                 for reservation in ec2_response.get(\"Reservations\", []):\n                     for instance in reservation.get(\"Instances\", []):\n                         instance_id = instance.get(\"InstanceId\", \"\")\n                         for tag in instance.get(\"Tags\", []):\n                             if tag.get(\"Key\") == \"Name\":\n-                                ec2_name_tags[instance_id] = tag.get(\"Value\", \"\")\n+                                ec2_name_tags[instance_id] = tag.get(\n+                                    \"Value\", \"\"\n+                                )\n                                 break\n \n                 # Update server_map with EC2 Name tags\n                 for source_id, details in server_map.items():\n                     instance_id = details.get(\"sourceInstanceId\")\n@@ -4730,11 +4962,13 @@\n                     \"Region\": region,\n                     \"SourceInstanceId\": details.get(\"sourceInstanceId\", \"\"),\n                     \"SourceAccountId\": details.get(\"sourceAccountId\", \"\"),\n                     \"SourceIp\": details.get(\"sourceIp\", \"\"),\n                     \"SourceRegion\": details.get(\"sourceRegion\", \"\"),\n-                    \"ReplicationState\": details.get(\"replicationState\", \"UNKNOWN\"),\n+                    \"ReplicationState\": details.get(\n+                        \"replicationState\", \"UNKNOWN\"\n+                    ),\n                 }\n             )\n \n         # Add enriched servers to wave\n         wave[\"EnrichedServers\"] = enriched_servers\n@@ -4853,17 +5087,23 @@\n             if execution.get(\"PlanName\"):\n                 execution[\"RecoveryPlanName\"] = execution[\"PlanName\"]\n \n             plan_id = execution.get(\"PlanId\")\n             if plan_id:\n-                plan_result = recovery_plans_table.get_item(Key={\"PlanId\": plan_id})\n+                plan_result = recovery_plans_table.get_item(\n+                    Key={\"PlanId\": plan_id}\n+                )\n                 if \"Item\" in plan_result:\n                     plan = plan_result[\"Item\"]\n                     # Only set from lookup if not already stored\n                     if not execution.get(\"RecoveryPlanName\"):\n-                        execution[\"RecoveryPlanName\"] = plan.get(\"PlanName\", \"Unknown\")\n-                    execution[\"RecoveryPlanDescription\"] = plan.get(\"Description\", \"\")\n+                        execution[\"RecoveryPlanName\"] = plan.get(\n+                            \"PlanName\", \"Unknown\"\n+                        )\n+                    execution[\"RecoveryPlanDescription\"] = plan.get(\n+                        \"Description\", \"\"\n+                    )\n                     execution[\"TotalWaves\"] = len(plan.get(\"Waves\", []))\n                 elif not execution.get(\"RecoveryPlanName\"):\n                     execution[\"RecoveryPlanName\"] = \"Deleted Plan\"\n         except Exception as e:\n             print(f\"Error enriching execution with plan details: {str(e)}\")\n@@ -4873,11 +5113,13 @@\n             execution = enrich_execution_with_server_details(execution)\n         except Exception as e:\n             print(f\"Error enriching execution with server details: {str(e)}\")\n \n         # Get current status from Step Functions if still running and has StateMachineArn\n-        if execution.get(\"Status\") == \"RUNNING\" and execution.get(\"StateMachineArn\"):\n+        if execution.get(\"Status\") == \"RUNNING\" and execution.get(\n+            \"StateMachineArn\"\n+        ):\n             try:\n                 sf_response = stepfunctions.describe_execution(\n                     executionArn=execution.get(\"StateMachineArn\")\n                 )\n                 execution[\"StepFunctionsStatus\"] = sf_response[\"status\"]\n@@ -4983,11 +5225,13 @@\n         timestamp = int(time.time())\n \n         # Get recovery plan to find waves that haven't started yet\n         plan_waves = []\n         try:\n-            plan_result = recovery_plans_table.get_item(Key={\"PlanId\": plan_id})\n+            plan_result = recovery_plans_table.get_item(\n+                Key={\"PlanId\": plan_id}\n+            )\n             if \"Item\" in plan_result:\n                 plan_waves = plan_result[\"Item\"].get(\"Waves\", [])\n         except Exception as e:\n             print(f\"Error getting recovery plan: {e}\")\n \n@@ -5032,11 +5276,13 @@\n             wave_number = plan_wave.get(\"WaveNumber\", i)\n             if wave_number not in existing_wave_numbers:\n                 # This wave hasn't started - add it as CANCELLED\n                 cancelled_wave = {\n                     \"WaveNumber\": wave_number,\n-                    \"WaveName\": plan_wave.get(\"WaveName\", f\"Wave {wave_number + 1}\"),\n+                    \"WaveName\": plan_wave.get(\n+                        \"WaveName\", f\"Wave {wave_number + 1}\"\n+                    ),\n                     \"Status\": \"CANCELLED\",\n                     \"EndTime\": timestamp,\n                     \"ProtectionGroupId\": plan_wave.get(\"ProtectionGroupId\"),\n                     \"ServerIds\": plan_wave.get(\"ServerIds\", []),\n                 }\n@@ -5320,11 +5566,13 @@\n \n         # Build the full application state that ResumeWavePlan expects\n         # This must match the state structure from orchestration_stepfunctions.py\n         # Get the original plan waves (execution history has different structure)\n         try:\n-            plan_response = recovery_plans_table.get_item(Key={\"PlanId\": plan_id})\n+            plan_response = recovery_plans_table.get_item(\n+                Key={\"PlanId\": plan_id}\n+            )\n             plan_waves = plan_response.get(\"Item\", {}).get(\"Waves\", [])\n             # Convert DynamoDB format to plain dicts (handles Decimal types)\n             waves_data = json.loads(json.dumps(plan_waves, cls=DecimalEncoder))\n             print(f\"Loaded {len(waves_data)} waves from recovery plan\")\n         except Exception as plan_error:\n@@ -5388,18 +5636,24 @@\n             )\n         except Exception as sfn_error:\n             print(f\"ERROR calling SendTaskSuccess: {str(sfn_error)}\")\n             return response(\n                 500,\n-                {\"error\": f\"Failed to resume Step Functions: {str(sfn_error)}\"},\n+                {\n+                    \"error\": f\"Failed to resume Step Functions: {str(sfn_error)}\"\n+                },\n             )\n \n         # Note: The orchestration Lambda (resume_wave action) will update the status to RUNNING\n         # and clear the TaskToken when it processes the resume\n \n-        wave_display = paused_before_wave + 1  # 0-indexed to 1-indexed for display\n-        print(f\"Resumed execution {execution_id}, wave {wave_display} will start\")\n+        wave_display = (\n+            paused_before_wave + 1\n+        )  # 0-indexed to 1-indexed for display\n+        print(\n+            f\"Resumed execution {execution_id}, wave {wave_display} will start\"\n+        )\n         return response(\n             200,\n             {\n                 \"executionId\": execution_id,\n                 \"status\": \"RESUMING\",\n@@ -5472,11 +5726,13 @@\n                 # Create regional DRS client with cross-account support\n                 # Use wave-specific region since DRS jobs are regional\n                 drs_client = create_drs_client(wave_region, account_context)\n \n                 # Get job log items from DRS\n-                log_response = drs_client.describe_job_log_items(jobID=wave_job_id)\n+                log_response = drs_client.describe_job_log_items(\n+                    jobID=wave_job_id\n+                )\n                 log_items = log_response.get(\"items\", [])\n \n                 # Transform log items for frontend\n                 wave_logs = {\n                     \"waveNumber\": wave_number,\n@@ -5496,11 +5752,13 @@\n                     if \"sourceServerID\" in event_data:\n                         event[\"sourceServerId\"] = event_data[\"sourceServerID\"]\n                     if \"rawError\" in event_data:\n                         event[\"error\"] = event_data[\"rawError\"]\n                     if \"conversionServerID\" in event_data:\n-                        event[\"conversionServerId\"] = event_data[\"conversionServerID\"]\n+                        event[\"conversionServerId\"] = event_data[\n+                            \"conversionServerID\"\n+                        ]\n \n                     wave_logs[\"events\"].append(event)\n \n                 # Sort events by timestamp (newest first for display)\n                 wave_logs[\"events\"].sort(\n@@ -5508,21 +5766,25 @@\n                 )\n \n                 all_job_logs.append(wave_logs)\n \n             except Exception as e:\n-                print(f\"Error getting job log items for job {wave_job_id}: {e}\")\n+                print(\n+                    f\"Error getting job log items for job {wave_job_id}: {e}\"\n+                )\n                 all_job_logs.append(\n                     {\n                         \"waveNumber\": wave_number,\n                         \"jobId\": wave_job_id,\n                         \"events\": [],\n                         \"error\": str(e),\n                     }\n                 )\n \n-        return response(200, {\"executionId\": execution_id, \"jobLogs\": all_job_logs})\n+        return response(\n+            200, {\"executionId\": execution_id, \"jobLogs\": all_job_logs}\n+        )\n \n     except Exception as e:\n         print(f\"Error getting job log items: {str(e)}\")\n         return response(500, {\"error\": str(e)})\n \n@@ -5560,24 +5822,30 @@\n \n         # Get the Recovery Plan to determine account context (for cross-account support)\n         account_context = None\n         if plan_id:\n             try:\n-                plan_result = recovery_plans_table.get_item(Key={\"PlanId\": plan_id})\n+                plan_result = recovery_plans_table.get_item(\n+                    Key={\"PlanId\": plan_id}\n+                )\n                 if \"Item\" in plan_result:\n                     plan = plan_result[\"Item\"]\n                     account_context = determine_target_account_context(plan)\n-                    print(f\"Using account context for terminate: {account_context}\")\n+                    print(\n+                        f\"Using account context for terminate: {account_context}\"\n+                    )\n                 else:\n                     print(\n                         f\"WARNING: Recovery Plan {plan_id} not found, using current account\"\n                     )\n             except Exception as e:\n                 print(\n                     f\"ERROR: Could not get Recovery Plan {plan_id} for account context: {e}\"\n                 )\n-                print(\"Falling back to current account for terminate operation\")\n+                print(\n+                    \"Falling back to current account for terminate operation\"\n+                )\n \n         if not waves:\n             return response(\n                 400,\n                 {\n@@ -5638,26 +5906,35 @@\n                         f\"DRS describe_jobs response for {job_id}: {len(job_response.get('items', []))} items\"\n                     )\n \n                     if job_response.get(\"items\"):\n                         job = job_response[\"items\"][0]\n-                        participating_servers = job.get(\"participatingServers\", [])\n+                        participating_servers = job.get(\n+                            \"participatingServers\", []\n+                        )\n \n                         print(\n                             f\"Job {job_id} has {len(participating_servers)} participating servers\"\n                         )\n \n                         for server in participating_servers:\n-                            recovery_instance_id = server.get(\"recoveryInstanceID\")\n-                            source_server_id = server.get(\"sourceServerID\", \"unknown\")\n+                            recovery_instance_id = server.get(\n+                                \"recoveryInstanceID\"\n+                            )\n+                            source_server_id = server.get(\n+                                \"sourceServerID\", \"unknown\"\n+                            )\n \n                             print(\n                                 f\"Server {source_server_id}: recoveryInstanceID={recovery_instance_id}\"\n                             )\n \n                             # Collect source server ID for alternative lookup\n-                            if source_server_id and source_server_id != \"unknown\":\n+                            if (\n+                                source_server_id\n+                                and source_server_id != \"unknown\"\n+                            ):\n                                 if region not in source_server_ids_by_region:\n                                     source_server_ids_by_region[region] = []\n                                 if (\n                                     source_server_id\n                                     not in source_server_ids_by_region[region]\n@@ -5677,16 +5954,18 @@\n                                                 ]\n                                             }\n                                         )\n                                     )\n                                     if ri_response.get(\"items\"):\n-                                        ec2_instance_id = ri_response[\"items\"][0].get(\n-                                            \"ec2InstanceID\"\n-                                        )\n+                                        ec2_instance_id = ri_response[\"items\"][\n+                                            0\n+                                        ].get(\"ec2InstanceID\")\n                                         if (\n                                             ec2_instance_id\n-                                            and ec2_instance_id.startswith(\"i-\")\n+                                            and ec2_instance_id.startswith(\n+                                                \"i-\"\n+                                            )\n                                         ):\n                                             instances_to_terminate.append(\n                                                 {\n                                                     \"instanceId\": ec2_instance_id,\n                                                     \"recoveryInstanceId\": recovery_instance_id,\n@@ -5695,22 +5974,29 @@\n                                                     \"serverId\": source_server_id,\n                                                     \"jobId\": job_id,\n                                                 }\n                                             )\n \n-                                            if region not in instances_by_region:\n-                                                instances_by_region[region] = []\n+                                            if (\n+                                                region\n+                                                not in instances_by_region\n+                                            ):\n+                                                instances_by_region[\n+                                                    region\n+                                                ] = []\n                                             instances_by_region[region].append(\n                                                 ec2_instance_id\n                                             )\n                                 except Exception as ri_err:\n                                     print(\n                                         f\"Could not get EC2 instance for recovery instance {recovery_instance_id}: {ri_err}\"\n                                     )\n \n                 except Exception as drs_err:\n-                    print(f\"Could not query DRS job {job_id} in {region}: {drs_err}\")\n+                    print(\n+                        f\"Could not query DRS job {job_id} in {region}: {drs_err}\"\n+                    )\n \n         # Alternative approach: Query describe_recovery_instances by source server IDs\n         # This works even when job's participatingServers doesn't have recoveryInstanceID\n         if not instances_to_terminate and source_server_ids_by_region:\n             print(\n@@ -5742,11 +6028,13 @@\n \n                         print(\n                             f\"Recovery instance: ec2={ec2_instance_id}, ri={recovery_instance_id}, source={source_server_id}\"\n                         )\n \n-                        if ec2_instance_id and ec2_instance_id.startswith(\"i-\"):\n+                        if ec2_instance_id and ec2_instance_id.startswith(\n+                            \"i-\"\n+                        ):\n                             instances_to_terminate.append(\n                                 {\n                                     \"instanceId\": ec2_instance_id,\n                                     \"recoveryInstanceId\": recovery_instance_id,\n                                     \"region\": region,\n@@ -5755,12 +6043,17 @@\n                                 }\n                             )\n \n                             if region not in instances_by_region:\n                                 instances_by_region[region] = []\n-                            if ec2_instance_id not in instances_by_region[region]:\n-                                instances_by_region[region].append(ec2_instance_id)\n+                            if (\n+                                ec2_instance_id\n+                                not in instances_by_region[region]\n+                            ):\n+                                instances_by_region[region].append(\n+                                    ec2_instance_id\n+                                )\n \n                 except Exception as e:\n                     print(\n                         f\"Error querying recovery instances by source server IDs in {region}: {e}\"\n                     )\n@@ -5788,11 +6081,13 @@\n                         instances_to_terminate.append(\n                             {\n                                 \"instanceId\": instance_id,\n                                 \"region\": region,\n                                 \"waveNumber\": wave_number,\n-                                \"serverId\": server.get(\"SourceServerId\", \"unknown\"),\n+                                \"serverId\": server.get(\n+                                    \"SourceServerId\", \"unknown\"\n+                                ),\n                             }\n                         )\n                         if region not in instances_by_region:\n                             instances_by_region[region] = []\n                         instances_by_region[region].append(instance_id)\n@@ -5817,11 +6112,13 @@\n                         instances_to_terminate.append(\n                             {\n                                 \"instanceId\": instance_id,\n                                 \"region\": server_region,\n                                 \"waveNumber\": wave_number,\n-                                \"serverId\": server.get(\"SourceServerId\", \"unknown\"),\n+                                \"serverId\": server.get(\n+                                    \"SourceServerId\", \"unknown\"\n+                                ),\n                             }\n                         )\n                         if server_region not in instances_by_region:\n                             instances_by_region[server_region] = []\n                         instances_by_region[server_region].append(instance_id)\n@@ -5841,11 +6138,13 @@\n                     \"totalFailed\": 0,\n                     \"noInstancesFound\": True,\n                 },\n             )\n \n-        print(f\"Found {len(instances_to_terminate)} recovery instances to terminate\")\n+        print(\n+            f\"Found {len(instances_to_terminate)} recovery instances to terminate\"\n+        )\n \n         # Group recovery instance IDs by region for DRS API call\n         recovery_instances_by_region = {}\n         for instance_info in instances_to_terminate:\n             region = instance_info.get(\"region\", \"us-east-1\")\n@@ -5854,12 +6153,17 @@\n                 \"recoveryInstanceId\"\n             ) or instance_info.get(\"instanceId\")\n             if recovery_instance_id:\n                 if region not in recovery_instances_by_region:\n                     recovery_instances_by_region[region] = []\n-                if recovery_instance_id not in recovery_instances_by_region[region]:\n-                    recovery_instances_by_region[region].append(recovery_instance_id)\n+                if (\n+                    recovery_instance_id\n+                    not in recovery_instances_by_region[region]\n+                ):\n+                    recovery_instances_by_region[region].append(\n+                        recovery_instance_id\n+                    )\n \n         # Use DRS TerminateRecoveryInstances API - this properly terminates via DRS\n         # and creates a TERMINATE job in DRS console\n         terminated = []\n         failed = []\n@@ -5922,11 +6226,13 @@\n                             \"errorType\": \"CONFLICT\",\n                         }\n                     )\n             except Exception as e:\n                 error_msg = str(e)\n-                print(f\"Error terminating recovery instances in {region}: {error_msg}\")\n+                print(\n+                    f\"Error terminating recovery instances in {region}: {error_msg}\"\n+                )\n                 for ri_id in recovery_instance_ids:\n                     failed.append(\n                         {\n                             \"recoveryInstanceId\": ri_id,\n                             \"region\": region,\n@@ -6274,12 +6580,16 @@\n                 )\n                 deleted_count += 1\n                 print(f\"Deleted execution: {execution_id}\")\n             except Exception as delete_error:\n                 error_msg = str(delete_error)\n-                print(f\"Failed to delete execution {execution_id}: {error_msg}\")\n-                failed_deletes.append({\"executionId\": execution_id, \"error\": error_msg})\n+                print(\n+                    f\"Failed to delete execution {execution_id}: {error_msg}\"\n+                )\n+                failed_deletes.append(\n+                    {\"executionId\": execution_id, \"error\": error_msg}\n+                )\n \n         # Build response\n         result = {\n             \"message\": \"Completed executions cleared successfully\",\n             \"deletedCount\": deleted_count,\n@@ -6465,12 +6775,16 @@\n                 deleted_count += 1\n                 print(f\"Deleted execution: {execution_id}\")\n \n             except Exception as delete_error:\n                 error_msg = str(delete_error)\n-                print(f\"Failed to delete execution {execution_id}: {error_msg}\")\n-                failed_deletes.append({\"executionId\": execution_id, \"error\": error_msg})\n+                print(\n+                    f\"Failed to delete execution {execution_id}: {error_msg}\"\n+                )\n+                failed_deletes.append(\n+                    {\"executionId\": execution_id, \"error\": error_msg}\n+                )\n \n         # Build response\n         result = {\n             \"message\": f\"Processed {len(execution_ids)} execution deletion requests\",\n             \"deletedCount\": deleted_count,\n@@ -6530,11 +6844,13 @@\n     if target_accounts_table:\n         ensure_default_account()\n \n     region = query_params.get(\"region\")\n     current_pg_id = query_params.get(\"currentProtectionGroupId\")\n-    filter_by_pg = query_params.get(\"filterByProtectionGroup\")  # NEW: Filter mode\n+    filter_by_pg = query_params.get(\n+        \"filterByProtectionGroup\"\n+    )  # NEW: Filter mode\n \n     if not region:\n         return response(400, {\"error\": \"region parameter is required\"})\n \n     # NEW: If filtering by PG, return only that PG's servers\n@@ -6542,11 +6858,13 @@\n         return get_protection_group_servers(filter_by_pg, region)\n \n     return list_source_servers(region, current_pg_id)\n \n \n-def list_source_servers(region: str, current_pg_id: Optional[str] = None) -> Dict:\n+def list_source_servers(\n+    region: str, current_pg_id: Optional[str] = None\n+) -> Dict:\n     \"\"\"\n     Discover DRS source servers in a region and track assignments\n \n     Returns:\n     - All DRS source servers in region\n@@ -6558,11 +6876,13 @@\n     try:\n         # 1. Query DRS for source servers\n         drs_client = boto3.client(\"drs\", region_name=region)\n \n         try:\n-            servers_response = drs_client.describe_source_servers(maxResults=200)\n+            servers_response = drs_client.describe_source_servers(\n+                maxResults=200\n+            )\n             drs_initialized = True\n         except drs_client.exceptions.UninitializedAccountException:\n             print(f\"DRS not initialized in {region}\")\n             return response(\n                 400,\n@@ -6664,11 +6984,13 @@\n                         if disk_bytes\n                         else 0,\n                     }\n                 )\n             total_disk_gib = (\n-                round(total_disk_bytes / (1024**3), 1) if total_disk_bytes else 0\n+                round(total_disk_bytes / (1024**3), 1)\n+                if total_disk_bytes\n+                else 0\n             )\n \n             # Extract OS info\n             os_info = source_props.get(\"os\", {})\n             os_string = os_info.get(\"fullString\", \"\")\n@@ -6741,17 +7063,20 @@\n             try:\n                 ec2_client = boto3.client(\"ec2\", region_name=region)\n                 # Batch in groups of 200\n                 for i in range(0, len(source_instance_ids), 200):\n                     batch = source_instance_ids[i : i + 200]\n-                    ec2_response = ec2_client.describe_instances(InstanceIds=batch)\n+                    ec2_response = ec2_client.describe_instances(\n+                        InstanceIds=batch\n+                    )\n                     for reservation in ec2_response.get(\"Reservations\", []):\n                         for instance in reservation.get(\"Instances\", []):\n                             instance_id = instance.get(\"InstanceId\", \"\")\n                             # Convert tag list to dict\n                             tags_dict = {\n-                                t[\"Key\"]: t[\"Value\"] for t in instance.get(\"Tags\", [])\n+                                t[\"Key\"]: t[\"Value\"]\n+                                for t in instance.get(\"Tags\", [])\n                             }\n                             ec2_tags_map[instance_id] = tags_dict\n             except Exception as e:\n                 print(f\"Warning: Could not fetch EC2 tags: {str(e)}\")\n \n@@ -6777,11 +7102,13 @@\n             # Skip current PG when editing - allows deselection\n             if current_pg_id and pg_id == current_pg_id:\n                 continue\n \n             pg_name = pg.get(\"GroupName\") or pg.get(\"name\")\n-            pg_servers = pg.get(\"SourceServerIds\") or pg.get(\"sourceServerIds\", [])\n+            pg_servers = pg.get(\"SourceServerIds\") or pg.get(\n+                \"sourceServerIds\", []\n+            )\n             pg_tags = pg.get(\"ServerSelectionTags\", {})\n \n             # Track explicit server assignments\n             for server_id in pg_servers:\n                 assignment_map[server_id] = {\n@@ -6790,11 +7117,13 @@\n                     \"assignmentType\": \"explicit\",\n                 }\n \n             # Track tag-based PGs for matching\n             if pg_tags:\n-                tag_based_pgs.append({\"id\": pg_id, \"name\": pg_name, \"tags\": pg_tags})\n+                tag_based_pgs.append(\n+                    {\"id\": pg_id, \"name\": pg_name, \"tags\": pg_tags}\n+                )\n \n         # 4. Update servers with assignment info (explicit + tag-based)\n         for server in servers:\n             server_id = server[\"sourceServerID\"]\n \n@@ -6833,13 +7162,17 @@\n                 \"region\": region,\n                 \"initialized\": True,\n                 \"servers\": servers,\n                 \"totalCount\": len(servers),\n                 \"availableCount\": sum(1 for s in servers if s[\"selectable\"]),\n-                \"assignedCount\": sum(1 for s in servers if not s[\"selectable\"]),\n+                \"assignedCount\": sum(\n+                    1 for s in servers if not s[\"selectable\"]\n+                ),\n                 \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n-                \"hardwareDataIncluded\": len([s for s in servers if s.get(\"hardware\")])\n+                \"hardwareDataIncluded\": len(\n+                    [s for s in servers if s.get(\"hardware\")]\n+                )\n                 > 0,\n             },\n         )\n \n     except Exception as e:\n@@ -6978,11 +7311,13 @@\n \n         # Skip current PG when editing\n         if current_pg_id and pg_id == current_pg_id:\n             continue\n \n-        assigned_servers = pg.get(\"SourceServerIds\") or pg.get(\"sourceServerIds\", [])\n+        assigned_servers = pg.get(\"SourceServerIds\") or pg.get(\n+            \"sourceServerIds\", []\n+        )\n         for server_id in server_ids:\n             if server_id in assigned_servers:\n                 pg_name = pg.get(\"GroupName\") or pg.get(\"name\")\n                 conflicts.append(\n                     {\n@@ -6993,11 +7328,13 @@\n                 )\n \n     return conflicts\n \n \n-def validate_servers_exist_in_drs(region: str, server_ids: List[str]) -> List[str]:\n+def validate_servers_exist_in_drs(\n+    region: str, server_ids: List[str]\n+) -> List[str]:\n     \"\"\"\n     Validate that server IDs actually exist in DRS\n \n     Args:\n     - region: AWS region to check\n@@ -7009,14 +7346,18 @@\n     try:\n         drs_client = boto3.client(\"drs\", region_name=region)\n \n         # Get all source servers in the region\n         response = drs_client.describe_source_servers(maxResults=200)\n-        valid_server_ids = {s[\"sourceServerID\"] for s in response.get(\"items\", [])}\n+        valid_server_ids = {\n+            s[\"sourceServerID\"] for s in response.get(\"items\", [])\n+        }\n \n         # Find invalid servers\n-        invalid_servers = [sid for sid in server_ids if sid not in valid_server_ids]\n+        invalid_servers = [\n+            sid for sid in server_ids if sid not in valid_server_ids\n+        ]\n \n         if invalid_servers:\n             print(f\"Invalid server IDs detected: {invalid_servers}\")\n \n         return invalid_servers\n@@ -7026,11 +7367,13 @@\n         # On error, assume servers might be valid (fail open for now)\n         # In production, might want to fail closed\n         return []\n \n \n-def validate_unique_pg_name(name: str, current_pg_id: Optional[str] = None) -> bool:\n+def validate_unique_pg_name(\n+    name: str, current_pg_id: Optional[str] = None\n+) -> bool:\n     \"\"\"\n     Validate that Protection Group name is unique (case-insensitive)\n \n     Args:\n     - name: Protection Group name to validate\n@@ -7054,11 +7397,13 @@\n             return False\n \n     return True\n \n \n-def validate_unique_rp_name(name: str, current_rp_id: Optional[str] = None) -> bool:\n+def validate_unique_rp_name(\n+    name: str, current_rp_id: Optional[str] = None\n+) -> bool:\n     \"\"\"\n     Validate that Recovery Plan name is unique (case-insensitive)\n \n     Args:\n     - name: Recovery Plan name to validate\n@@ -7157,11 +7502,13 @@\n                 )\n \n     return conflicts\n \n \n-def check_tag_conflicts_for_create(tags: Dict[str, str], region: str) -> List[Dict]:\n+def check_tag_conflicts_for_create(\n+    tags: Dict[str, str], region: str\n+) -> List[Dict]:\n     \"\"\"\n     Check if the specified tags would conflict with existing tag-based Protection Groups.\n     A conflict occurs if another PG has the EXACT SAME tags (all keys and values match).\n     \"\"\"\n     if not tags:\n@@ -7350,11 +7697,13 @@\n     transformed_conflict = None\n     if conflict_info:\n         transformed_conflict = {\n             \"hasConflict\": conflict_info.get(\"hasConflict\", False),\n             \"conflictingServers\": conflict_info.get(\"conflictingServers\", []),\n-            \"conflictingExecutionId\": conflict_info.get(\"conflictingExecutionId\"),\n+            \"conflictingExecutionId\": conflict_info.get(\n+                \"conflictingExecutionId\"\n+            ),\n             \"conflictingPlanId\": conflict_info.get(\"conflictingPlanId\"),\n             \"conflictingStatus\": conflict_info.get(\"conflictingStatus\"),\n             \"reason\": conflict_info.get(\"reason\"),\n         }\n \n@@ -7369,12 +7718,16 @@\n         \"rpo\": rp.get(\"RPO\"),\n         \"rto\": rp.get(\"RTO\"),\n         \"waves\": waves,  # Now properly transformed\n         \"createdAt\": int(created_at * 1000) if created_at else None,\n         \"updatedAt\": int(updated_at * 1000) if updated_at else None,\n-        \"lastExecutedAt\": int(last_executed_at * 1000) if last_executed_at else None,\n-        \"lastExecutionStatus\": rp.get(\"LastExecutionStatus\"),  # NEW: Execution status\n+        \"lastExecutedAt\": int(last_executed_at * 1000)\n+        if last_executed_at\n+        else None,\n+        \"lastExecutionStatus\": rp.get(\n+            \"LastExecutionStatus\"\n+        ),  # NEW: Execution status\n         \"lastStartTime\": last_start_time,  # NEW: Unix timestamp (seconds) - no conversion needed\n         \"lastEndTime\": last_end_time,  # NEW: Unix timestamp (seconds) - no conversion needed\n         \"waveCount\": len(waves),\n         \"hasServerConflict\": rp.get(\n             \"HasServerConflict\", False\n@@ -7446,24 +7799,34 @@\n                 server_id = server.get(\"SourceServerId\")\n                 enriched = enriched_map.get(server_id, {})\n                 servers.append(\n                     {\n                         \"sourceServerId\": server_id,\n-                        \"recoveryJobId\": wave.get(\"JobId\"),  # JobId is at wave level\n+                        \"recoveryJobId\": wave.get(\n+                            \"JobId\"\n+                        ),  # JobId is at wave level\n                         \"instanceId\": server.get(\"RecoveryInstanceID\"),\n                         \"status\": server.get(\"LaunchStatus\", \"UNKNOWN\"),\n-                        \"launchTime\": safe_timestamp_to_int(wave.get(\"StartTime\")),\n+                        \"launchTime\": safe_timestamp_to_int(\n+                            wave.get(\"StartTime\")\n+                        ),\n                         \"error\": server.get(\"Error\"),\n                         # Enriched fields from DRS source server\n                         \"hostname\": enriched.get(\"Hostname\", \"\"),\n                         \"serverName\": enriched.get(\"NameTag\", \"\"),\n-                        \"region\": enriched.get(\"Region\", wave.get(\"Region\", \"\")),\n-                        \"sourceInstanceId\": enriched.get(\"SourceInstanceId\", \"\"),\n+                        \"region\": enriched.get(\n+                            \"Region\", wave.get(\"Region\", \"\")\n+                        ),\n+                        \"sourceInstanceId\": enriched.get(\n+                            \"SourceInstanceId\", \"\"\n+                        ),\n                         \"sourceAccountId\": enriched.get(\"SourceAccountId\", \"\"),\n                         \"sourceIp\": enriched.get(\"SourceIp\", \"\"),\n                         \"sourceRegion\": enriched.get(\"SourceRegion\", \"\"),\n-                        \"replicationState\": enriched.get(\"ReplicationState\", \"\"),\n+                        \"replicationState\": enriched.get(\n+                            \"ReplicationState\", \"\"\n+                        ),\n                     }\n                 )\n         else:\n             # Fallback to ServerIds (legacy format) or Servers array\n             server_ids = wave.get(\"ServerIds\", [])\n@@ -7485,16 +7848,24 @@\n                             ),\n                             \"error\": server.get(\"Error\"),\n                             # Enriched fields from DRS source server\n                             \"hostname\": enriched.get(\"Hostname\", \"\"),\n                             \"serverName\": enriched.get(\"NameTag\", \"\"),\n-                            \"region\": enriched.get(\"Region\", wave.get(\"Region\", \"\")),\n-                            \"sourceInstanceId\": enriched.get(\"SourceInstanceId\", \"\"),\n-                            \"sourceAccountId\": enriched.get(\"SourceAccountId\", \"\"),\n+                            \"region\": enriched.get(\n+                                \"Region\", wave.get(\"Region\", \"\")\n+                            ),\n+                            \"sourceInstanceId\": enriched.get(\n+                                \"SourceInstanceId\", \"\"\n+                            ),\n+                            \"sourceAccountId\": enriched.get(\n+                                \"SourceAccountId\", \"\"\n+                            ),\n                             \"sourceIp\": enriched.get(\"SourceIp\", \"\"),\n                             \"sourceRegion\": enriched.get(\"SourceRegion\", \"\"),\n-                            \"replicationState\": enriched.get(\"ReplicationState\", \"\"),\n+                            \"replicationState\": enriched.get(\n+                                \"ReplicationState\", \"\"\n+                            ),\n                         }\n                     )\n             elif server_ids:\n                 # Build servers from ServerIds list (minimal info)\n                 for server_id in server_ids:\n@@ -7502,22 +7873,34 @@\n                     servers.append(\n                         {\n                             \"sourceServerId\": server_id,\n                             \"recoveryJobId\": wave.get(\"JobId\"),\n                             \"instanceId\": None,\n-                            \"status\": wave.get(\"Status\", \"UNKNOWN\"),  # Use wave status\n-                            \"launchTime\": safe_timestamp_to_int(wave.get(\"StartTime\")),\n+                            \"status\": wave.get(\n+                                \"Status\", \"UNKNOWN\"\n+                            ),  # Use wave status\n+                            \"launchTime\": safe_timestamp_to_int(\n+                                wave.get(\"StartTime\")\n+                            ),\n                             \"error\": None,\n                             # Enriched fields from DRS source server\n                             \"hostname\": enriched.get(\"Hostname\", \"\"),\n                             \"serverName\": enriched.get(\"NameTag\", \"\"),\n-                            \"region\": enriched.get(\"Region\", wave.get(\"Region\", \"\")),\n-                            \"sourceInstanceId\": enriched.get(\"SourceInstanceId\", \"\"),\n-                            \"sourceAccountId\": enriched.get(\"SourceAccountId\", \"\"),\n+                            \"region\": enriched.get(\n+                                \"Region\", wave.get(\"Region\", \"\")\n+                            ),\n+                            \"sourceInstanceId\": enriched.get(\n+                                \"SourceInstanceId\", \"\"\n+                            ),\n+                            \"sourceAccountId\": enriched.get(\n+                                \"SourceAccountId\", \"\"\n+                            ),\n                             \"sourceIp\": enriched.get(\"SourceIp\", \"\"),\n                             \"sourceRegion\": enriched.get(\"SourceRegion\", \"\"),\n-                            \"replicationState\": enriched.get(\"ReplicationState\", \"\"),\n+                            \"replicationState\": enriched.get(\n+                                \"ReplicationState\", \"\"\n+                            ),\n                         }\n                     )\n \n         waves.append(\n             {\n@@ -7642,13 +8025,13 @@\n                     {\n                         \"SourceServerId\": server[\"sourceServerID\"],\n                         \"Hostname\": server.get(\"sourceProperties\", {})\n                         .get(\"identificationHints\", {})\n                         .get(\"hostname\", \"Unknown\"),\n-                        \"ReplicationStatus\": server.get(\"dataReplicationInfo\", {}).get(\n-                            \"dataReplicationState\", \"Unknown\"\n-                        ),\n+                        \"ReplicationStatus\": server.get(\n+                            \"dataReplicationInfo\", {}\n+                        ).get(\"dataReplicationState\", \"Unknown\"),\n                         \"LastSeenTime\": server.get(\"sourceProperties\", {}).get(\n                             \"lastUpdatedDateTime\", \"\"\n                         ),\n                         \"LifeCycleState\": server.get(\"lifeCycle\", {}).get(\n                             \"state\", \"Unknown\"\n@@ -7687,11 +8070,12 @@\n         dependency_graph = {}\n         for wave in waves:\n             wave_id = wave.get(\"WaveId\")\n             if wave_id:\n                 dependencies = [\n-                    d.get(\"DependsOnWaveId\") for d in wave.get(\"Dependencies\", [])\n+                    d.get(\"DependsOnWaveId\")\n+                    for d in wave.get(\"Dependencies\", [])\n                 ]\n                 dependency_graph[wave_id] = dependencies\n \n         if dependency_graph and has_circular_dependencies(dependency_graph):\n             return \"Circular dependency detected in wave configuration\"\n@@ -7705,18 +8089,24 @@\n \n             if \"WaveName\" not in wave and \"name\" not in wave:\n                 return \"Wave missing required field: WaveName or name\"\n \n             # NEW: Accept either protectionGroupId (single) OR protectionGroupIds (multi)\n-            has_single_pg = \"ProtectionGroupId\" in wave or \"protectionGroupId\" in wave\n-            has_multi_pg = \"ProtectionGroupIds\" in wave or \"protectionGroupIds\" in wave\n+            has_single_pg = (\n+                \"ProtectionGroupId\" in wave or \"protectionGroupId\" in wave\n+            )\n+            has_multi_pg = (\n+                \"ProtectionGroupIds\" in wave or \"protectionGroupIds\" in wave\n+            )\n \n             if not has_single_pg and not has_multi_pg:\n                 return \"Wave missing Protection Group assignment (protectionGroupId or protectionGroupIds required)\"\n \n             # Validate protectionGroupIds is an array if present\n-            pg_ids = wave.get(\"protectionGroupIds\") or wave.get(\"ProtectionGroupIds\")\n+            pg_ids = wave.get(\"protectionGroupIds\") or wave.get(\n+                \"ProtectionGroupIds\"\n+            )\n             if pg_ids is not None:\n                 if not isinstance(pg_ids, list):\n                     return f\"protectionGroupIds must be an array, got {type(pg_ids)}\"\n                 if len(pg_ids) == 0:\n                     return \"protectionGroupIds array cannot be empty\"\n@@ -7782,11 +8172,13 @@\n                     \"message\": f\"Account {account_id} not accessible. Only current account {current_account_id} is supported.\",\n                 },\n             )\n \n         # Use provided region for regional operations (jobs, etc.)\n-        region = query_params.get(\"region\", os.environ.get(\"AWS_REGION\", \"us-east-1\"))\n+        region = query_params.get(\n+            \"region\", os.environ.get(\"AWS_REGION\", \"us-east-1\")\n+        )\n \n         # Get account capacity for the specified region\n         capacity = get_drs_account_capacity(region)\n \n         # Get concurrent jobs info\n@@ -7826,11 +8218,13 @@\n         return response(500, {\"error\": str(e)})\n \n \n def handle_drs_accounts(query_params: Dict) -> Dict:\n     \"\"\"Get available DRS accounts\"\"\"\n-    print(f\"DEBUG: handle_drs_accounts called with query_params: {query_params}\")\n+    print(\n+        f\"DEBUG: handle_drs_accounts called with query_params: {query_params}\"\n+    )\n     try:\n         # For now, only return current account\n         # In future, this will query cross-account roles\n         current_account_id = get_current_account_id()\n         print(f\"DEBUG: Current account ID: {current_account_id}\")\n@@ -7897,22 +8291,24 @@\n     \"\"\"Transform Target Account from DynamoDB PascalCase to frontend camelCase\"\"\"\n     current_account_id = get_current_account_id()\n \n     return {\n         \"accountId\": account.get(\"AccountId\") or account.get(\"accountId\"),\n-        \"accountName\": account.get(\"AccountName\") or account.get(\"accountName\"),\n+        \"accountName\": account.get(\"AccountName\")\n+        or account.get(\"accountName\"),\n         \"isCurrentAccount\": account.get(\"AccountId\", account.get(\"accountId\"))\n         == current_account_id,\n         \"status\": account.get(\"Status\") or account.get(\"status\", \"active\"),\n         \"stagingAccountId\": account.get(\"StagingAccountId\")\n         or account.get(\"stagingAccountId\"),\n         \"stagingAccountName\": account.get(\"StagingAccountName\")\n         or account.get(\"stagingAccountName\"),\n         \"crossAccountRoleArn\": account.get(\"CrossAccountRoleArn\")\n         or account.get(\"crossAccountRoleArn\"),\n         \"createdAt\": account.get(\"CreatedAt\") or account.get(\"createdAt\"),\n-        \"lastValidated\": account.get(\"LastValidated\") or account.get(\"lastValidated\"),\n+        \"lastValidated\": account.get(\"LastValidated\")\n+        or account.get(\"lastValidated\"),\n     }\n \n \n def transform_target_account_from_camelcase(camel_account: Dict) -> Dict:\n     \"\"\"Transform Target Account from frontend camelCase to DynamoDB PascalCase\"\"\"\n@@ -7925,13 +8321,17 @@\n     if camel_account.get(\"status\"):\n         pascal_account[\"Status\"] = camel_account[\"status\"]\n     if camel_account.get(\"stagingAccountId\"):\n         pascal_account[\"StagingAccountId\"] = camel_account[\"stagingAccountId\"]\n     if camel_account.get(\"stagingAccountName\"):\n-        pascal_account[\"StagingAccountName\"] = camel_account[\"stagingAccountName\"]\n+        pascal_account[\"StagingAccountName\"] = camel_account[\n+            \"stagingAccountName\"\n+        ]\n     if camel_account.get(\"crossAccountRoleArn\"):\n-        pascal_account[\"CrossAccountRoleArn\"] = camel_account[\"crossAccountRoleArn\"]\n+        pascal_account[\"CrossAccountRoleArn\"] = camel_account[\n+            \"crossAccountRoleArn\"\n+        ]\n     if camel_account.get(\"createdAt\"):\n         pascal_account[\"CreatedAt\"] = camel_account[\"createdAt\"]\n     if camel_account.get(\"lastValidated\"):\n         pascal_account[\"LastValidated\"] = camel_account[\"lastValidated\"]\n \n@@ -7992,11 +8392,12 @@\n \n         return response(\n             200,\n             {\n                 \"accountId\": current_account_id,\n-                \"accountName\": current_account_name or f\"Account {current_account_id}\",\n+                \"accountName\": current_account_name\n+                or f\"Account {current_account_id}\",\n                 \"isCurrentAccount\": True,\n             },\n         )\n \n     except Exception as e:\n@@ -8013,11 +8414,12 @@\n \n         if total_accounts == 0:\n             # No accounts exist - auto-add current account as default\n             current_account_id = get_current_account_id()\n             current_account_name = (\n-                get_account_name(current_account_id) or f\"Account {current_account_id}\"\n+                get_account_name(current_account_id)\n+                or f\"Account {current_account_id}\"\n             )\n \n             now = datetime.utcnow().isoformat() + \"Z\"\n \n             # Create default account entry (using PascalCase for DynamoDB)\n@@ -8111,11 +8513,13 @@\n                 },\n             )\n \n         # Check if account already exists (use PascalCase for DynamoDB key)\n         try:\n-            existing = target_accounts_table.get_item(Key={\"AccountId\": account_id})\n+            existing = target_accounts_table.get_item(\n+                Key={\"AccountId\": account_id}\n+            )\n             if \"Item\" in existing:\n                 return response(\n                     400,\n                     {\n                         \"error\": \"ACCOUNT_EXISTS\",\n@@ -8168,11 +8572,13 @@\n                 )\n \n         # If no name provided, try to get account name\n         if not account_name:\n             if is_current_account:\n-                account_name = get_account_name(account_id) or f\"Account {account_id}\"\n+                account_name = (\n+                    get_account_name(account_id) or f\"Account {account_id}\"\n+                )\n             else:\n                 account_name = f\"Account {account_id}\"\n \n         # Validate staging account ID if provided\n         if body.get(\"stagingAccountId\"):\n@@ -8204,11 +8610,13 @@\n             \"status\": \"active\",\n             \"isCurrentAccount\": is_current_account,\n             \"isFirstAccount\": is_first_account,  # Flag for frontend to know this should be default\n         }\n \n-        account_item = transform_target_account_from_camelcase(body_with_timestamps)\n+        account_item = transform_target_account_from_camelcase(\n+            body_with_timestamps\n+        )\n \n         # Store in DynamoDB\n         target_accounts_table.put_item(Item=account_item)\n \n         # Transform back to camelCase for response\n@@ -8276,11 +8684,13 @@\n                 )\n \n         # Build update expression (using PascalCase for DynamoDB)\n         set_clauses = [\"LastValidated = :lastValidated\"]\n         remove_clauses = []\n-        expression_values = {\":lastValidated\": datetime.utcnow().isoformat() + \"Z\"}\n+        expression_values = {\n+            \":lastValidated\": datetime.utcnow().isoformat() + \"Z\"\n+        }\n         expression_names = {}\n \n         # Update account name if provided (including empty string to clear)\n         if \"accountName\" in body:\n             account_name = body[\"accountName\"]\n@@ -8317,11 +8727,13 @@\n \n         # Update cross-account role ARN if provided\n         if \"crossAccountRoleArn\" in body:\n             cross_account_role = body[\"crossAccountRoleArn\"]\n             if cross_account_role:\n-                set_clauses.append(\"CrossAccountRoleArn = :crossAccountRoleArn\")\n+                set_clauses.append(\n+                    \"CrossAccountRoleArn = :crossAccountRoleArn\"\n+                )\n                 expression_values[\":crossAccountRoleArn\"] = cross_account_role\n             else:\n                 # Remove cross-account role\n                 remove_clauses.append(\"CrossAccountRoleArn\")\n \n@@ -8330,11 +8742,13 @@\n         if remove_clauses:\n             update_expression += \" REMOVE \" + \", \".join(remove_clauses)\n \n         # Perform update\n         update_args = {\n-            \"Key\": {\"AccountId\": account_id},  # Use PascalCase for DynamoDB key\n+            \"Key\": {\n+                \"AccountId\": account_id\n+            },  # Use PascalCase for DynamoDB key\n             \"UpdateExpression\": update_expression,\n             \"ExpressionAttributeValues\": expression_values,\n             \"ReturnValues\": \"ALL_NEW\",\n         }\n \n@@ -8482,11 +8896,13 @@\n                 ExpressionAttributeValues={\n                     \":lastValidated\": validation_results[\"lastValidated\"]\n                 },\n             )\n         except Exception as update_error:\n-            print(f\"Warning: Could not update lastValidated timestamp: {update_error}\")\n+            print(\n+                f\"Warning: Could not update lastValidated timestamp: {update_error}\"\n+            )\n \n         return response(200, validation_results)\n \n     except Exception as e:\n         print(f\"Error validating target account: {e}\")\n@@ -8619,11 +9035,13 @@\n                 )\n             ec2_client = ec2_clients[source_region]\n \n             # Get EC2 instance tags\n             try:\n-                ec2_response = ec2_client.describe_instances(InstanceIds=[instance_id])\n+                ec2_response = ec2_client.describe_instances(\n+                    InstanceIds=[instance_id]\n+                )\n                 if not ec2_response[\"Reservations\"]:\n                     continue\n                 instance = ec2_response[\"Reservations\"][0][\"Instances\"][0]\n                 ec2_tags = {\n                     tag[\"Key\"]: tag[\"Value\"]\n@@ -8714,11 +9132,13 @@\n     results = {\"applied\": 0, \"skipped\": 0, \"failed\": 0, \"details\": []}\n \n     for server_id in server_ids:\n         try:\n             # Get DRS launch configuration to find template ID\n-            drs_config = regional_drs.get_launch_configuration(sourceServerID=server_id)\n+            drs_config = regional_drs.get_launch_configuration(\n+                sourceServerID=server_id\n+            )\n             template_id = drs_config.get(\"ec2LaunchTemplateID\")\n \n             if not template_id:\n                 results[\"skipped\"] += 1\n                 results[\"details\"].append(\n@@ -8735,16 +9155,20 @@\n \n             if launch_config.get(\"InstanceType\"):\n                 template_data[\"InstanceType\"] = launch_config[\"InstanceType\"]\n \n             # Network interface settings (subnet and security groups)\n-            if launch_config.get(\"SubnetId\") or launch_config.get(\"SecurityGroupIds\"):\n+            if launch_config.get(\"SubnetId\") or launch_config.get(\n+                \"SecurityGroupIds\"\n+            ):\n                 network_interface = {\"DeviceIndex\": 0}\n                 if launch_config.get(\"SubnetId\"):\n                     network_interface[\"SubnetId\"] = launch_config[\"SubnetId\"]\n                 if launch_config.get(\"SecurityGroupIds\"):\n-                    network_interface[\"Groups\"] = launch_config[\"SecurityGroupIds\"]\n+                    network_interface[\"Groups\"] = launch_config[\n+                        \"SecurityGroupIds\"\n+                    ]\n                 template_data[\"NetworkInterfaces\"] = [network_interface]\n \n             if launch_config.get(\"InstanceProfileName\"):\n                 template_data[\"IamInstanceProfile\"] = {\n                     \"Name\": launch_config[\"InstanceProfileName\"]\n@@ -8759,15 +9183,17 @@\n             if \"CopyTags\" in launch_config:\n                 drs_update[\"copyTags\"] = launch_config[\"CopyTags\"]\n             if \"Licensing\" in launch_config:\n                 drs_update[\"licensing\"] = launch_config[\"Licensing\"]\n             if \"TargetInstanceTypeRightSizingMethod\" in launch_config:\n-                drs_update[\"targetInstanceTypeRightSizingMethod\"] = launch_config[\n-                    \"TargetInstanceTypeRightSizingMethod\"\n+                drs_update[\n+                    \"targetInstanceTypeRightSizingMethod\"\n+                ] = launch_config[\"TargetInstanceTypeRightSizingMethod\"]\n+            if \"LaunchDisposition\" in launch_config:\n+                drs_update[\"launchDisposition\"] = launch_config[\n+                    \"LaunchDisposition\"\n                 ]\n-            if \"LaunchDisposition\" in launch_config:\n-                drs_update[\"launchDisposition\"] = launch_config[\"LaunchDisposition\"]\n \n             if len(drs_update) > 1:  # More than just sourceServerID\n                 regional_drs.update_launch_configuration(**drs_update)\n \n             # THEN update EC2 launch template (after DRS, so our changes stick)\n@@ -8782,13 +9208,17 @@\n                 if protection_group_id:\n                     desc_parts.append(f\"ID: {protection_group_id[:8]}\")\n                 # Add config details\n                 config_details = []\n                 if launch_config.get(\"InstanceType\"):\n-                    config_details.append(f\"Type:{launch_config['InstanceType']}\")\n+                    config_details.append(\n+                        f\"Type:{launch_config['InstanceType']}\"\n+                    )\n                 if launch_config.get(\"SubnetId\"):\n-                    config_details.append(f\"Subnet:{launch_config['SubnetId'][-8:]}\")\n+                    config_details.append(\n+                        f\"Subnet:{launch_config['SubnetId'][-8:]}\"\n+                    )\n                 if launch_config.get(\"SecurityGroupIds\"):\n                     sg_count = len(launch_config[\"SecurityGroupIds\"])\n                     config_details.append(f\"SGs:{sg_count}\")\n                 if launch_config.get(\"InstanceProfileName\"):\n                     profile = launch_config[\"InstanceProfileName\"]\n@@ -8863,20 +9293,26 @@\n def get_ec2_subnets(query_params: Dict) -> Dict:\n     \"\"\"Get VPC subnets for dropdown selection.\"\"\"\n     region = query_params.get(\"region\")\n \n     if not region:\n-        return response(400, {\"error\": \"region is required\", \"code\": \"MISSING_REGION\"})\n+        return response(\n+            400, {\"error\": \"region is required\", \"code\": \"MISSING_REGION\"}\n+        )\n \n     try:\n         ec2 = boto3.client(\"ec2\", region_name=region)\n         result = ec2.describe_subnets()\n \n         subnets = []\n         for subnet in result[\"Subnets\"]:\n             name = next(\n-                (t[\"Value\"] for t in subnet.get(\"Tags\", []) if t[\"Key\"] == \"Name\"),\n+                (\n+                    t[\"Value\"]\n+                    for t in subnet.get(\"Tags\", [])\n+                    if t[\"Key\"] == \"Name\"\n+                ),\n                 None,\n             )\n             label = f\"{subnet['SubnetId']}\"\n             if name:\n                 label = f\"{name} ({subnet['SubnetId']})\"\n@@ -8903,11 +9339,13 @@\n     \"\"\"Get security groups for dropdown selection.\"\"\"\n     region = query_params.get(\"region\")\n     vpc_id = query_params.get(\"vpcId\")  # Optional filter\n \n     if not region:\n-        return response(400, {\"error\": \"region is required\", \"code\": \"MISSING_REGION\"})\n+        return response(\n+            400, {\"error\": \"region is required\", \"code\": \"MISSING_REGION\"}\n+        )\n \n     try:\n         ec2 = boto3.client(\"ec2\", region_name=region)\n         filters = [{\"Name\": \"vpc-id\", \"Values\": [vpc_id]}] if vpc_id else []\n         result = (\n@@ -8939,11 +9377,13 @@\n def get_ec2_instance_profiles(query_params: Dict) -> Dict:\n     \"\"\"Get IAM instance profiles for dropdown selection.\"\"\"\n     region = query_params.get(\"region\")\n \n     if not region:\n-        return response(400, {\"error\": \"region is required\", \"code\": \"MISSING_REGION\"})\n+        return response(\n+            400, {\"error\": \"region is required\", \"code\": \"MISSING_REGION\"}\n+        )\n \n     try:\n         # IAM is global but we accept region for consistency\n         iam = boto3.client(\"iam\")\n         profiles = []\n@@ -8969,11 +9409,13 @@\n def get_ec2_instance_types(query_params: Dict) -> Dict:\n     \"\"\"Get ALL EC2 instance types available in the specified region for DRS launch settings.\"\"\"\n     region = query_params.get(\"region\")\n \n     if not region:\n-        return response(400, {\"error\": \"region is required\", \"code\": \"MISSING_REGION\"})\n+        return response(\n+            400, {\"error\": \"region is required\", \"code\": \"MISSING_REGION\"}\n+        )\n \n     try:\n         ec2 = boto3.client(\"ec2\", region_name=region)\n \n         types = []\n@@ -9032,11 +9474,13 @@\n         # Get user roles and permissions\n         user_roles = get_user_roles(user)\n         user_permissions = get_user_permissions(user)\n \n         print(f\"\ud83d\udd10 User roles: {[role.value for role in user_roles]}\")\n-        print(f\"\ud83d\udd11 User permissions: {[perm.value for perm in user_permissions]}\")\n+        print(\n+            f\"\ud83d\udd11 User permissions: {[perm.value for perm in user_permissions]}\"\n+        )\n \n         return response(\n             200,\n             {\n                 \"user\": {\n@@ -9062,11 +9506,13 @@\n                 \"message\": f\"Failed to get user permissions: {str(e)}\",\n             },\n         )\n \n \n-def handle_config(method: str, path: str, body: Dict, query_params: Dict) -> Dict:\n+def handle_config(\n+    method: str, path: str, body: Dict, query_params: Dict\n+) -> Dict:\n     \"\"\"Route configuration export/import requests and tag sync settings\"\"\"\n     if path == \"/config/export\" and method == \"GET\":\n         return export_configuration(query_params)\n     elif path == \"/config/import\" and method == \"POST\":\n         return import_configuration(body)\n@@ -9108,11 +9554,12 @@\n             )\n             recovery_plans.extend(rp_result.get(\"Items\", []))\n \n         # Build PG ID -> Name mapping for wave export\n         pg_id_to_name = {\n-            pg.get(\"GroupId\", \"\"): pg.get(\"GroupName\", \"\") for pg in protection_groups\n+            pg.get(\"GroupId\", \"\"): pg.get(\"GroupName\", \"\")\n+            for pg in protection_groups\n         }\n \n         # Transform Protection Groups for export (exclude internal fields)\n         exported_pgs = []\n         for pg in protection_groups:\n@@ -9142,11 +9589,13 @@\n             for wave in rp.get(\"Waves\", []):\n                 exported_wave = dict(wave)\n                 pg_id = wave.get(\"ProtectionGroupId\", \"\")\n                 if pg_id:\n                     if pg_id in pg_id_to_name:\n-                        exported_wave[\"ProtectionGroupName\"] = pg_id_to_name[pg_id]\n+                        exported_wave[\"ProtectionGroupName\"] = pg_id_to_name[\n+                            pg_id\n+                        ]\n                         # Remove ID - use name only for portability\n                         exported_wave.pop(\"ProtectionGroupId\", None)\n                     else:\n                         # Keep ID if name can't be resolved (orphaned reference)\n                         orphaned_pg_ids.append(pg_id)\n@@ -9161,11 +9610,13 @@\n                 \"Waves\": exported_waves,\n             }\n             exported_rps.append(exported_rp)\n \n         if orphaned_pg_ids:\n-            print(f\"Export contains {len(orphaned_pg_ids)} orphaned PG references\")\n+            print(\n+                f\"Export contains {len(orphaned_pg_ids)} orphaned PG references\"\n+            )\n \n         # Build export payload\n         export_data = {\n             \"metadata\": {\n                 \"schemaVersion\": SCHEMA_VERSION,\n@@ -9206,11 +9657,13 @@\n         correlation_id = str(uuid.uuid4())\n         print(f\"[{correlation_id}] Starting configuration import\")\n \n         # Extract parameters\n         dry_run = body.get(\"dryRun\", False)\n-        config = body.get(\"config\", body)  # Support both wrapped and direct format\n+        config = body.get(\n+            \"config\", body\n+        )  # Support both wrapped and direct format\n \n         # Validate schema version\n         metadata = config.get(\"metadata\", {})\n         schema_version = metadata.get(\"schemaVersion\", \"\")\n \n@@ -9259,11 +9712,12 @@\n         available_pg_names = set(existing_pgs.keys())\n         failed_pg_names = set()\n \n         # Build name->ID mapping from existing PGs (case-insensitive keys)\n         pg_name_to_id = {\n-            name.lower(): pg.get(\"GroupId\", \"\") for name, pg in existing_pgs.items()\n+            name.lower(): pg.get(\"GroupId\", \"\")\n+            for name, pg in existing_pgs.items()\n         }\n \n         # Process Protection Groups first (RPs depend on them)\n         for pg in import_pgs:\n             pg_result = _process_protection_group_import(\n@@ -9343,11 +9797,13 @@\n def _get_existing_recovery_plans() -> Dict[str, Dict]:\n     \"\"\"Get all existing Recovery Plans indexed by name (case-insensitive)\"\"\"\n     result = recovery_plans_table.scan()\n     rps = result.get(\"Items\", [])\n     while \"LastEvaluatedKey\" in result:\n-        result = recovery_plans_table.scan(ExclusiveStartKey=result[\"LastEvaluatedKey\"])\n+        result = recovery_plans_table.scan(\n+            ExclusiveStartKey=result[\"LastEvaluatedKey\"]\n+        )\n         rps.extend(result.get(\"Items\", []))\n     return {rp.get(\"PlanName\", \"\").lower(): rp for rp in rps}\n \n \n def _get_active_execution_servers() -> Dict[str, Dict]:\n@@ -9386,11 +9842,13 @@\n                                 \"executionId\": exec_id,\n                                 \"planName\": plan_name,\n                                 \"status\": exec_status,\n                             }\n         except Exception as e:\n-            print(f\"Warning: Could not query executions for status {status}: {e}\")\n+            print(\n+                f\"Warning: Could not query executions for status {status}: {e}\"\n+            )\n \n     return servers\n \n \n def _get_all_assigned_servers() -> Dict[str, str]:\n@@ -9450,11 +9908,13 @@\n         try:\n             regional_drs = boto3.client(\"drs\", region_name=region)\n             drs_response = regional_drs.describe_source_servers(\n                 filters={\"sourceServerIDs\": source_server_ids}\n             )\n-            found_ids = {s[\"sourceServerID\"] for s in drs_response.get(\"items\", [])}\n+            found_ids = {\n+                s[\"sourceServerID\"] for s in drs_response.get(\"items\", [])\n+            }\n             missing = set(source_server_ids) - found_ids\n \n             if missing:\n                 result[\"reason\"] = \"SERVER_NOT_FOUND\"\n                 result[\"details\"] = {\n@@ -9543,11 +10003,13 @@\n             result[\"details\"] = {\n                 \"tags\": server_selection_tags,\n                 \"region\": region,\n                 \"error\": str(e),\n             }\n-            print(f\"[{correlation_id}] Failed PG '{pg_name}': tag resolution error {e}\")\n+            print(\n+                f\"[{correlation_id}] Failed PG '{pg_name}': tag resolution error {e}\"\n+            )\n             return result\n     else:\n         result[\"reason\"] = \"NO_SELECTION_METHOD\"\n         result[\"details\"] = {\n             \"message\": \"Either SourceServerIds or ServerSelectionTags required\"\n@@ -9583,11 +10045,13 @@\n             if launch_config:\n                 item[\"LaunchConfig\"] = launch_config\n \n             protection_groups_table.put_item(Item=item)\n             result[\"details\"] = {\"groupId\": group_id}\n-            print(f\"[{correlation_id}] Created PG '{pg_name}' with ID {group_id}\")\n+            print(\n+                f\"[{correlation_id}] Created PG '{pg_name}' with ID {group_id}\"\n+            )\n \n             # Apply LaunchConfig to DRS servers (same as create/update)\n             if launch_config:\n                 server_ids_to_apply = []\n                 if source_server_ids:\n@@ -9616,16 +10080,16 @@\n                             launch_config,\n                             region,\n                             protection_group_id=group_id,\n                             protection_group_name=pg_name,\n                         )\n-                        result[\"details\"][\"launchConfigApplied\"] = apply_results.get(\n-                            \"applied\", 0\n-                        )\n-                        result[\"details\"][\"launchConfigFailed\"] = apply_results.get(\n-                            \"failed\", 0\n-                        )\n+                        result[\"details\"][\n+                            \"launchConfigApplied\"\n+                        ] = apply_results.get(\"applied\", 0)\n+                        result[\"details\"][\n+                            \"launchConfigFailed\"\n+                        ] = apply_results.get(\"failed\", 0)\n                         print(\n                             f\"[{correlation_id}] Applied LaunchConfig to {apply_results.get('applied', 0)} servers\"\n                         )\n                     except Exception as lc_err:\n                         print(\n@@ -9778,11 +10242,13 @@\n         result[\"reason\"] = \"MISSING_PROTECTION_GROUP\"\n         result[\"details\"] = {\n             \"missingProtectionGroups\": list(set(missing_pgs)),\n             \"message\": \"Referenced Protection Groups do not exist\",\n         }\n-        print(f\"[{correlation_id}] Failed RP '{plan_name}': missing PGs {missing_pgs}\")\n+        print(\n+            f\"[{correlation_id}] Failed RP '{plan_name}': missing PGs {missing_pgs}\"\n+        )\n         return result\n \n     # Create the Recovery Plan (unless dry run)\n     if not dry_run:\n         try:\n@@ -9799,11 +10265,13 @@\n                 \"Version\": 1,\n             }\n \n             recovery_plans_table.put_item(Item=item)\n             result[\"details\"] = {\"planId\": plan_id}\n-            print(f\"[{correlation_id}] Created RP '{plan_name}' with ID {plan_id}\")\n+            print(\n+                f\"[{correlation_id}] Created RP '{plan_name}' with ID {plan_id}\"\n+            )\n         except Exception as e:\n             result[\"reason\"] = \"CREATE_ERROR\"\n             result[\"details\"] = {\"error\": str(e)}\n             print(f\"[{correlation_id}] Failed to create RP '{plan_name}': {e}\")\n             return result\n@@ -9850,11 +10318,13 @@\n             settings = {\n                 \"enabled\": rule_response.get(\"State\") == \"ENABLED\",\n                 \"intervalHours\": interval_hours,\n                 \"scheduleExpression\": schedule_expression,\n                 \"ruleName\": rule_name,\n-                \"lastModified\": rule_response.get(\"ModifiedDate\", \"\").isoformat()\n+                \"lastModified\": rule_response.get(\n+                    \"ModifiedDate\", \"\"\n+                ).isoformat()\n                 if rule_response.get(\"ModifiedDate\")\n                 else None,\n             }\n \n             return response(200, settings)\n@@ -9876,21 +10346,25 @@\n     except Exception as e:\n         print(f\"Error getting tag sync settings: {e}\")\n         import traceback\n \n         traceback.print_exc()\n-        return response(500, {\"error\": f\"Failed to get tag sync settings: {str(e)}\"})\n+        return response(\n+            500, {\"error\": f\"Failed to get tag sync settings: {str(e)}\"}\n+        )\n \n \n def update_tag_sync_settings(body: Dict) -> Dict:\n     \"\"\"Update tag sync configuration settings\"\"\"\n     try:\n         import boto3\n \n         # Validate input\n         if not isinstance(body, dict):\n-            return response(400, {\"error\": \"Request body must be a JSON object\"})\n+            return response(\n+                400, {\"error\": \"Request body must be a JSON object\"}\n+            )\n \n         enabled = body.get(\"enabled\")\n         interval_hours = body.get(\"intervalHours\")\n \n         if enabled is None:\n@@ -9905,11 +10379,13 @@\n                 or interval_hours < 1\n                 or interval_hours > 24\n             ):\n                 return response(\n                     400,\n-                    {\"error\": \"intervalHours must be a number between 1 and 24\"},\n+                    {\n+                        \"error\": \"intervalHours must be a number between 1 and 24\"\n+                    },\n                 )\n             interval_hours = int(interval_hours)\n \n         # Get EventBridge client\n         events_client = boto3.client(\"events\")\n@@ -9998,20 +10474,22 @@\n     except Exception as e:\n         print(f\"Error updating tag sync settings: {e}\")\n         import traceback\n \n         traceback.print_exc()\n-        return response(500, {\"error\": f\"Failed to update tag sync settings: {str(e)}\"})\n+        return response(\n+            500, {\"error\": f\"Failed to update tag sync settings: {str(e)}\"}\n+        )\n \n \n def parse_schedule_expression(schedule_expression: str) -> int:\n     \"\"\"Parse EventBridge schedule expression to extract interval hours\"\"\"\n     try:\n         # Handle rate expressions like \"rate(4 hours)\"\n-        if schedule_expression.startswith(\"rate(\") and schedule_expression.endswith(\n-            \")\"\n-        ):\n+        if schedule_expression.startswith(\n+            \"rate(\"\n+        ) and schedule_expression.endswith(\")\"):\n             rate_part = schedule_expression[5:-1]  # Remove \"rate(\" and \")\"\n \n             if \"hour\" in rate_part:\n                 # Extract number from \"4 hours\" or \"1 hour\"\n                 import re\n@@ -10028,11 +10506,13 @@\n \n         # Default fallback\n         return 4\n \n     except Exception as e:\n-        print(f\"Error parsing schedule expression '{schedule_expression}': {e}\")\n+        print(\n+            f\"Error parsing schedule expression '{schedule_expression}': {e}\"\n+        )\n         return 4\n \n \n def handle_eventbridge_tag_sync(event: Dict) -> Dict:\n     \"\"\"Handle EventBridge-triggered tag sync requests with enhanced security validation\"\"\"\n@@ -10041,32 +10521,42 @@\n \n         # Enhanced security validation for EventBridge requests\n         request_context = event.get(\"requestContext\", {})\n \n         # Validate request came through API Gateway (not direct Lambda invoke)\n-        if not request_context.get(\"requestId\") or not request_context.get(\"stage\"):\n-            print(\"Security warning: EventBridge request missing API Gateway context\")\n-            return response(403, {\"error\": \"Invalid EventBridge request context\"})\n+        if not request_context.get(\"requestId\") or not request_context.get(\n+            \"stage\"\n+        ):\n+            print(\n+                \"Security warning: EventBridge request missing API Gateway context\"\n+            )\n+            return response(\n+                403, {\"error\": \"Invalid EventBridge request context\"}\n+            )\n \n         # Validate no user authentication headers present (EventBridge shouldn't have them)\n         headers = event.get(\"headers\", {})\n         if headers.get(\"Authorization\") or headers.get(\"authorization\"):\n             print(\n                 \"Security warning: EventBridge request contains authentication headers\"\n             )\n             return response(\n                 403,\n-                {\"error\": \"Invalid EventBridge request - unexpected auth headers\"},\n+                {\n+                    \"error\": \"Invalid EventBridge request - unexpected auth headers\"\n+                },\n             )\n \n         # Validate EventBridge rule name matches expected pattern\n         invocation_details = event.get(\"invocationDetails\", {})\n         rule_name = invocation_details.get(\"scheduleRuleName\", \"\")\n         expected_rule_pattern = f\"aws-drs-orchestrator-tag-sync-schedule-\"\n \n         if not rule_name or not rule_name.startswith(expected_rule_pattern):\n-            print(f\"Security warning: Invalid EventBridge rule name: {rule_name}\")\n+            print(\n+                f\"Security warning: Invalid EventBridge rule name: {rule_name}\"\n+            )\n             return response(403, {\"error\": \"Invalid EventBridge rule source\"})\n \n         # Log security audit information\n         print(\n             f\"EventBridge security audit - requestId: {request_context.get('requestId')}, \"\n@@ -10088,6 +10578,8 @@\n     except Exception as e:\n         print(f\"Error in EventBridge tag sync handler: {e}\")\n         import traceback\n \n         traceback.print_exc()\n-        return response(500, {\"error\": f\"EventBridge tag sync failed: {str(e)}\"})\n+        return response(\n+            500, {\"error\": f\"EventBridge tag sync failed: {str(e)}\"}\n+        )\n",
      "errors": "would reformat lambda/build_and_deploy.py\nwould reformat lambda/deploy_lambda.py\nwould reformat tests/python/cleanup_all_data.py\nwould reformat tests/python/conftest.py\nwould reformat lambda/drs_tag_sync.py\nwould reformat lambda/execution_registry.py\nwould reformat tests/python/create_test_ui.py\nwould reformat tests/python/create_test_plan.py\nwould reformat scripts/add-current-account.py\nwould reformat tests/python/create_real_test_data.py\nwould reformat scripts/monitor_lambda_drill.py\nwould reformat tests/python/e2e/get_auth_token.py\nwould reformat tests/python/e2e/test_protection_groups_fix.py\nwould reformat lambda/tag_discovery.py\nwould reformat scripts/test_drs_drill_trace.py\nwould reformat scripts/manage-user-roles.py\nwould reformat tests/python/e2e/test_protection_group_crud.py\nwould reformat tests/python/e2e/test_recovery_plan_bugs.py\nwould reformat tests/python/monitor_execution.py\nwould reformat tests/python/test_drs_validation.py\nwould reformat tests/python/e2e/test_recovery_plan_api_crud.py\nwould reformat scripts/generate_quality_report.py\nwould reformat tests/python/monitor_drs_drill.py\nwould reformat tests/python/mocks/mock_drs_client.py\nwould reformat tests/python/standalone_drs_drill.py\nwould reformat tests/python/fixtures/recovery_plan_fixtures.py\nwould reformat tests/python/e2e/test_recovery_plan_e2e.py\nwould reformat tests/python/unit/test_infrastructure_smoke.py\nwould reformat tests/python/test_with_lambda_invoke.py\nwould reformat lambda/rbac_middleware.py\nwould reformat tests/python/unit/test_fixtures.py\nwould reformat tests/python/validate_setup.py\nwould reformat tests/security/demo_security_test.py\nwould reformat tests/python/automated_e2e_test.py\nwould reformat tests/python/unit/test_recovery_plan_delete.py\nwould reformat lambda/orchestration_stepfunctions.py\nwould reformat tests/python/unit/test_data_generator.py\nwould reformat tests/python/unit/test_mock_drs_client.py\nwould reformat tests/python/unit/test_wave_transformation.py\nwould reformat tests/security/run_security_tests.py\nwould reformat tests/python/utils/test_data_generator.py\nwould reformat tests/python/unit/test_drs_service_limits.py\nwould reformat tests/security/rbac_security_tests.py\nwould reformat lambda/index.py\n\nOh no! \ud83d\udca5 \ud83d\udc94 \ud83d\udca5\n44 files would be reformatted, 9 files would be left unchanged.\n",
      "return_code": 1
    },
    "flake8": {
      "tool": "flake8",
      "status": "failed",
      "files_checked": 53,
      "total_violations": 372,
      "violations": [
        {
          "file": "lambda/deploy_lambda.py",
          "line": 83,
          "column": 43,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/deploy_lambda.py",
          "line": 84,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/deploy_lambda.py",
          "line": 91,
          "column": 38,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/deploy_lambda.py",
          "line": 112,
          "column": 9,
          "message": "F841 local variable 'response' is assigned to but never used"
        },
        {
          "file": "lambda/deploy_lambda.py",
          "line": 196,
          "column": 53,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/execution_registry.py",
          "line": 145,
          "column": 21,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/execution_registry.py",
          "line": 148,
          "column": 26,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/execution_registry.py",
          "line": 151,
          "column": 21,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/execution_registry.py",
          "line": 155,
          "column": 28,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/execution_registry.py",
          "line": 157,
          "column": 33,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/execution_registry.py",
          "line": 161,
          "column": 21,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 11,
          "column": 1,
          "message": "F401 'datetime.timezone' imported but unused"
        },
        {
          "file": "lambda/index.py",
          "line": 19,
          "column": 1,
          "message": "F401 'rbac_middleware.DRSPermission' imported but unused"
        },
        {
          "file": "lambda/index.py",
          "line": 139,
          "column": 1,
          "message": "C901 'determine_target_account_context' is too complex (20)"
        },
        {
          "file": "lambda/index.py",
          "line": 249,
          "column": 71,
          "message": "E713 test for membership should be 'not in'"
        },
        {
          "file": "lambda/index.py",
          "line": 308,
          "column": 25,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 308,
          "column": 29,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 308,
          "column": 33,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 308,
          "column": 34,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 308,
          "column": 47,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 339,
          "column": 38,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 340,
          "column": 67,
          "message": "E713 test for membership should be 'not in'"
        },
        {
          "file": "lambda/index.py",
          "line": 347,
          "column": 67,
          "message": "E713 test for membership should be 'not in'"
        },
        {
          "file": "lambda/index.py",
          "line": 540,
          "column": 20,
          "message": "F821 undefined name 'ClientError'"
        },
        {
          "file": "lambda/index.py",
          "line": 570,
          "column": 1,
          "message": "C901 'get_servers_in_active_executions' is too complex (13)"
        },
        {
          "file": "lambda/index.py",
          "line": 594,
          "column": 13,
          "message": "F841 local variable 'wave_status' is assigned to but never used"
        },
        {
          "file": "lambda/index.py",
          "line": 812,
          "column": 1,
          "message": "C901 'get_plans_with_conflicts' is too complex (22)"
        },
        {
          "file": "lambda/index.py",
          "line": 1248,
          "column": 1,
          "message": "C901 'get_drs_account_capacity' is too complex (13)"
        },
        {
          "file": "lambda/index.py",
          "line": 1312,
          "column": 30,
          "message": "E713 test for membership should be 'not in'"
        },
        {
          "file": "lambda/index.py",
          "line": 1331,
          "column": 34,
          "message": "E713 test for membership should be 'not in'"
        },
        {
          "file": "lambda/index.py",
          "line": 1344,
          "column": 34,
          "message": "E713 test for membership should be 'not in'"
        },
        {
          "file": "lambda/index.py",
          "line": 1364,
          "column": 1,
          "message": "C901 'lambda_handler' is too complex (26)"
        },
        {
          "file": "lambda/index.py",
          "line": 1407,
          "column": 17,
          "message": "W504 line break after binary operator"
        },
        {
          "file": "lambda/index.py",
          "line": 1412,
          "column": 21,
          "message": "W504 line break after binary operator"
        },
        {
          "file": "lambda/index.py",
          "line": 1415,
          "column": 21,
          "message": "W504 line break after binary operator"
        },
        {
          "file": "lambda/index.py",
          "line": 1535,
          "column": 19,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 1540,
          "column": 19,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 1555,
          "column": 82,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 1672,
          "column": 1,
          "message": "C901 'query_drs_servers_by_tags' is too complex (21)"
        },
        {
          "file": "lambda/index.py",
          "line": 1881,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 1881,
          "column": 37,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 1882,
          "column": 17,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "lambda/index.py",
          "line": 1883,
          "column": 17,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "lambda/index.py",
          "line": 1900,
          "column": 25,
          "message": "E713 test for membership should be 'not in'"
        },
        {
          "file": "lambda/index.py",
          "line": 1906,
          "column": 1,
          "message": "C901 'create_protection_group' is too complex (29)"
        },
        {
          "file": "lambda/index.py",
          "line": 2221,
          "column": 1,
          "message": "C901 'update_protection_group' is too complex (36)"
        },
        {
          "file": "lambda/index.py",
          "line": 2272,
          "column": 36,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 2723,
          "column": 1,
          "message": "C901 'get_recovery_plans' is too complex (32)"
        },
        {
          "file": "lambda/index.py",
          "line": 2902,
          "column": 1,
          "message": "C901 'update_recovery_plan' is too complex (22)"
        },
        {
          "file": "lambda/index.py",
          "line": 3032,
          "column": 56,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 3122,
          "column": 1,
          "message": "C901 'check_existing_recovery_instances' is too complex (39)"
        },
        {
          "file": "lambda/index.py",
          "line": 3373,
          "column": 1,
          "message": "C901 'handle_executions' is too complex (13)"
        },
        {
          "file": "lambda/index.py",
          "line": 3425,
          "column": 1,
          "message": "C901 'execute_recovery_plan' is too complex (29)"
        },
        {
          "file": "lambda/index.py",
          "line": 3836,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 4159,
          "column": 9,
          "message": "F841 local variable 'launch_configs' is assigned to but never used"
        },
        {
          "file": "lambda/index.py",
          "line": 4172,
          "column": 13,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 4177,
          "column": 13,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 4199,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 4232,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 4335,
          "column": 9,
          "message": "F841 local variable 'e' is assigned to but never used"
        },
        {
          "file": "lambda/index.py",
          "line": 4398,
          "column": 1,
          "message": "C901 'list_executions' is too complex (30)"
        },
        {
          "file": "lambda/index.py",
          "line": 4518,
          "column": 39,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 4552,
          "column": 1,
          "message": "C901 'get_server_details_map' is too complex (24)"
        },
        {
          "file": "lambda/index.py",
          "line": 4745,
          "column": 1,
          "message": "C901 'recalculate_execution_status' is too complex (14)"
        },
        {
          "file": "lambda/index.py",
          "line": 4772,
          "column": 5,
          "message": "F841 local variable 'terminal_statuses' is assigned to but never used"
        },
        {
          "file": "lambda/index.py",
          "line": 4822,
          "column": 1,
          "message": "C901 'get_execution_details' is too complex (18)"
        },
        {
          "file": "lambda/index.py",
          "line": 4928,
          "column": 1,
          "message": "C901 'cancel_execution' is too complex (17)"
        },
        {
          "file": "lambda/index.py",
          "line": 5108,
          "column": 1,
          "message": "C901 'pause_execution' is too complex (12)"
        },
        {
          "file": "lambda/index.py",
          "line": 5253,
          "column": 1,
          "message": "C901 'resume_execution' is too complex (13)"
        },
        {
          "file": "lambda/index.py",
          "line": 5419,
          "column": 1,
          "message": "C901 'get_job_log_items' is too complex (13)"
        },
        {
          "file": "lambda/index.py",
          "line": 5530,
          "column": 1,
          "message": "C901 'terminate_recovery_instances' is too complex (60)"
        },
        {
          "file": "lambda/index.py",
          "line": 5717,
          "column": 17,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 6012,
          "column": 1,
          "message": "C901 'get_termination_job_status' is too complex (13)"
        },
        {
          "file": "lambda/index.py",
          "line": 6152,
          "column": 1,
          "message": "C901 'delete_completed_executions' is too complex (21)"
        },
        {
          "file": "lambda/index.py",
          "line": 6178,
          "column": 9,
          "message": "F841 local variable 'active_states' is assigned to but never used"
        },
        {
          "file": "lambda/index.py",
          "line": 6304,
          "column": 24,
          "message": "E702 multiple statements on one line (semicolon)"
        },
        {
          "file": "lambda/index.py",
          "line": 6329,
          "column": 1,
          "message": "C901 'delete_executions_by_ids' is too complex (22)"
        },
        {
          "file": "lambda/index.py",
          "line": 6357,
          "column": 9,
          "message": "F841 local variable 'terminal_states' is assigned to but never used"
        },
        {
          "file": "lambda/index.py",
          "line": 6497,
          "column": 24,
          "message": "E702 multiple statements on one line (semicolon)"
        },
        {
          "file": "lambda/index.py",
          "line": 6547,
          "column": 1,
          "message": "C901 'list_source_servers' is too complex (32)"
        },
        {
          "file": "lambda/index.py",
          "line": 6564,
          "column": 13,
          "message": "F841 local variable 'drs_initialized' is assigned to but never used"
        },
        {
          "file": "lambda/index.py",
          "line": 6566,
          "column": 25,
          "message": "E713 test for membership should be 'not in'"
        },
        {
          "file": "lambda/index.py",
          "line": 7387,
          "column": 1,
          "message": "C901 'transform_execution_to_camelcase' is too complex (19)"
        },
        {
          "file": "lambda/index.py",
          "line": 7666,
          "column": 1,
          "message": "C901 'validate_waves' is too complex (16)"
        },
        {
          "file": "lambda/index.py",
          "line": 7883,
          "column": 17,
          "message": "F821 undefined name 'ClientError'"
        },
        {
          "file": "lambda/index.py",
          "line": 8085,
          "column": 1,
          "message": "C901 'create_target_account' is too complex (21)"
        },
        {
          "file": "lambda/index.py",
          "line": 8241,
          "column": 1,
          "message": "C901 'update_target_account' is too complex (18)"
        },
        {
          "file": "lambda/index.py",
          "line": 8264,
          "column": 9,
          "message": "F841 local variable 'existing_account' is assigned to but never used"
        },
        {
          "file": "lambda/index.py",
          "line": 8395,
          "column": 1,
          "message": "C901 'validate_target_account' is too complex (13)"
        },
        {
          "file": "lambda/index.py",
          "line": 8573,
          "column": 1,
          "message": "C901 'sync_tags_in_region' is too complex (16)"
        },
        {
          "file": "lambda/index.py",
          "line": 8647,
          "column": 20,
          "message": "F821 undefined name 'ClientError'"
        },
        {
          "file": "lambda/index.py",
          "line": 8686,
          "column": 1,
          "message": "C901 'apply_launch_config_to_servers' is too complex (30)"
        },
        {
          "file": "lambda/index.py",
          "line": 8787,
          "column": 49,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 8789,
          "column": 51,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 8792,
          "column": 48,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 8798,
          "column": 52,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 8805,
          "column": 36,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 8809,
          "column": 33,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 9081,
          "column": 1,
          "message": "C901 'export_configuration' is too complex (14)"
        },
        {
          "file": "lambda/index.py",
          "line": 9192,
          "column": 1,
          "message": "C901 'import_configuration' is too complex (12)"
        },
        {
          "file": "lambda/index.py",
          "line": 9204,
          "column": 9,
          "message": "F401 'datetime' imported but unused"
        },
        {
          "file": "lambda/index.py",
          "line": 9414,
          "column": 1,
          "message": "C901 '_process_protection_group_import' is too complex (31)"
        },
        {
          "file": "lambda/index.py",
          "line": 9522,
          "column": 16,
          "message": "F821 undefined name 'body'"
        },
        {
          "file": "lambda/index.py",
          "line": 9522,
          "column": 25,
          "message": "F821 undefined name 'body'"
        },
        {
          "file": "lambda/index.py",
          "line": 9524,
          "column": 34,
          "message": "F821 undefined name 'body'"
        },
        {
          "file": "lambda/index.py",
          "line": 9525,
          "column": 39,
          "message": "F821 undefined name 'body'"
        },
        {
          "file": "lambda/index.py",
          "line": 9598,
          "column": 24,
          "message": "F821 undefined name 'body'"
        },
        {
          "file": "lambda/index.py",
          "line": 9598,
          "column": 33,
          "message": "F821 undefined name 'body'"
        },
        {
          "file": "lambda/index.py",
          "line": 9600,
          "column": 42,
          "message": "F821 undefined name 'body'"
        },
        {
          "file": "lambda/index.py",
          "line": 9601,
          "column": 47,
          "message": "F821 undefined name 'body'"
        },
        {
          "file": "lambda/index.py",
          "line": 9648,
          "column": 1,
          "message": "C901 '_process_recovery_plan_import' is too complex (21)"
        },
        {
          "file": "lambda/index.py",
          "line": 9721,
          "column": 44,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/index.py",
          "line": 9884,
          "column": 1,
          "message": "C901 'update_tag_sync_settings' is too complex (14)"
        },
        {
          "file": "lambda/index.py",
          "line": 9986,
          "column": 24,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/index.py",
          "line": 10064,
          "column": 33,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 12,
          "column": 1,
          "message": "F401 'datetime.datetime' imported but unused"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 12,
          "column": 1,
          "message": "F401 'datetime.timezone' imported but unused"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 14,
          "column": 1,
          "message": "F401 'typing.Any' imported but unused"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 90,
          "column": 30,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 90,
          "column": 34,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 90,
          "column": 38,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 90,
          "column": 39,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 90,
          "column": 52,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 330,
          "column": 1,
          "message": "C901 'query_drs_servers_by_tags' is too complex (11)"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 395,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 395,
          "column": 37,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 396,
          "column": 17,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 397,
          "column": 17,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 528,
          "column": 1,
          "message": "C901 'update_wave_status' is too complex (57)"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 550,
          "column": 23,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 616,
          "column": 23,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 747,
          "column": 19,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/orchestration_stepfunctions.py",
          "line": 834,
          "column": 27,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "lambda/rbac_middleware.py",
          "line": 9,
          "column": 1,
          "message": "F401 'typing.Optional' imported but unused"
        },
        {
          "file": "lambda/tag_discovery.py",
          "line": 10,
          "column": 1,
          "message": "F401 'typing.Optional' imported but unused"
        },
        {
          "file": "lambda/tag_discovery.py",
          "line": 168,
          "column": 1,
          "message": "C901 'discover_multi_account' is too complex (11)"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 5,
          "column": 74,
          "message": "W291 trailing whitespace"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 6,
          "column": 70,
          "message": "W291 trailing whitespace"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 14,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 41,
          "column": 5,
          "message": "E722 do not use bare 'except'"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 49,
          "column": 5,
          "message": "E722 do not use bare 'except'"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 89,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 89,
          "column": 17,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 90,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 90,
          "column": 17,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 91,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 91,
          "column": 17,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "scripts/add-current-account.py",
          "line": 142,
          "column": 22,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 15,
          "column": 1,
          "message": "F401 'os' imported but unused"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 18,
          "column": 1,
          "message": "F401 'tempfile' imported but unused"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 21,
          "column": 1,
          "message": "F401 'typing.Optional' imported but unused"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 279,
          "column": 1,
          "message": "W293 blank line contains whitespace"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 298,
          "column": 1,
          "message": "W293 blank line contains whitespace"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 300,
          "column": 1,
          "message": "W293 blank line contains whitespace"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 324,
          "column": 23,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 325,
          "column": 43,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 328,
          "column": 57,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 331,
          "column": 36,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 333,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 333,
          "column": 72,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 333,
          "column": 92,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 334,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 336,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 336,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 344,
          "column": 40,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 349,
          "column": 21,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 351,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 352,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 352,
          "column": 26,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 354,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 452,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/generate_quality_report.py",
          "line": 494,
          "column": 19,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/manage-user-roles.py",
          "line": 8,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "scripts/manage-user-roles.py",
          "line": 10,
          "column": 1,
          "message": "F401 'typing.Optional' imported but unused"
        },
        {
          "file": "scripts/manage-user-roles.py",
          "line": 179,
          "column": 23,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/manage-user-roles.py",
          "line": 179,
          "column": 33,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "scripts/manage-user-roles.py",
          "line": 184,
          "column": 23,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/manage-user-roles.py",
          "line": 197,
          "column": 1,
          "message": "C901 'main' is too complex (13)"
        },
        {
          "file": "scripts/manage-user-roles.py",
          "line": 207,
          "column": 5,
          "message": "F841 local variable 'list_parser' is assigned to but never used"
        },
        {
          "file": "scripts/manage-user-roles.py",
          "line": 212,
          "column": 5,
          "message": "F841 local variable 'roles_parser' is assigned to but never used"
        },
        {
          "file": "scripts/manage-user-roles.py",
          "line": 308,
          "column": 19,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/monitor_lambda_drill.py",
          "line": 11,
          "column": 1,
          "message": "C901 'monitor_execution' is too complex (14)"
        },
        {
          "file": "scripts/monitor_lambda_drill.py",
          "line": 15,
          "column": 5,
          "message": "F841 local variable 'drs' is assigned to but never used"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 7,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 23,
          "column": 19,
          "message": "E226 missing whitespace around arithmetic operator"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 25,
          "column": 17,
          "message": "E226 missing whitespace around arithmetic operator"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 28,
          "column": 1,
          "message": "C901 'test_drs_recovery' is too complex (26)"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 44,
          "column": 21,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 47,
          "column": 23,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 70,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 70,
          "column": 45,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 88,
          "column": 19,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 117,
          "column": 30,
          "message": "E226 missing whitespace around arithmetic operator"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 126,
          "column": 32,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 152,
          "column": 23,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 179,
          "column": 34,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 185,
          "column": 27,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "scripts/test_drs_drill_trace.py",
          "line": 188,
          "column": 27,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 10,
          "column": 1,
          "message": "F401 'typing.Optional' imported but unused"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 90,
          "column": 25,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 101,
          "column": 25,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 106,
          "column": 25,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 110,
          "column": 13,
          "message": "F841 local variable 'ec2_results' is assigned to but never used"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 111,
          "column": 25,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 133,
          "column": 36,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 225,
          "column": 48,
          "message": "E713 test for membership should be 'not in'"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 263,
          "column": 28,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 435,
          "column": 27,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 440,
          "column": 27,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 447,
          "column": 15,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 453,
          "column": 32,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/automated_e2e_test.py",
          "line": 458,
          "column": 34,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/check_pg_response.py",
          "line": 27,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/check_pg_response.py",
          "line": 27,
          "column": 24,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/create_real_test_data.py",
          "line": 115,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_real_test_data.py",
          "line": 117,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_real_test_data.py",
          "line": 118,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_real_test_data.py",
          "line": 119,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_real_test_data.py",
          "line": 119,
          "column": 30,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/create_real_test_data.py",
          "line": 120,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_real_test_data.py",
          "line": 121,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_test_plan.py",
          "line": 105,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_test_plan.py",
          "line": 105,
          "column": 34,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/create_test_plan.py",
          "line": 114,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_test_plan.py",
          "line": 114,
          "column": 47,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/create_test_plan.py",
          "line": 118,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_test_plan.py",
          "line": 118,
          "column": 35,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/create_test_plan.py",
          "line": 119,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_test_plan.py",
          "line": 121,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_test_ui.py",
          "line": 64,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_test_ui.py",
          "line": 67,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/create_test_ui.py",
          "line": 67,
          "column": 30,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/create_test_ui.py",
          "line": 68,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/get_auth_token.py",
          "line": 6,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "tests/python/e2e/get_auth_token.py",
          "line": 43,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/get_auth_token.py",
          "line": 44,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/get_auth_token.py",
          "line": 44,
          "column": 43,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/e2e/get_auth_token.py",
          "line": 46,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/get_auth_token.py",
          "line": 46,
          "column": 40,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/e2e/test_protection_group_crud.py",
          "line": 6,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "tests/python/e2e/test_protection_group_crud.py",
          "line": 79,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_protection_group_crud.py",
          "line": 113,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_protection_group_crud.py",
          "line": 134,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_protection_group_crud.py",
          "line": 144,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_protection_group_crud.py",
          "line": 164,
          "column": 19,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_protection_group_crud.py",
          "line": 165,
          "column": 19,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_protection_group_crud.py",
          "line": 171,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_protection_group_crud.py",
          "line": 173,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_protection_groups_fix.py",
          "line": 6,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_api_crud.py",
          "line": 64,
          "column": 43,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_api_crud.py",
          "line": 251,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_api_crud.py",
          "line": 284,
          "column": 5,
          "message": "F841 local variable 'updated_plan' is assigned to but never used"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_api_crud.py",
          "line": 286,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_api_crud.py",
          "line": 287,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_api_crud.py",
          "line": 299,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_bugs.py",
          "line": 5,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_bugs.py",
          "line": 7,
          "column": 1,
          "message": "F401 'typing.Any' imported but unused"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_bugs.py",
          "line": 7,
          "column": 1,
          "message": "F401 'typing.Dict' imported but unused"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_bugs.py",
          "line": 116,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_bugs.py",
          "line": 134,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_bugs.py",
          "line": 164,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 6,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 129,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 130,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 131,
          "column": 13,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 132,
          "column": 13,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 133,
          "column": 13,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 209,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 210,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 210,
          "column": 13,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 211,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 211,
          "column": 13,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 212,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 212,
          "column": 13,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/python/e2e/test_recovery_plan_e2e.py",
          "line": 239,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 60,
          "column": 39,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 70,
          "column": 40,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 80,
          "column": 40,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 110,
          "column": 40,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 140,
          "column": 40,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 150,
          "column": 39,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 160,
          "column": 40,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 170,
          "column": 40,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 180,
          "column": 40,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 301,
          "column": 41,
          "message": "E226 missing whitespace around arithmetic operator"
        },
        {
          "file": "tests/python/fixtures/recovery_plan_fixtures.py",
          "line": 301,
          "column": 43,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 5,
          "column": 1,
          "message": "F401 'time' imported but unused"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 91,
          "column": 29,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 91,
          "column": 33,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 91,
          "column": 37,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 91,
          "column": 47,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 91,
          "column": 60,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 205,
          "column": 25,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 205,
          "column": 29,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 205,
          "column": 33,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 205,
          "column": 43,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 205,
          "column": 56,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 355,
          "column": 29,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 355,
          "column": 33,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 355,
          "column": 37,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 355,
          "column": 47,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/mocks/mock_drs_client.py",
          "line": 355,
          "column": 60,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/monitor_drs_drill.py",
          "line": 14,
          "column": 1,
          "message": "C901 'monitor_drill' is too complex (11)"
        },
        {
          "file": "tests/python/monitor_drs_drill.py",
          "line": 22,
          "column": 62,
          "message": "E226 missing whitespace around arithmetic operator"
        },
        {
          "file": "tests/python/monitor_execution.py",
          "line": 13,
          "column": 1,
          "message": "C901 'monitor_execution' is too complex (11)"
        },
        {
          "file": "tests/python/monitor_execution.py",
          "line": 16,
          "column": 5,
          "message": "F841 local variable 'drs' is assigned to but never used"
        },
        {
          "file": "tests/python/standalone_drs_drill.py",
          "line": 2,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "tests/python/standalone_drs_drill.py",
          "line": 64,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/standalone_drs_drill.py",
          "line": 72,
          "column": 19,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/standalone_drs_drill.py",
          "line": 173,
          "column": 1,
          "message": "C901 'monitor_drill_progress' is too complex (14)"
        },
        {
          "file": "tests/python/standalone_drs_drill.py",
          "line": 257,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/standalone_drs_drill.py",
          "line": 257,
          "column": 27,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/python/standalone_drs_drill.py",
          "line": 280,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/test_drs_validation.py",
          "line": 7,
          "column": 1,
          "message": "F401 'json' imported but unused"
        },
        {
          "file": "tests/python/test_drs_validation.py",
          "line": 43,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/test_drs_validation.py",
          "line": 78,
          "column": 7,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/test_drs_validation.py",
          "line": 91,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/test_with_lambda_invoke.py",
          "line": 8,
          "column": 1,
          "message": "F401 'datetime.datetime' imported but unused"
        },
        {
          "file": "tests/python/test_with_lambda_invoke.py",
          "line": 8,
          "column": 1,
          "message": "F401 'datetime.timezone' imported but unused"
        },
        {
          "file": "tests/python/test_with_lambda_invoke.py",
          "line": 55,
          "column": 13,
          "message": "F841 local variable 'drs_status' is assigned to but never used"
        },
        {
          "file": "tests/python/test_with_lambda_invoke.py",
          "line": 56,
          "column": 25,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/python/unit/test_data_generator.py",
          "line": 7,
          "column": 1,
          "message": "F401 'pytest' imported but unused"
        },
        {
          "file": "tests/python/unit/test_recovery_plan_delete.py",
          "line": 8,
          "column": 1,
          "message": "F401 'unittest.mock.MagicMock' imported but unused"
        },
        {
          "file": "tests/python/unit/test_recovery_plan_delete.py",
          "line": 27,
          "column": 1,
          "message": "E402 module level import not at top of file"
        },
        {
          "file": "tests/python/unit/test_wave_transformation.py",
          "line": 25,
          "column": 1,
          "message": "E402 module level import not at top of file"
        },
        {
          "file": "tests/python/utils/test_data_generator.py",
          "line": 149,
          "column": 5,
          "message": "F841 local variable 'total_servers' is assigned to but never used"
        },
        {
          "file": "tests/python/validate_setup.py",
          "line": 96,
          "column": 31,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/demo_security_test.py",
          "line": 99,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/security/demo_security_test.py",
          "line": 100,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/security/demo_security_test.py",
          "line": 148,
          "column": 11,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/security/demo_security_test.py",
          "line": 148,
          "column": 38,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 15,
          "column": 1,
          "message": "F401 'time' imported but unused"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 20,
          "column": 1,
          "message": "F401 'typing.Tuple' imported but unused"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 363,
          "column": 39,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 383,
          "column": 39,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 406,
          "column": 35,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 421,
          "column": 5,
          "message": "C901 'RBACSecurityTester._test_permission' is too complex (13)"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 807,
          "column": 17,
          "message": "E722 do not use bare 'except'"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 843,
          "column": 9,
          "message": "F841 local variable 'test_id' is assigned to but never used"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 1107,
          "column": 9,
          "message": "F841 local variable 'findings' is assigned to but never used"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 1131,
          "column": 75,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 1132,
          "column": 15,
          "message": "F541 f-string is missing placeholders"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 1132,
          "column": 41,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/rbac_security_tests.py",
          "line": 1139,
          "column": 81,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 18,
          "column": 1,
          "message": "F401 'os' imported but unused"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 26,
          "column": 1,
          "message": "E402 module level import not at top of file"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 204,
          "column": 38,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 206,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 207,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 208,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 208,
          "column": 41,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 209,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 209,
          "column": 48,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 211,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 211,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 211,
          "column": 44,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 211,
          "column": 79,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 214,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 214,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 225,
          "column": 37,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 227,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 228,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 230,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 230,
          "column": 0,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 230,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 230,
          "column": 27,
          "message": "E221 multiple spaces before operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 232,
          "column": 0,
          "message": "E222 multiple spaces after operator"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 349,
          "column": 9,
          "message": "F841 local variable 'findings' is assigned to but never used"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 385,
          "column": 77,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/run_security_tests.py",
          "line": 396,
          "column": 83,
          "message": "E231 missing whitespace after ':'"
        },
        {
          "file": "tests/security/validate_framework.py",
          "line": 16,
          "column": 1,
          "message": "C901 'validate_configuration' is too complex (12)"
        },
        {
          "file": "tests/security/validate_framework.py",
          "line": 93,
          "column": 9,
          "message": "F401 'jwt' imported but unused"
        }
      ],
      "output": "lambda/deploy_lambda.py:83:43: E231 missing whitespace after ':'\nlambda/deploy_lambda.py:84:11: F541 f-string is missing placeholders\nlambda/deploy_lambda.py:91:38: E231 missing whitespace after ':'\nlambda/deploy_lambda.py:112:9: F841 local variable 'response' is assigned to but never used\nlambda/deploy_lambda.py:196:53: E231 missing whitespace after ':'\nlambda/execution_registry.py:145:21: E231 missing whitespace after ':'\nlambda/execution_registry.py:148:26: E231 missing whitespace after ':'\nlambda/execution_registry.py:151:21: E231 missing whitespace after ':'\nlambda/execution_registry.py:155:28: E231 missing whitespace after ':'\nlambda/execution_registry.py:157:33: E231 missing whitespace after ':'\nlambda/execution_registry.py:161:21: E231 missing whitespace after ':'\nlambda/index.py:11:1: F401 'datetime.timezone' imported but unused\nlambda/index.py:19:1: F401 'rbac_middleware.DRSPermission' imported but unused\nlambda/index.py:139:1: C901 'determine_target_account_context' is too complex (20)\nlambda/index.py:249:71: E713 test for membership should be 'not in'\nlambda/index.py:308:25: E231 missing whitespace after ':'\nlambda/index.py:308:29: E231 missing whitespace after ':'\nlambda/index.py:308:33: E231 missing whitespace after ':'\nlambda/index.py:308:34: E231 missing whitespace after ':'\nlambda/index.py:308:47: E231 missing whitespace after ':'\nlambda/index.py:339:38: E231 missing whitespace after ':'\nlambda/index.py:340:67: E713 test for membership should be 'not in'\nlambda/index.py:347:67: E713 test for membership should be 'not in'\nlambda/index.py:540:20: F821 undefined name 'ClientError'\nlambda/index.py:570:1: C901 'get_servers_in_active_executions' is too complex (13)\nlambda/index.py:594:13: F841 local variable 'wave_status' is assigned to but never used\nlambda/index.py:812:1: C901 'get_plans_with_conflicts' is too complex (22)\nlambda/index.py:1248:1: C901 'get_drs_account_capacity' is too complex (13)\nlambda/index.py:1312:30: E713 test for membership should be 'not in'\nlambda/index.py:1331:34: E713 test for membership should be 'not in'\nlambda/index.py:1344:34: E713 test for membership should be 'not in'\nlambda/index.py:1364:1: C901 'lambda_handler' is too complex (26)\nlambda/index.py:1407:17: W504 line break after binary operator\nlambda/index.py:1412:21: W504 line break after binary operator\nlambda/index.py:1415:21: W504 line break after binary operator\nlambda/index.py:1535:19: F541 f-string is missing placeholders\nlambda/index.py:1540:19: F541 f-string is missing placeholders\nlambda/index.py:1555:82: E231 missing whitespace after ':'\nlambda/index.py:1672:1: C901 'query_drs_servers_by_tags' is too complex (21)\nlambda/index.py:1881:15: F541 f-string is missing placeholders\nlambda/index.py:1881:37: E231 missing whitespace after ':'\nlambda/index.py:1882:17: E221 multiple spaces before operator\nlambda/index.py:1883:17: E221 multiple spaces before operator\nlambda/index.py:1900:25: E713 test for membership should be 'not in'\nlambda/index.py:1906:1: C901 'create_protection_group' is too complex (29)\nlambda/index.py:2221:1: C901 'update_protection_group' is too complex (36)\nlambda/index.py:2272:36: F541 f-string is missing placeholders\nlambda/index.py:2723:1: C901 'get_recovery_plans' is too complex (32)\nlambda/index.py:2902:1: C901 'update_recovery_plan' is too complex (22)\nlambda/index.py:3032:56: E231 missing whitespace after ':'\nlambda/index.py:3122:1: C901 'check_existing_recovery_instances' is too complex (39)\nlambda/index.py:3373:1: C901 'handle_executions' is too complex (13)\nlambda/index.py:3425:1: C901 'execute_recovery_plan' is too complex (29)\nlambda/index.py:3836:15: F541 f-string is missing placeholders\nlambda/index.py:4159:9: F841 local variable 'launch_configs' is assigned to but never used\nlambda/index.py:4172:13: F541 f-string is missing placeholders\nlambda/index.py:4177:13: F541 f-string is missing placeholders\nlambda/index.py:4199:15: F541 f-string is missing placeholders\nlambda/index.py:4232:15: F541 f-string is missing placeholders\nlambda/index.py:4335:9: F841 local variable 'e' is assigned to but never used\nlambda/index.py:4398:1: C901 'list_executions' is too complex (30)\nlambda/index.py:4518:39: F541 f-string is missing placeholders\nlambda/index.py:4552:1: C901 'get_server_details_map' is too complex (24)\nlambda/index.py:4745:1: C901 'recalculate_execution_status' is too complex (14)\nlambda/index.py:4772:5: F841 local variable 'terminal_statuses' is assigned to but never used\nlambda/index.py:4822:1: C901 'get_execution_details' is too complex (18)\nlambda/index.py:4928:1: C901 'cancel_execution' is too complex (17)\nlambda/index.py:5108:1: C901 'pause_execution' is too complex (12)\nlambda/index.py:5253:1: C901 'resume_execution' is too complex (13)\nlambda/index.py:5419:1: C901 'get_job_log_items' is too complex (13)\nlambda/index.py:5530:1: C901 'terminate_recovery_instances' is too complex (60)\nlambda/index.py:5717:17: F541 f-string is missing placeholders\nlambda/index.py:6012:1: C901 'get_termination_job_status' is too complex (13)\nlambda/index.py:6152:1: C901 'delete_completed_executions' is too complex (21)\nlambda/index.py:6178:9: F841 local variable 'active_states' is assigned to but never used\nlambda/index.py:6304:24: E702 multiple statements on one line (semicolon)\nlambda/index.py:6329:1: C901 'delete_executions_by_ids' is too complex (22)\nlambda/index.py:6357:9: F841 local variable 'terminal_states' is assigned to but never used\nlambda/index.py:6497:24: E702 multiple statements on one line (semicolon)\nlambda/index.py:6547:1: C901 'list_source_servers' is too complex (32)\nlambda/index.py:6564:13: F841 local variable 'drs_initialized' is assigned to but never used\nlambda/index.py:6566:25: E713 test for membership should be 'not in'\nlambda/index.py:7387:1: C901 'transform_execution_to_camelcase' is too complex (19)\nlambda/index.py:7666:1: C901 'validate_waves' is too complex (16)\nlambda/index.py:7883:17: F821 undefined name 'ClientError'\nlambda/index.py:8085:1: C901 'create_target_account' is too complex (21)\nlambda/index.py:8241:1: C901 'update_target_account' is too complex (18)\nlambda/index.py:8264:9: F841 local variable 'existing_account' is assigned to but never used\nlambda/index.py:8395:1: C901 'validate_target_account' is too complex (13)\nlambda/index.py:8573:1: C901 'sync_tags_in_region' is too complex (16)\nlambda/index.py:8647:20: F821 undefined name 'ClientError'\nlambda/index.py:8686:1: C901 'apply_launch_config_to_servers' is too complex (30)\nlambda/index.py:8787:49: E231 missing whitespace after ':'\nlambda/index.py:8789:51: E231 missing whitespace after ':'\nlambda/index.py:8792:48: E231 missing whitespace after ':'\nlambda/index.py:8798:52: E231 missing whitespace after ':'\nlambda/index.py:8805:36: E231 missing whitespace after ':'\nlambda/index.py:8809:33: E231 missing whitespace after ':'\nlambda/index.py:9081:1: C901 'export_configuration' is too complex (14)\nlambda/index.py:9192:1: C901 'import_configuration' is too complex (12)\nlambda/index.py:9204:9: F401 'datetime' imported but unused\nlambda/index.py:9414:1: C901 '_process_protection_group_import' is too complex (31)\nlambda/index.py:9522:16: F821 undefined name 'body'\nlambda/index.py:9522:25: F821 undefined name 'body'\nlambda/index.py:9524:34: F821 undefined name 'body'\nlambda/index.py:9525:39: F821 undefined name 'body'\nlambda/index.py:9598:24: F821 undefined name 'body'\nlambda/index.py:9598:33: F821 undefined name 'body'\nlambda/index.py:9600:42: F821 undefined name 'body'\nlambda/index.py:9601:47: F821 undefined name 'body'\nlambda/index.py:9648:1: C901 '_process_recovery_plan_import' is too complex (21)\nlambda/index.py:9721:44: E231 missing whitespace after ':'\nlambda/index.py:9884:1: C901 'update_tag_sync_settings' is too complex (14)\nlambda/index.py:9986:24: F541 f-string is missing placeholders\nlambda/index.py:10064:33: F541 f-string is missing placeholders\nlambda/orchestration_stepfunctions.py:12:1: F401 'datetime.datetime' imported but unused\nlambda/orchestration_stepfunctions.py:12:1: F401 'datetime.timezone' imported but unused\nlambda/orchestration_stepfunctions.py:14:1: F401 'typing.Any' imported but unused\nlambda/orchestration_stepfunctions.py:90:30: E231 missing whitespace after ':'\nlambda/orchestration_stepfunctions.py:90:34: E231 missing whitespace after ':'\nlambda/orchestration_stepfunctions.py:90:38: E231 missing whitespace after ':'\nlambda/orchestration_stepfunctions.py:90:39: E231 missing whitespace after ':'\nlambda/orchestration_stepfunctions.py:90:52: E231 missing whitespace after ':'\nlambda/orchestration_stepfunctions.py:330:1: C901 'query_drs_servers_by_tags' is too complex (11)\nlambda/orchestration_stepfunctions.py:395:15: F541 f-string is missing placeholders\nlambda/orchestration_stepfunctions.py:395:37: E231 missing whitespace after ':'\nlambda/orchestration_stepfunctions.py:396:17: E221 multiple spaces before operator\nlambda/orchestration_stepfunctions.py:397:17: E221 multiple spaces before operator\nlambda/orchestration_stepfunctions.py:528:1: C901 'update_wave_status' is too complex (57)\nlambda/orchestration_stepfunctions.py:550:23: F541 f-string is missing placeholders\nlambda/orchestration_stepfunctions.py:616:23: F541 f-string is missing placeholders\nlambda/orchestration_stepfunctions.py:747:19: F541 f-string is missing placeholders\nlambda/orchestration_stepfunctions.py:834:27: F541 f-string is missing placeholders\nlambda/rbac_middleware.py:9:1: F401 'typing.Optional' imported but unused\nlambda/tag_discovery.py:10:1: F401 'typing.Optional' imported but unused\nlambda/tag_discovery.py:168:1: C901 'discover_multi_account' is too complex (11)\nscripts/add-current-account.py:5:74: W291 trailing whitespace\nscripts/add-current-account.py:6:70: W291 trailing whitespace\nscripts/add-current-account.py:14:1: F401 'json' imported but unused\nscripts/add-current-account.py:41:5: E722 do not use bare 'except'\nscripts/add-current-account.py:49:5: E722 do not use bare 'except'\nscripts/add-current-account.py:89:15: F541 f-string is missing placeholders\nscripts/add-current-account.py:89:17: E221 multiple spaces before operator\nscripts/add-current-account.py:90:15: F541 f-string is missing placeholders\nscripts/add-current-account.py:90:17: E221 multiple spaces before operator\nscripts/add-current-account.py:91:15: F541 f-string is missing placeholders\nscripts/add-current-account.py:91:17: E221 multiple spaces before operator\nscripts/add-current-account.py:142:22: F541 f-string is missing placeholders\nscripts/generate_quality_report.py:15:1: F401 'os' imported but unused\nscripts/generate_quality_report.py:18:1: F401 'tempfile' imported but unused\nscripts/generate_quality_report.py:21:1: F401 'typing.Optional' imported but unused\nscripts/generate_quality_report.py:279:1: W293 blank line contains whitespace\nscripts/generate_quality_report.py:298:1: W293 blank line contains whitespace\nscripts/generate_quality_report.py:300:1: W293 blank line contains whitespace\nscripts/generate_quality_report.py:324:23: F541 f-string is missing placeholders\nscripts/generate_quality_report.py:325:43: E231 missing whitespace after ':'\nscripts/generate_quality_report.py:328:57: E231 missing whitespace after ':'\nscripts/generate_quality_report.py:331:36: E221 multiple spaces before operator\nscripts/generate_quality_report.py:333:0: E222 multiple spaces after operator\nscripts/generate_quality_report.py:333:72: E231 missing whitespace after ':'\nscripts/generate_quality_report.py:333:92: E231 missing whitespace after ':'\nscripts/generate_quality_report.py:334:0: E222 multiple spaces after operator\nscripts/generate_quality_report.py:336:-27: E222 multiple spaces after operator\nscripts/generate_quality_report.py:336:0: E222 multiple spaces after operator\nscripts/generate_quality_report.py:344:40: E231 missing whitespace after ':'\nscripts/generate_quality_report.py:349:21: E221 multiple spaces before operator\nscripts/generate_quality_report.py:351:0: E222 multiple spaces after operator\nscripts/generate_quality_report.py:352:0: E222 multiple spaces after operator\nscripts/generate_quality_report.py:352:26: E221 multiple spaces before operator\nscripts/generate_quality_report.py:354:0: E222 multiple spaces after operator\nscripts/generate_quality_report.py:452:15: F541 f-string is missing placeholders\nscripts/generate_quality_report.py:494:19: F541 f-string is missing placeholders\nscripts/manage-user-roles.py:8:1: F401 'json' imported but unused\nscripts/manage-user-roles.py:10:1: F401 'typing.Optional' imported but unused\nscripts/manage-user-roles.py:179:23: F541 f-string is missing placeholders\nscripts/manage-user-roles.py:179:33: E231 missing whitespace after ':'\nscripts/manage-user-roles.py:184:23: F541 f-string is missing placeholders\nscripts/manage-user-roles.py:197:1: C901 'main' is too complex (13)\nscripts/manage-user-roles.py:207:5: F841 local variable 'list_parser' is assigned to but never used\nscripts/manage-user-roles.py:212:5: F841 local variable 'roles_parser' is assigned to but never used\nscripts/manage-user-roles.py:308:19: F541 f-string is missing placeholders\nscripts/monitor_lambda_drill.py:11:1: C901 'monitor_execution' is too complex (14)\nscripts/monitor_lambda_drill.py:15:5: F841 local variable 'drs' is assigned to but never used\nscripts/test_drs_drill_trace.py:7:1: F401 'json' imported but unused\nscripts/test_drs_drill_trace.py:23:19: E226 missing whitespace around arithmetic operator\nscripts/test_drs_drill_trace.py:25:17: E226 missing whitespace around arithmetic operator\nscripts/test_drs_drill_trace.py:28:1: C901 'test_drs_recovery' is too complex (26)\nscripts/test_drs_drill_trace.py:44:21: E221 multiple spaces before operator\nscripts/test_drs_drill_trace.py:47:23: F541 f-string is missing placeholders\nscripts/test_drs_drill_trace.py:70:15: F541 f-string is missing placeholders\nscripts/test_drs_drill_trace.py:70:45: E231 missing whitespace after ':'\nscripts/test_drs_drill_trace.py:88:19: E221 multiple spaces before operator\nscripts/test_drs_drill_trace.py:117:30: E226 missing whitespace around arithmetic operator\nscripts/test_drs_drill_trace.py:126:32: E231 missing whitespace after ':'\nscripts/test_drs_drill_trace.py:152:23: F541 f-string is missing placeholders\nscripts/test_drs_drill_trace.py:179:34: E231 missing whitespace after ':'\nscripts/test_drs_drill_trace.py:185:27: F541 f-string is missing placeholders\nscripts/test_drs_drill_trace.py:188:27: F541 f-string is missing placeholders\ntests/python/automated_e2e_test.py:10:1: F401 'typing.Optional' imported but unused\ntests/python/automated_e2e_test.py:90:25: F541 f-string is missing placeholders\ntests/python/automated_e2e_test.py:101:25: F541 f-string is missing placeholders\ntests/python/automated_e2e_test.py:106:25: F541 f-string is missing placeholders\ntests/python/automated_e2e_test.py:110:13: F841 local variable 'ec2_results' is assigned to but never used\ntests/python/automated_e2e_test.py:111:25: F541 f-string is missing placeholders\ntests/python/automated_e2e_test.py:133:36: E221 multiple spaces before operator\ntests/python/automated_e2e_test.py:225:48: E713 test for membership should be 'not in'\ntests/python/automated_e2e_test.py:263:28: F541 f-string is missing placeholders\ntests/python/automated_e2e_test.py:435:27: E221 multiple spaces before operator\ntests/python/automated_e2e_test.py:440:27: E221 multiple spaces before operator\ntests/python/automated_e2e_test.py:447:15: E221 multiple spaces before operator\ntests/python/automated_e2e_test.py:453:32: E221 multiple spaces before operator\ntests/python/automated_e2e_test.py:458:34: E221 multiple spaces before operator\ntests/python/check_pg_response.py:27:7: F541 f-string is missing placeholders\ntests/python/check_pg_response.py:27:24: E231 missing whitespace after ':'\ntests/python/create_real_test_data.py:115:7: F541 f-string is missing placeholders\ntests/python/create_real_test_data.py:117:7: F541 f-string is missing placeholders\ntests/python/create_real_test_data.py:118:7: F541 f-string is missing placeholders\ntests/python/create_real_test_data.py:119:7: F541 f-string is missing placeholders\ntests/python/create_real_test_data.py:119:30: E231 missing whitespace after ':'\ntests/python/create_real_test_data.py:120:7: F541 f-string is missing placeholders\ntests/python/create_real_test_data.py:121:7: F541 f-string is missing placeholders\ntests/python/create_test_plan.py:105:7: F541 f-string is missing placeholders\ntests/python/create_test_plan.py:105:34: E231 missing whitespace after ':'\ntests/python/create_test_plan.py:114:11: F541 f-string is missing placeholders\ntests/python/create_test_plan.py:114:47: E231 missing whitespace after ':'\ntests/python/create_test_plan.py:118:11: F541 f-string is missing placeholders\ntests/python/create_test_plan.py:118:35: E231 missing whitespace after ':'\ntests/python/create_test_plan.py:119:11: F541 f-string is missing placeholders\ntests/python/create_test_plan.py:121:11: F541 f-string is missing placeholders\ntests/python/create_test_ui.py:64:7: F541 f-string is missing placeholders\ntests/python/create_test_ui.py:67:7: F541 f-string is missing placeholders\ntests/python/create_test_ui.py:67:30: E231 missing whitespace after ':'\ntests/python/create_test_ui.py:68:7: F541 f-string is missing placeholders\ntests/python/e2e/get_auth_token.py:6:1: F401 'json' imported but unused\ntests/python/e2e/get_auth_token.py:43:15: F541 f-string is missing placeholders\ntests/python/e2e/get_auth_token.py:44:15: F541 f-string is missing placeholders\ntests/python/e2e/get_auth_token.py:44:43: E231 missing whitespace after ':'\ntests/python/e2e/get_auth_token.py:46:15: F541 f-string is missing placeholders\ntests/python/e2e/get_auth_token.py:46:40: E231 missing whitespace after ':'\ntests/python/e2e/test_protection_group_crud.py:6:1: F401 'json' imported but unused\ntests/python/e2e/test_protection_group_crud.py:79:15: F541 f-string is missing placeholders\ntests/python/e2e/test_protection_group_crud.py:113:15: F541 f-string is missing placeholders\ntests/python/e2e/test_protection_group_crud.py:134:15: F541 f-string is missing placeholders\ntests/python/e2e/test_protection_group_crud.py:144:11: F541 f-string is missing placeholders\ntests/python/e2e/test_protection_group_crud.py:164:19: F541 f-string is missing placeholders\ntests/python/e2e/test_protection_group_crud.py:165:19: F541 f-string is missing placeholders\ntests/python/e2e/test_protection_group_crud.py:171:15: F541 f-string is missing placeholders\ntests/python/e2e/test_protection_group_crud.py:173:15: F541 f-string is missing placeholders\ntests/python/e2e/test_protection_groups_fix.py:6:1: F401 'json' imported but unused\ntests/python/e2e/test_recovery_plan_api_crud.py:64:43: E231 missing whitespace after ':'\ntests/python/e2e/test_recovery_plan_api_crud.py:251:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_api_crud.py:284:5: F841 local variable 'updated_plan' is assigned to but never used\ntests/python/e2e/test_recovery_plan_api_crud.py:286:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_api_crud.py:287:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_api_crud.py:299:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_bugs.py:5:1: F401 'json' imported but unused\ntests/python/e2e/test_recovery_plan_bugs.py:7:1: F401 'typing.Any' imported but unused\ntests/python/e2e/test_recovery_plan_bugs.py:7:1: F401 'typing.Dict' imported but unused\ntests/python/e2e/test_recovery_plan_bugs.py:116:15: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_bugs.py:134:15: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_bugs.py:164:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_e2e.py:6:1: F401 'json' imported but unused\ntests/python/e2e/test_recovery_plan_e2e.py:129:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_e2e.py:130:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_e2e.py:131:13: E221 multiple spaces before operator\ntests/python/e2e/test_recovery_plan_e2e.py:132:13: E221 multiple spaces before operator\ntests/python/e2e/test_recovery_plan_e2e.py:133:13: E221 multiple spaces before operator\ntests/python/e2e/test_recovery_plan_e2e.py:209:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_e2e.py:210:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_e2e.py:210:13: E221 multiple spaces before operator\ntests/python/e2e/test_recovery_plan_e2e.py:211:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_e2e.py:211:13: E221 multiple spaces before operator\ntests/python/e2e/test_recovery_plan_e2e.py:212:11: F541 f-string is missing placeholders\ntests/python/e2e/test_recovery_plan_e2e.py:212:13: E221 multiple spaces before operator\ntests/python/e2e/test_recovery_plan_e2e.py:239:11: F541 f-string is missing placeholders\ntests/python/fixtures/recovery_plan_fixtures.py:60:39: E231 missing whitespace after ':'\ntests/python/fixtures/recovery_plan_fixtures.py:70:40: E231 missing whitespace after ':'\ntests/python/fixtures/recovery_plan_fixtures.py:80:40: E231 missing whitespace after ':'\ntests/python/fixtures/recovery_plan_fixtures.py:110:40: E231 missing whitespace after ':'\ntests/python/fixtures/recovery_plan_fixtures.py:140:40: E231 missing whitespace after ':'\ntests/python/fixtures/recovery_plan_fixtures.py:150:39: E231 missing whitespace after ':'\ntests/python/fixtures/recovery_plan_fixtures.py:160:40: E231 missing whitespace after ':'\ntests/python/fixtures/recovery_plan_fixtures.py:170:40: E231 missing whitespace after ':'\ntests/python/fixtures/recovery_plan_fixtures.py:180:40: E231 missing whitespace after ':'\ntests/python/fixtures/recovery_plan_fixtures.py:301:41: E226 missing whitespace around arithmetic operator\ntests/python/fixtures/recovery_plan_fixtures.py:301:43: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:5:1: F401 'time' imported but unused\ntests/python/mocks/mock_drs_client.py:91:29: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:91:33: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:91:37: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:91:47: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:91:60: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:205:25: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:205:29: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:205:33: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:205:43: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:205:56: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:355:29: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:355:33: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:355:37: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:355:47: E231 missing whitespace after ':'\ntests/python/mocks/mock_drs_client.py:355:60: E231 missing whitespace after ':'\ntests/python/monitor_drs_drill.py:14:1: C901 'monitor_drill' is too complex (11)\ntests/python/monitor_drs_drill.py:22:62: E226 missing whitespace around arithmetic operator\ntests/python/monitor_execution.py:13:1: C901 'monitor_execution' is too complex (11)\ntests/python/monitor_execution.py:16:5: F841 local variable 'drs' is assigned to but never used\ntests/python/standalone_drs_drill.py:2:1: F401 'json' imported but unused\ntests/python/standalone_drs_drill.py:64:15: F541 f-string is missing placeholders\ntests/python/standalone_drs_drill.py:72:19: F541 f-string is missing placeholders\ntests/python/standalone_drs_drill.py:173:1: C901 'monitor_drill_progress' is too complex (14)\ntests/python/standalone_drs_drill.py:257:11: F541 f-string is missing placeholders\ntests/python/standalone_drs_drill.py:257:27: E231 missing whitespace after ':'\ntests/python/standalone_drs_drill.py:280:15: F541 f-string is missing placeholders\ntests/python/test_drs_validation.py:7:1: F401 'json' imported but unused\ntests/python/test_drs_validation.py:43:7: F541 f-string is missing placeholders\ntests/python/test_drs_validation.py:78:7: F541 f-string is missing placeholders\ntests/python/test_drs_validation.py:91:11: F541 f-string is missing placeholders\ntests/python/test_with_lambda_invoke.py:8:1: F401 'datetime.datetime' imported but unused\ntests/python/test_with_lambda_invoke.py:8:1: F401 'datetime.timezone' imported but unused\ntests/python/test_with_lambda_invoke.py:55:13: F841 local variable 'drs_status' is assigned to but never used\ntests/python/test_with_lambda_invoke.py:56:25: F541 f-string is missing placeholders\ntests/python/unit/test_data_generator.py:7:1: F401 'pytest' imported but unused\ntests/python/unit/test_recovery_plan_delete.py:8:1: F401 'unittest.mock.MagicMock' imported but unused\ntests/python/unit/test_recovery_plan_delete.py:27:1: E402 module level import not at top of file\ntests/python/unit/test_wave_transformation.py:25:1: E402 module level import not at top of file\ntests/python/utils/test_data_generator.py:149:5: F841 local variable 'total_servers' is assigned to but never used\ntests/python/validate_setup.py:96:31: E231 missing whitespace after ':'\ntests/security/demo_security_test.py:99:11: F541 f-string is missing placeholders\ntests/security/demo_security_test.py:100:11: F541 f-string is missing placeholders\ntests/security/demo_security_test.py:148:11: F541 f-string is missing placeholders\ntests/security/demo_security_test.py:148:38: E231 missing whitespace after ':'\ntests/security/rbac_security_tests.py:15:1: F401 'time' imported but unused\ntests/security/rbac_security_tests.py:20:1: F401 'typing.Tuple' imported but unused\ntests/security/rbac_security_tests.py:363:39: F541 f-string is missing placeholders\ntests/security/rbac_security_tests.py:383:39: F541 f-string is missing placeholders\ntests/security/rbac_security_tests.py:406:35: F541 f-string is missing placeholders\ntests/security/rbac_security_tests.py:421:5: C901 'RBACSecurityTester._test_permission' is too complex (13)\ntests/security/rbac_security_tests.py:807:17: E722 do not use bare 'except'\ntests/security/rbac_security_tests.py:843:9: F841 local variable 'test_id' is assigned to but never used\ntests/security/rbac_security_tests.py:1107:9: F841 local variable 'findings' is assigned to but never used\ntests/security/rbac_security_tests.py:1131:75: E231 missing whitespace after ':'\ntests/security/rbac_security_tests.py:1132:15: F541 f-string is missing placeholders\ntests/security/rbac_security_tests.py:1132:41: E231 missing whitespace after ':'\ntests/security/rbac_security_tests.py:1139:81: E231 missing whitespace after ':'\ntests/security/run_security_tests.py:18:1: F401 'os' imported but unused\ntests/security/run_security_tests.py:26:1: E402 module level import not at top of file\ntests/security/run_security_tests.py:204:38: E221 multiple spaces before operator\ntests/security/run_security_tests.py:206:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:207:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:208:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:208:41: E231 missing whitespace after ':'\ntests/security/run_security_tests.py:209:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:209:48: E231 missing whitespace after ':'\ntests/security/run_security_tests.py:211:-54: E222 multiple spaces after operator\ntests/security/run_security_tests.py:211:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:211:44: E231 missing whitespace after ':'\ntests/security/run_security_tests.py:211:79: E221 multiple spaces before operator\ntests/security/run_security_tests.py:214:-23: E222 multiple spaces after operator\ntests/security/run_security_tests.py:214:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:225:37: E221 multiple spaces before operator\ntests/security/run_security_tests.py:227:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:228:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:230:-38: E222 multiple spaces after operator\ntests/security/run_security_tests.py:230:-10: E231 missing whitespace after ':'\ntests/security/run_security_tests.py:230:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:230:27: E221 multiple spaces before operator\ntests/security/run_security_tests.py:232:0: E222 multiple spaces after operator\ntests/security/run_security_tests.py:349:9: F841 local variable 'findings' is assigned to but never used\ntests/security/run_security_tests.py:385:77: E231 missing whitespace after ':'\ntests/security/run_security_tests.py:396:83: E231 missing whitespace after ':'\ntests/security/validate_framework.py:16:1: C901 'validate_configuration' is too complex (12)\ntests/security/validate_framework.py:93:9: F401 'jwt' imported but unused\n",
      "errors": "/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n/Users/jocousen/.local/share/mise/installs/python/3.12.11/lib/python3.12/site-packages/flake8_import_order/styles.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  from pkg_resources import iter_entry_points\n",
      "return_code": 1
    },
    "isort": {
      "tool": "isort",
      "status": "passed",
      "files_checked": 53,
      "files_needing_sort": 0,
      "output": "",
      "errors": "",
      "return_code": 0
    }
  }
}